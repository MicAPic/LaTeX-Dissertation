\chapter{Интеллектуальных анализ данных Web-ресурсов}\label{ch:ch5}

\section{Анализ Web-графов}\label{sec:ch5/sect1}

\subsection{Beyond left and right: Real-world political polarization in twitter discussions on inter-ethnic conflicts}\label{subsec:ch5/sec1/sub1}

\subsubsection{1. Introduction}

Today, social polarization is believed to be growing both along traditional and newer lines along which schisms form \cite{DucaSaving}, of which political ones are, arguably, the sharpest. Despite the ever-increasing body of knowledge on political attitudes and alignments online, we still lack understanding of how political divisions show up in issue-oriented discussions and whether there is a cross-country pattern.

Despite all the well-described representation distortions \cite{Daniels}, the content of social media is still used for predicting consumer and/or electoral choices \cite{ColleoniRozzaArvidsson}, and the studies of political polarization on social media, including Twitter, are growing in popularity \cite{Barbera}. However, there are several shortcomings in today’s studies of political polarization in user-generated content.

Thus, in most cases, audience polarization is studied by examining purely political issues or events, while social conflicts of race, gender or religious origins with both evident and idiosyncratic polarization and politicisation \cite{McCrightDunlap} are rarely studied. Due to context and language differences, multi-country studies are also rare, especially where both established democracies and countries beyond the Euro-Atlantics are included, as, for most observers, these remain politically incomparable. However, conditions other than political regimes may create grounds for cross-cultural juxtapositions \cite{BodrunovaLitvinenkoBlekanov,BodrunovaBlekanovMaksimov}.

Another conceptual limitation is that, even in the most advanced studies, the detection of users’ political affiliations or ideologies is done via proxies, most often via structural network factors, such as: friendship affiliations; patterns of following \cite{BarberaJostNagler,Rivero}; or content sharing \cite{ColleoniRozzaArvidsson}, which could be misleading. Addressing this gap, newer works show that group polarization in social media may be studied by looking at user texts, including complex referrals to specific phenomena that matter for group identity \cite{Evolvi}. We argue that the analysis of political divisions needs to unite both structural and content aspects \cite{Bodrunova}.

In order to bridge these gaps in previous studies, we look at Twitter discussions regarding inter-ethnic clashes; they have similar conflict triggers and structure of social groups involved into conflict \cite{BodrunovaLitvinenkoBlekanov2017}. Whilst avoiding making straightfor- ward comparisons, we explore users’ political polariza- tion and suggest a mixed method to detect it across three cases in different political regimes: the USA; Germany; and Russia. By the UN estimates of 2013–2017, these countries have recently been the three most attractive countries to migrants in the world \cite{UN2013,UN2017} and have all witnessed violent inter-ethnic clashes that became global trending topics on Twitter.

This article, thus, is organized as follows: In Section 1, we review the approaches of assessing user polarization on social media and the conflicts under our scrutiny. In Section 2, we formulate the research questions and describe our methodology. In Section 3, we provide the results; in Section 4, we interpret and discuss them.

\subsubsection{2. Political Polarization on Twitter: The Current State of Research}

\paragraph{2.1. Political Polarization Studies and the Current Research Gaps}

Throughout recent years, mixed evidence has persisted in social media studies on whether users go online to agree or to argue \cite{YardiBoyd}. Research into echo chambers \cite{ColleoniRozzaArvidsson,Sunstein2002} has shown that user homophily, both structurally and semantically, may prevent the formation of online ‘opinion crossroads’, as there is ‘evidence of persistent ideological sorting in online communication networks’ \cite[p.~2]{Barbera}. However, a range of works point to the opposite effects in Twitter communication, with weaker ties responsible for the diversification of the consumption of political information \cite{Barbera} as well as different platform features on Twitter leading simultaneously to echo chambers and inter-community communication \cite{ConoverRatkiewiczFrancisco}. Thus, evidence suggests more research is needed to assess the patterns of users’ political clusterization on social networks.

Until today, most Twitter polarization studies are bound to the one-country-one-case strategy -- with a few notable exceptions \cite{Barbera,BarberaJostNagler}. Another problem arises from today’s understanding of online political polarization \cite{BramsonGrimSinger} as a situation when ‘a social or political group is divided into two opposing sub-groups having conflicting and contrasting positions, goals and viewpoints, with few individuals remaining neutral or holding an intermediate position’ \cite[p.~215]{CalaisGuerraMeiraJrCardie} \cite{Isenberg,Sunstein2002}.

Empirical evidence suggests that, if a heterogeneous group containing users with two opposing views has a non-zero cross-view retweet rate, it will end up as two polarized communities \cite{ConoverRatkiewiczFrancisco}. Following this logic, the studies of political polarization result in pre- defined binary descriptions of polarized communities -- see \cite{MoralesBorondoLosada} for Venezuela; 
\cite{AgathangelouKatakisRori} for Greece; or \cite{WeberGarimellaBatayneh} for Egypt.

However, for studies beyond the two-party electoral process, it seems useful to remember that polarization is an individual case of clusterization along schismatic lines, disregarding the number of resulting clusters \cite{EstebanRay}. In social conflicts, conflicting groups are not necessarily structured along binary political party divisions. The classic work of \cite{TajfelTurnerAustin} shows how social identity (including ethnic identity) divides in- and out-groups, while a later normative model of dissent in social groups \cite{Packer} implies that, in inter-ethnic conflicts, the majority may divide into pro- minority and anti-minority clusters if the anti-minority at- titude is perceived as harmful to the collective \cite[p.~5]{PackerChasteen}. Also, the very political spectra may be highly multi-dimensional, as The Manifesto Project (https://manifesto-project.wzb.eu/) or Polity Project (http://www.systemicpeace.org/polity/polity4.htm) suggest. Thus, we consider polarization more as multi-polar fragmentation of divergent clusters, of which bipolar clusterization is just an option.

Non-bipolar clusterization seems to be especially probable for ‘issue’ or ‘ad hoc’ publics \cite{BrunsBurgess,Papacharissi} that emerge on social networks. This claim is supported by research on the topic and issue-based discussions \cite{ElgesemSteskalDiakopoulos}. In single case studies, user polar- ization has been studied in regard to abortion, same-sex marriage, gun control, and climate change \cite{Elgesem,CalaisGuerraMeiraJrCardie,YardiBoyd}, with varying degrees and directions of polarization detected. Moreover, there is a clear difference in polarization patterns between political and non-political issues \cite{BarberaJostNagler}. But the evidence of differences in polarization patterns is still scarce in academic literature.

The biggest challenge in today’s polarization studies is that instead of taking into account the actual content of user posts, detection of users’ political affiliations is conducted via proxies. Of those, the most interesting results come from assessing structural network factors such as friendship affiliations \cite{BarberaRivero}, retweeting patterns \cite{CalaisGuerraVelosoMeiraJr}, patterns of political following \cite{BarberaJostNagler,Rivero} or content-sharing patterns \cite{AdamicGlance,BakshyMessingAdamic,ColleoniRozzaArvidsson,Elgesem}. However, using proxies may be misleading \cite{AdamicGlance} and even express analysis of actual tweets shows the extreme diversity of political views, both in the form of direct expression and in opinionated content.

However, if not proxies, then what? Several studies suggest that group polarization in social media may be examined by analysing complex user referrals to phenomena that matter for their identity and group alignment \cite{Evolvi}, as it is how the attitudes are expressed in natural language. In the simplest possible terms, one would take user attitudes (positive, negative, and neutral) towards particular objects for such referrals.

Thus, we will try to construct group divisions from the actual Tweet content by coding user referrals towards political players and then defining which of these attitudes divide the users most, and for how many clusters.

\paragraph{2.2. Lexicon-Based Approaches to the Analysis of User Polarization}

The area of research closest to our idea of bringing content into polarization studies is a lexicon-based analysis of Twitter data. In recent years, the field has experienced explosive growth, predominantly based on the analysis of sentiments. Without delving fully into these methodologies, we will simply note that the possibility of use of vocabulary-based approaches for polarization assessment tasks \cite{HillmanTrier} is usually based on combining lexical and structural analysis. Several researchers went beyond so-called ‘naïve’ sentiment and have tried to link affect \cite{StieglitzDangXuan} or appraisal \cite{DangXuanStieglitzWladarsch} in user texts, types of lexical units \cite{SperiosuSudanUpadhyay}, and structural elements of Twitter discussions, like graphs of following or speed and volume of Tweet dissemination. Using these and other works, one could conclude that a sentiment-based approach to detecting left and right differences would imply developing a ‘negative’ (say, leftist) + neutral + ‘positive’ (say, rightist) lexicon and applying them to the discussion bulk. However, the problem that we have run into with this approach is the following:

\begin{enumerate}
	\item The users expressed not ‘left views’ or ‘right views’ but attitudes (with their lexical markers) towards politicians, institutions, social groups, or events (‘actors’);
	\item A given user would express attitudes towards not just one but many actors of different political stances;
	\item The same user could express recognized- as-rightist attitudes towards one actor and recognized-as-leftist attitudes towards another actor of comparable significance (e.g., immigrants and nationalists);
	\item The same user could express negative views on both leftist and rightist actors (say, Barak Obama and the KKK in the USA).
\end{enumerate}

In case 3, the user’s preferences, as measured by one-dimensional positive/negative sentiment analysis, would create a zero-sum, and assigning the bias would not be possible. In case 4, an at least two-dimensional measurement of the political spectrum is needed. Taking this into consideration, we have further developed our research questions and the exploratory research design based on user sentiment, but not on pre-defined target-independent lexicons. Instead, to better capture user attitudes, we will use expert coding of Tweets, standardising the coding process with the help of the idea of ‘complex user referrals’ by Evolvi (2017) \cite{Evolvi}.

\paragraph{2.3. The Research Cases}

As stated above, we have studied three intergroup conflicts of ethnic or racial origins in the three leading immigration recipient countries: the USA, Germany, and Russia. Direct comparisons of ad hoc discussions \cite{BrunsBurgess} are currently viewed with some doubt in academic literature. Without developing a strictly com- parative research design, we have argued elsewhere \cite{BodrunovaLitvinenkoBlekanov2017,BodrunovaBlekanovMaksimov} that the conflicts we picked for the analysis are similar enough as research cases. They share a range of attributes: a violent interpersonal trigger, outbursts of public discussion across media platforms (becoming trending topics on Twitter), social polarization along the inter-ethnic or inter-race chasms, street action, and involvement of federal authorities. In addition, they were chosen because they were the first in a line of similar conflictual cases and, at least partly, set the communicative patterns for later discussions.

The cases are described as follows:
\begin{enumerate}
	\item A violent uprising against immigrants from Central Asia in the district of Biryulyovo, Moscow, Russia, in September 2013. After immigrant Orkhan Zeinalov, allegedly, killed local youngster Egor Scherbakov, the Biryolyovo residents destroyed the local warehouse and a trade centre around which hundreds of illegal immigrants had been dwelling. Several non-violent ‘people’s gatherings’ followed.
	
	\item Ferguson riots, Missouri, USA, in August 2014. There, unarmed African American teenager, Mike Brown was shot to death by white police officer Darren Wilson. The killing, as well as the defensive behaviour of the local police department, spurred several waves of street protests and peaceful support actions, including crowds at Mike Brown’s funeral ceremony.
	
	\item Mass harassment and rape of females on New Year’s Eve of 2016 in Cologne, Germany. Over 1,000 women reported being harassed during the celebrations on the city’s main square, allegedly, by re-settlers from North Africa and the Middle East. After that, demonstrations in protest were organized by radical political actors (PEGIDA movement and the party ‘Alternative for Germany’).
\end{enumerate}

\subsubsection{3. Research Questions and Methodology}

\paragraph{3.1. The Outline of the Research Design}

\paragraph{3.1.1. Research Questions} From what was said above, we have formulated the fol- lowing research questions:

\paragraph{RQ1. How, if at all, do the users cluster within the discussions, based on their attitudes to the major conflict actors? Does binary clusterization best describe user grouping?}

\paragraph{RQ2. Can the clusters be described as left or right in relation to the respective national political spectra? If not, then how could these clusters be described?}

\paragraph{RQ3. Are there similarities in the cluster structure of the discussions?}

\paragraph{3.1.2. The Research Design}

The way the RQs were formulated demanded an exploratory research design. To answer the research questions, we had to see which user groups emerged among the influencers and what discourses they conveyed.

Our concept for detecting user polarization was that political grouping within a discussion was constructed via a multi-dimensional combination of attitudes towards political actors (as defined above). These major political actors needed to be deduced from the discussions themselves. Then, the attitudes towards these players would be decrypted by expert coders for the key users, or influencers \cite{BodrunovaBlekanovMaksimov}, usually the bearers of the spectrum of attitudes.

The data received after coding would undergo clusterization, with each user belonging to one non-fuzzy cluster. Tweets by the users in the detected clusters would provide the word frequency vocabularies, which, after expert assessment, would turn into clusterization vocabularies. The latter would then be applied to all the users in the discussions, to see which users get into clusters and which discourses form there.

This approach, even if simple enough and reliant on expert intrusion, allows us to take into account the nature of the users’ political discussion, as well as the lack of linearity of their political positioning. We consider this crucial for studies on conflict discussions, as it may allow the inclusion of conflict-invoked (e.g., pro- or anti- minority), actor-oriented (e.g., authorities), and traditional political divisions (e.g., left and right and centre and radical). At this exploratory stage, our method does not imply machine learning or supervised approaches to data classification; we use big datasets at this stage of data collection only.

\paragraph{3.2. The Research Procedures}

\paragraph{3.2.1. Data Collection and Pre-Processing}

As this work is part of a bigger research project, our methods of data collection were described in detail previously \cite{BodrunovaLitvinenkoBlekanov2017}. Here, we briefly describe the steps we followed.

We used trendinalia.com to detect the initial discussion keywords and snowball reading to amplify this collection, thus forming the vocabularies for crawling. Trendinalia.com is a web service that allows daily monitoring of both world, regional, and national Twitter trending hashtags and words with no hashtag on an hourly basis, with the possibility of backdating; it has worked best in terms of detecting the trending topics, compared to the over ten other websites we had tried since 2013.

Using an API-independent Twitter crawler \cite{BlekanovSergeevMartynenko}, we collected the content of the discussions. All publicly available Tweets and the data on user interactions (likes, retweets, comments) were collected by a two-step procedure. Step one in- cluded the users who posted under the hashtags. Step two detected a wider community of likers, retweeters and commenters. On the discussion graph, only the step one nodes have been visualized.

Due to reasons regarding feasibility and sample comparability, collection periods differed. Thus, for Russia and Germany, the download period was 30 days after the trigger event. For the USA, we had to select the two weeks following the shooting, with Mike Brown’s funeral as the central event. The user samples included:

\begin{itemize}
	\item For Biryulyovo -- Step1: over 3,700 users; Step2: over 12,000 users;
	\item For Ferguson -- Step1: over 70,000 users; Step2: over 210,000 users;
	\item For Cologne -- Step1: over 12,000 users; Step2: over 99,000 users.
\end{itemize}

\paragraph{3.2.2. Data Analysis}

To answer RQ1, we needed to cluster key users by their political views, define the cluster vocabularies, and apply these vocabularies to the rest of the users, in order to see the discussion clusters and interpret their discourses.

As we expected the influencers to be bearers of the polarizing discourse, for each case, we defined the group of influencers based on nine parameters: the number of Tweets, likes, retweets, comments, in-degree, out- degree, degree, betweenness, and pagerank centralities. After these experiments, using various thresholds, the top 50 users were chosen as the cutting line for each parameter. As many users were repeated in the top due to several specifications, the duplicates were deleted. After elimination of influencers with low numbers of Tweets and bot-like influencers (with a percentage of repeated Tweets over 50\%) 156 users for Germany, 105 users for Russia, and 105 the USA were left. But, for the USA sample (which was several times bigger in the number of Tweets), the number of users was reduced by half, to 52 users. Their respective tweet collections for reading and coding included 13,359, 3,012, and 9,540 tweets.

To define user attitudes towards political actors, we developed scales for coding and coded the users (not their Tweets). The coders were experts in inter-ethnic conflict and, additionally, academic native-speakers, and the level of inter-coder reliability as measured by Cohen’s kappa reached at least 0.68 for any two sub-samples. The coders used the scale from \(-2\) to 2 to assess the attitude of each user to the following groups that had been identified as attitude triggers by reading the tweets before coding: 1) the minority (immigrants or African Americans); 2) ‘radical right’ or ‘radical white’ -- nationalists in Russia, PEGIDA and AFD in Germany, and the Tea Party and the KKK (as a label for radical whites) in the USA; 3) the incumbent country leaders -- Vladimir Putin and Dmitry Medvedev in Russia, Angela Merkel in Germany, and Barak Obama and Hillary Clinton in the USA; 4) local authorities and police forces grouped together as the ‘oppressive and responsible’ actors. Attitudes towards liberal opposition in Russia and to Republicans and Democrats were also coded, as we found them salient in the Tweets.

Based on this coding, the influences were clustered to form groups with similar combinations of attitudes. After clustering, the Tweets of each group were merged, and fully divergent frequency vocabularies of their discourses were formed with the use of expert vocabulary ‘cleaning’. Then, we applied the vocabularies to all the users in the discussions, to see how the discourses distribute within the discussion structure. We also measured whether these discourses formed distinct nebulae; but even if they did not, we assessed which users belonged to these discourses and interpreted the semantics of their speech qualitatively. What mattered for our analysis was whether the influencers formed distinct groups; all the following steps were the consequence.

In detail, the research steps were the following.

\begin{enumerate}
	\item Based on our coding, the influences of each case were grouped with the help of a k-means clustering algorithm with sorted the distances. With the number of clusters and the number of variables being diverse, the best solutions were finally chosen based on Silhouette metric S, within- and between-cluster square sums, examinations of variable means in each cluster (see Figures~\cref{fig:userAttitudesRussia} to~\cref{fig:userAttitudesUSA} for Russia, Germany, and the USA, respectively), and expert reading of tweets in each cluster. All the three influencer groups clustered well; Germany clustered best, Russia followed, and the USA the least, but all the solutions were sufficient by Silhouette from 2 to 10 possible clusters. To identify the best solutions, other aforementioned metrics were used. Those solutions were:
	\begin{itemize}
		\item For Russia: 4 variables (attitude to liberal opposition excluded), 3 clusters of 49, 36, and 20 users, \(S > 0.4\); 
		
		\item For Germany: 4 variables, 3 clusters of 99, 48, and 9 users, \(S > 0.5\);
		
		\item For the USA: 5 variables (attitude to Democrats excluded), 4 clusters of 15, 15, 12, and 10 users, \(S > 0.2\).
	\end{itemize}
	\item For each cluster, word frequency vocabularies were formed by merging the Tweets, ranging the words by frequency, and expert reading. After reading, only the words unique or highly characteristic for each cluster (for example, two mentions in one cluster and 160 in another would result into eliminating the word in the first one and leaving it in the other). If the difference between the numbers of mentions in any two clusters of the case was smaller than ten times, the word was eliminated in all the clusters.
	
	\item We applied the thesaurus to the rest of the users in each case; we wanted to identify the users who use the words from the divergent thesauri. As a result, we have received three types of users in each case: 1) the users who belonged to clusters 1 to 3 or 4; 2) the ‘overlappers’ who used the language of more than one cluster; 3) the users who did not use the discourses (mostly due to a low number of their Tweets). To ensure a higher quality of marking users, rather than using individual words from the thesauri, two-word combinations were used.
	
	\item Based on this information, we constructed the graphs of discourse distribution, with users as apexes and user interactions (comments and/or retweets) as edges, and calculated the indices for user centralities. We assessed who were the most influential discourse bearers and what they spoke about. We used Gephi algorithms OpenOrd and Force Atlas 2 for graph construction (see \cite{MartinBrownKlavans}, on OpenOrd), as the former favours centripetal graphing and the latter better shows visual homophily (see Figures~\cref{fig:openOrdRussia} to~\cref{fig:openOrdUSA}). To see whether the groups bearing the discourses were tighter than inter-group connections, we calculated the mean number of in-group and inter-group edges.
\end{enumerate}

\begin{figure}[ht]
	\centerfloat{
		\includegraphics[scale=0.5]{userAttitudesRussia}
	}
	\caption{Mean values of user attitudes to the selected political actors in attitude-based clusters for Russia.}\label{fig:userAttitudesRussia}
\end{figure}

\begin{figure}[ht]
	\centerfloat{
		\includegraphics[scale=0.5]{userAttitudesGermany}
	}
	\caption{Mean values of user attitudes to the selected political actors in attitude-based clusters for Germany.}\label{fig:userAttitudesGermany}
\end{figure}

\begin{figure}[ht]
	\centerfloat{
		\includegraphics[scale=0.52]{userAttitudesUSA}
	}
	\caption{Mean values of user attitudes to the selected political actors in attitude-based clusters for the USA.}\label{fig:userAttitudesUSA}
\end{figure}

\begin{figure}[ht]
	\centerfloat{
		\hfill
		\subcaptionbox[List-of-Figures entry]{\label{fig:openOrdRussia-1}}{%
			\includegraphics[width=0.419\linewidth]{openOrdRussia1}}
		\subcaptionbox{\label{fig:openOrdRussia-2}}{%
			\includegraphics[width=0.4\linewidth]{openOrdRussia2}}
		\hfill
	}
	\legend{Blue: Cluster 1, ‘anti-establishment nationalists’; red: Cluster 2, ‘news disseminators’; green: Cluster 3, ‘angry citizens’; black: ‘overlappers’; grey: non-clustered users.}
	\caption{Communication within and between discursive groups of users in the discussions, with users as vertices and interactions (retweets and comments) as edges; reconstructed by OpenOrd and Force Atlas 2 algorithms for Russia.}\label{fig:openOrdRussia}
\end{figure}

\begin{figure}[ht]
	\centerfloat{
		\hfill
		\subcaptionbox[List-of-Figures entry]{\label{fig:openOrdGermany-1}}{%
			\includegraphics[width=0.419\linewidth]{openOrdGermany1}}
		\subcaptionbox{\label{fig:openOrdGermany-2}}{%
			\includegraphics[width=0.4\linewidth]{openOrdGermany2}}
		\hfill
	}
	\legend{Blue: Cluster 1, ‘nationalists’; red: Cluster 2, ‘news disseminators’; green: Cluster 3, ‘anti-nationalists’; black: ‘overlappers’; grey: non-clustered users.}
	\caption{Communication within and between discursive groups of users in the discussions, with users as vertices and interactions (retweets and comments) as edges; reconstructed by OpenOrd and Force Atlas 2 algorithms for Germany.}\label{fig:openOrdGermany}
\end{figure}

\begin{figure}[ht]
	\centerfloat{
		\hfill
		\subcaptionbox[List-of-Figures entry]{\label{fig:openOrdUSA-1}}{%
			\includegraphics[width=0.419\linewidth]{openOrdUSA1}}
		\subcaptionbox{\label{fig:openOrdUSA-2}}{%
			\includegraphics[width=0.4\linewidth]{openOrdUSA2}}
		\hfill
	}
	\legend{Blue: Cluster 1, ‘politicized observers’; red: Cluster 2, ‘media-oriented users’; green: Cluster 3, ‘human rights activists’; purple: Cluster 4, ‘whites’ blamers’; black: ‘overlappers’; grey: non-clustered users.}
	\caption{Communication within and between discursive groups of users in the discussions, with users as vertices and interactions (retweets and comments) as edges; reconstructed by OpenOrd and Force Atlas 2 algorithms for the USA.}\label{fig:openOrdUSA}
\end{figure}

To answer RQ2 about the left or right nature of the clusters, we partly recoded our coding data and corrected the graphs of means (Figures~\cref{fig:userAttitudesRussia} to~\cref{fig:userAttitudesUSA}) accordingly. Recoding was needed to re-interpret attitudes for and against a given actor as pro-left or pro-right. E.g., the influencers expressed attitudes towards political leaders (Obama, Merkel, and Putin), coded \(-2\) to 2. But, for the respective political spectra, Obama is leftist, while Merkel and Putin \cite{BluhmVarga} represent the rightist spectrum side. To ‘normalize’ the user attitudes, we recoded all the pro-left views as \(-1\) to \(-2\), and all pro-right views as 1 to 2 (see Table~\cref{tab:variableLeftRightNormalization}). By doing this, we could show on the graphs of means whether the clusters (and how many of them) were pro-left, pro-right, or mixed -- see Figures~\cref{fig:recordedDataRussia} to~\cref{fig:recordedDataUSA} for Russia, Germany, and the USA, respectively.

\begin{table}[ht]%
	\centering
	\caption{Recoding of variables for their left-right normalization.}%
	\label{tab:variableLeftRightNormalization}% label всегда желательно идти после caption
		\begin{adjustbox}{width=1\textwidth}
				\small
		\begin{tabular}{ c  c  c  c  c  c  c  c }% Вертикальные полосы не используются принципиально, как и лишние горизонтальные (допускается по ГОСТ 2.105 пункт 4.4.5) % @{} позволяет прижиматься к краям
			\toprule
			Country & Minority & President & Police-Authorities & Nationalists & Opposition & Democrats & Republicans\\
			\hline
			Russia & Recoded & Not & Not & Not & Recoded & -- & --\\
			Germany & Recoded & Not & Not & Not & -- & -- & -- \\
			USA & Recoded & Recoded & Not & Not & -- & Recoded & Not \\
			\bottomrule
		\end{tabular}%
			\end{adjustbox}
\end{table}

\begin{figure}[ht]
	\centerfloat{
		\includegraphics[scale=0.5]{recordedDataRussia}
	}
	\caption{Mean values for the recoded data on user attitudes towards the selected political actors for Russia.}\label{fig:recordedDataRussia}
\end{figure}

\begin{figure}[ht]
	\centerfloat{
		\includegraphics[scale=0.5]{recordedDataGermany}
	}
	\caption{Mean values for the recoded data on user attitudes towards the selected political actors for Germany.}\label{fig:recordedDataGermany}
\end{figure}

\begin{figure}[ht]
	\centerfloat{
		\includegraphics[scale=0.5]{recordedDataUSA}
	}
	\caption{Mean values for the recoded data on user attitudes towards the selected political actors for the USA.}\label{fig:recordedDataUSA}
\end{figure}

To answer RQ3, we qualitatively assessed the results for RQ1 and RQ2.

\subsubsection{4. Results}

Our results show that the discourses identified by coding influencers cover a substantial part of the discourse in all the cases: for Russia, the thesauri covered 31,5\%, in Germany, 63,4\% and, in the USA, 73,5\% of the users. This shows that influencers’ talk reflects the discourse of ‘ordinary users’ to different extents in each country, but everywhere we were able to detect the discourses that were important for the overall discussion.

As the figures suggest, in all the three cases, group structure was not binary; moreover, binary solutions for each country would hide important discourses that actually constituted the discussions. Neither did the group divisions correspond to the minority/pro-minority majority/anti-minority majority scheme. Instead, the clusters may be described as follows:

For Russia, the clusters include: ‘news disseminators’; ‘anti-establishment nationalists’; and ‘angry citizens’. The first group was mostly neutral but formed a substantial part of the political discussions by supplying (posting or retweeting) news at each stage of the conflict. The second cluster was clearly anti-immigrant and nationalistic but differed from European nationalism. Within the discussion, there was also an evident divide between the nationalist groups who supported the current establishment and those who actively opposed it. The former saw the incumbent leadership as the flesh of the 1990s’ elites who ‘had stolen the country’; such users, therefore, blamed the national policymakers for supporting the post-Soviet immigration. The second type of nationalism -- the pro-establishment one -- showed up in the third cluster of ‘angry citizens’. This cluster united anti-institutionalists who were raising voices against \textit{bespredel} (‘the absence of limits’ and rules of the game), but in differing ways. This diverse group included pro-Putin nationalists who were ready to fight with the Moscow riot police, liberal oppositional media and public figures who criticized the policymakers, and ‘tired citizens’ who negatively treated the immigrants, and the country leaders, and the local authorities, and the nationalists. Unlike in the ‘news disseminators’ cluster, the close-to-zero means for these variables here were the result of pro- and anti-establishment views compensating each other while the users united against police (see Figure~\cref{fig:userAttitudesRussia}).

For Germany, the clusters include: ‘news disseminators’; ‘nationalists’; and ‘anti-nationalists’. Discursively, the biggest group of ‘nationalists’ unites two similar sub-groups, one with slightly more aggressive tendencies towards small liberal-oriented parties and activist movements (like Antifa), and the other more critical of the national government. The anti-nationalist group is, however, also salient, making the German picture one-dimensional in terms of political divisions (pro- and anti-minority), even if the dimension is not political-party but issue-based. Also, the overlappers play a significant role here, as they visually stand in between the two opposing clusters, thus creating bridges for public dialogue (see Figure~\cref{fig:openOrdGermany}).

For the USA, the clusters include: ‘media-oriented users’; ‘human rights activists’; ‘politicized observers’; and ‘whites’ blamers’ (see Figure~\cref{fig:userAttitudesUSA}). Within the influencers, the clusters were similar in volume, but, on the big graph, the last two groups were relatively small- scale, while the first two dominated the graph. Just as in Russia, the media-oriented discourse was a part of the political discussion, but the three other groups were not neutral, especially ‘whites’ blamers’ and ‘human rights activists.’ The former actively blamed ‘the white dominance’ and called for action against oppression. Interestingly, the hashtag \#blacklivesmatter was less important for this group than for the media-oriented discourse. However, blaming hashtags and words like ‘murderer,’ ‘republikkklan,’ or ‘kkkop,’ and calls for action (like ‘\#arrestdarrenwilson,’ ‘\#boycottgofundme,’ or ‘\#donotshopmonday’), were prominent. The other group, very different from ‘whites’ haters,’ and linked the case to human rights issues like abortion (\#prolife), gender inequality (\#womeninequalityday), morality (\#moralmonday), and others. The group itself, as one can see even from the hashtags, was polar in itself in terms of left and right divisions on human rights. For this group, positioning on Mike Brown’s death was different, expressed mostly by ‘don’t shoot’ hashtags. ‘Politicized observers’ abstained from taking clear sides, but discussed the Ferguson events in terms of its influence upon the political process in America. Interestingly, the cluster that mostly reposted media, was the most pro-Wilson, as media, evidently, tried to remain balanced; they also reported police press conferences that were modestly defensive towards Darren Wilson.

Then, we looked at how the discourses we described spread inside the graph. Our task was not to calculate the level of homophily and prove user clustering for all the discussions; the goal was to see how the discourses actually spread and whether they spread in a similar way -- and they did not. For Russia and the USA, the discourses mixed, but if in Russia we saw inter-cluster talk, in the USA overlappers took almost all the space in the graph centre. And in Germany, the graph was clearly structurally divided. This was also proved by the mean in- and inter-cluster weighted number of edges: in Russia, the inter-cluster links took over (216 vs. 323.5, respectively), while in Germany (4392.75 vs. 2890.25) and the USA (21114.4 vs. 3755.2) in-group connections were stronger.

Thus, the attitude-based grouping was different in each of the three cases. Also, it was far from clear left-right identifications. In order to show it, we have recoded the variables as stated above, making pro- left views negative (\(-1\) to \(-2\)) and pro-right views positive (1 to 2). We considered anti-minority, anti-Obama/Clinton, pro-Putin/Medvedev, pro-Merkel, pro-police, pro-nationalist, anti-opposition (in Russia), anti-Democrat, and pro-Republican (in the USA) views pro-right, while the opposite was marked pro-left. See the full recoding scheme in Table~\cref{tab:variableLeftRightNormalization}.

The resulting graphs of means are quite telling (see Figures~\cref{fig:recordedDataRussia} to~\cref{fig:recordedDataUSA}). Both in Russia and Germany, the leaders representing rightist sides of the spectra have actually taken pro-migration stance, and this has made right-wing users who support nationalist movements and speak against immigrants, move left and be against the incumbent leaders, as well as against the local authorities and police for ‘not protecting’ the host communities. But the other clusters in the two countries quite strongly differ from each other. While in Germany issue-based leftism is clearly seen, the other Russian cluster of ‘angry citizens’ diverges into three discourses that combine clearly rightist, pro-establishment nationalism; liberal, anti-establishment oppositional speakers; and politicised citizens. These politicised citizens, paradoxically for external observers, do not support any of the existing political factions, due to their impotence in resolving local problems. Thus, at least two nationalist discourses were detected by us for Russia -- while in the USA there are two very different left-wing clusters, one clearly left, supportive of either Obama or Clinton and based on human rights’ discourse, and another that was sharply anti-white, even blaming Obama for not being protective enough, which, in our rough coding, made the cluster stick out to anti-Obama views on the rightist side of Figure~\cref{fig:recordedDataUSA} (in effect, being extreme left). The cluster of ‘politicized observers’, interestingly, is reminiscent of the ‘tired citizens’ in Russia, as they are, on average, only slightly pro-African-American and, more strongly, anti- leader, anti-police, and anti-majority.

Another crucial observation is that, while the divisions in the discussion clearly stem from local political contexts, they are quite far from expectations determined by the systemic political features of the countries. Thus, in the majoritarian USA where one would expect two-sided polarization, the clusters were, in fact, numerous and the discussion was based on overlappers. It was rather coalitional Germany that showed polarization. And in Russia, just one side of the spectrum was present in the discussion. Thus, it is not only the local political markets but also the nature of the issue and issue-based divisions that shape political clustering
.
Overall conclusions are thus the following: The discursive schisms do exist in issue-based discussions, but they do not fall into binary categories according to majoritarian political divisions, and; they only partially fall into the three-side divisions expected by the nature of the issue. Instead, local political spectra may provoke the formation of, for example, two leftist or two rightist clusters. Only Germany has demonstrated the expected divisions between anti- and pro-minority majority, while the minority remained highly under-represented at all, like in Russia -- and unlike in America.

The similarities can also be traced, but not in terms of left and right divisions. First, in all the discussions, a politically neutral news-based cluster played a significant structural role. Second, all three discussions revealed harsh anti-institutionalism, including that from the users who, in conventional logic, were expected to support the incumbents. Third, Germany and Russia were similar in how nationalist clusters were against the conservative governments, and Russia and the USA were similar in how the ‘tired citizens’ were politicised against all the political sides.

\subsubsection{5. Conclusion}

In our article, we have combined content analysis of social media with cluster analysis and graph construction. Our method has revealed greater complexity of politicised discourse within ad hoc Twitter discussions on inter-ethnic conflicts. Thus, we have found that there may be several clusters of leftist or rightist views even if the number of clusters is minimal, and users may combine formally leftist and rightist views if positions of political actors or the nature of the issue demand it. The groups we have detected differ highly in their conceptualisation from the traditional left and right divisions and left or right labels cannot be attached to individual users based on their preferences, like pro- and anti-minority stances or treatments of country leaders or parties. We have also shown that, on the graphs, the discourses intertwine quite intensely if we do not force the graphs to artificially diverge according to users’ political views.

Our research provides new input for rethinking the political divisions that form online, on what grounds they form, and how to detect them. The local political contexts, as well as the nature of the issues under scrutiny, are major factors to be taken into account. In our article, the ‘issue publics’ provide clues on how political opinion is veering away from traditional left and right divisions, and Twitter communication is more complicated than the imaginary cocooned talk in echo chambers, especially for issues beyond elections and direct policing.

Limitations of our method stem from the subjectivity of coding and from the low number of coded influencers, but these may be partially overcome by automatisation of coding collections and the increase of the number of coded users thanks to automatisation. Our method may be applied to detect hidden issue-oriented polarization beyond one-dimensional left-right political spectra.

\subsection{Please Follow Us: Media roles in Twitter discussions in the United States, Germany, France, and Russia}\label{subsec:ch5/sec1/sub2}

\subsection{Multi-dimensional echo chambers: Language and sentiment structure of Twitter discussions on the Charlie Hebdo case}\label{subsec:ch5/sec1/sub3}

\subsubsection{1. Introduction}

Public discussions on social networks potentially have trans-border and multilingual nature. This comes true in heated conflictual discussions that reach global trending topics. Such discussions are expected to demonstrate ‘civilizational clashes’ \cite{AnKwakMejova}.

Being part of the global public sphere, since the 1990s, such discussions were expected by many observers to be more horizontal, all-involving, and democratically efficient \cite{Fuchs} than the traditional mass-mediated discussions \cite{McQuail}. But, with time, criticism towards the democratic quality of discussions in social media arose, with many works discovering the patterns of echo chambering and discourse polarization in social networks \cite{Sunstein2001,Sunstein2002,BarberaJostNagler,BastosMerceaBaronchelli,ColleoniRozzaArvidsson,ConoverRatkiewiczFrancisco}, which lowered the capacities of inter-group discussions and, thus, just formed an additional line of social segregation.

Object-oriented hashtagged discussions have been thoroughly studied in the 2010s, including those on political and social conflicts. But there is still scarce knowledge on whether affective hashtags \cite{Papacharissi} that convey emotions -- either of solidarity with or of anger towards a particular social group -- work in terms of user clusterization. Also, there is no clear understanding of comparative democratic quality of emotionally ‘positive’ and ‘negative’ hashtags in terms of echo chambering.

In this paper, we address these gaps by analyzing the Twitter discussion on the \textit{Charlie Hebdo} massacre of 2015. In the discussion upon the mass killings, the Twittershpere has created \#jesuischarlie and \#jenesuispascharlie -- two emotionally differing discussion clusters with, allegedly, opposite sentiments towards the journal’s ethics and freedom of speech; the hashtags soon became ‘role models’ for online solidarity towards the victims of terrorist attacks and anthropogenic disasters.

To analyze the echo chambering patterns in the two discussions, we have focused upon two levels of echo chambers. We were wondering whether echo chambers formed on the level of a hashtag (based on language use) and within a particular language (based on user sentiment of French-speaking users).
The remainder of the paper is organized as follows. Section 2 reviews the literature on echo chambering in social media. Section 3 presents our methodology and the conduct of the research. Section 4 presents our results and discusses them.

\subsubsection{2. User Groupings on Twitter and the Efficacy of Public Sphere}

\paragraph{2.1 Social Media and the Public Sphere: Echo Chambers vs. Opinion Crossroads}
Public sphere as a spatial metaphor for a complex of discussions and procedures with a public status and decision-making goals \cite{Kleinstuber} has been amplified by the appearance of social media in the 2000s. By 1990s, it had been established in the academic literature that mediatized public sphere with traditional media playing the role of information hubs was uneven and hardly efficient in terms of access to opinion expression, as whole social groups remained under-represented, and newsmakers privileged in comparison to \textit{vox populi}. With the appearance of social media, hopes arose that the new communicative milieus would foster horizontalization of communication and provide for democratization and higher political participation \cite{Fuchs}. Also, hopes for better understanding and resolution of non-political inter-group conflicts existed.

But with time, these hopes fainted, as offline disparities seemed to reproduce online, including political interests, race, gender, and other inequalities \cite{Daniels}; emotion and affect proved to rule the discourse \cite{Papacharissi}, with publics even in most democratically developed countries moving from diverse in opinion to dissonant and disconnected \cite{Pfetsch}. With the development of social network analysis (SNA) and its application to social media research, the question of the efficacy of the public discussions in social media \cite{BrunsHighfield} became linked to network and structural features of the discussions, such as the influencer status \cite{BodrunovaBlekanovMaksimov,BodrunovaLitvinenkoBlekanov2016} and clusterization of users also known as user polarization \cite{BarberaJostNagler,BastosMerceaBaronchelli} and echo chambering \cite{ColleoniRozzaArvidsson,ConoverRatkiewiczFrancisco}. Some evidence was also gathered on discussion sphericules forming on the global scale just as well as nationally \cite{CammaertsAudenhove}.

\paragraph{2.2 Why the Twitter Discussions Fragment: Linguistic Properties of Speech as Catalyzers of Echo Chambering}
In early studies of social networks and its users, authors interested in testing the ability of networks to pull together users from distant locations and weak ties linked geo- graphical distance with factors like residence of users and their language profile \cite{TakhteyevGruzdWellman}. This is why Twitter that enabled the (arguably) quickest possible information spread across locations and languages became a major attractor of scholarly attention \cite{LotanGraeffAnanny,HongConvertinoChi}. But despite the global reach of the platform, several studies have found that people were still connected locally on Twitter \cite{CammaertsAudenhove,YardiBoyd}.

Along with locality and residence, linguistic factors, arguably, play a major role in user grouping on the global scale. Thus, the language(s) used by the discussion participants is the first natural barrier that is expected to make users group together and communicate within their language-based echo chambers \cite{ChenTuZheng}, both on Twitter on the whole and within particular hashtags \cite{BastosPuschmannTravitzki}.

Other factors have also been discussed as the catalyzers of user grouping on Twitter. Among those, political attitudes lead the research agenda \cite{BarberaJostNagler,BastosMerceaBaronchelli}. Here, several ways to detect user clusterization exist. Of them, use of network or semantic proxies like friendship ties \cite{BarberaRivero}, patterns of following \cite{Rivero} and retweeting \cite{CalaisGuerraMeiraJrCardie}, content sharing \cite{ColleoniRozzaArvidsson,BakshyMessingAdamic} etc. is till today the most prominent; another is automated analysis of user sentiment, either in general or toward an issue/actor in question (object-oriented) \cite{ConoverGoncalvesRatkiewicz}.

But all these studies depict user groupings within a single dimension; our idea is to try and trace user groupings multi-dimensionally -- both on the level of a hashtag (based on language use) and within a language nebula (based on user sentiment).

\paragraph{2.3 The \textit{Charlie Hebdo} Case: Emotional Hashtags and User Groupings}
To search for multi-dimensional echo chambers, we have chosen the case of the \textit{Charlie Hebdo} massacre of 2015. Here, we could hypothesize that the existence of emotionally opposite hashtags (\#jesuischarlie and \#jenesuispascharlie) already creates enclaves within the general discussion on the case. Then, within the hashtags, several clusters based on language structure may exist. Then, on the third level, we will look whether within the language clusters sub-clusters of sentiment form. In general, our idea is to see how exactly the language clusters correspond to the sentiment clusters in each language and whether ‘positive’ users within one language are linked to such in another language, while ‘negative’ users also group across languages in a similar way. But here we present only preliminary results that check if the user clusters may at all be detected based on language and on sentiment within a language.

\paragraph{2.4 Research Hypotheses}

Thus, our hypotheses are the following:
\begin{itemize}
	\item H1a. Non-random user groups will be detected for both hashtags, as based on
	language use.
	\item H1b. \#jesuischarie will not differ from \#jenesuispascharlie in their language
	structure, as both hashtags have reached global trending topics and are expected to show ‘civilizational clashes’.
	\item H2a. Non-random user groups will be detected within one language (French), as based on positive and negative sentiment.
	\item H2b. \#jesuischarlie will differ from \#jenesuispascharlie in the grouping based on user sentiment, due to the emotional opposition of the hashtags themselves.
	\item H3. Multi-layer echo chambering may be detected in trans-border hashtagged discussions of global reach.
\end{itemize}

\subsubsection{3. Data Collection and Conduct of Research}

\paragraph{3.1 Data Collection and the Datasets}
Using a web crawler developed especially for the Twitter data collection, we gathered all the tweets published openly under the hashtags \#jesuischarlie and \#jenesuispascharlie (by separate crawls) within January 7 to 9, 2015, as these three days covered the active conflict (from the killings in the editorial office to the assailants’ death) when the users provided virtually millions of tweets for collection.

The collected datasets included: for \#jesuischarlie: 420,080 tweets; 266,904 tweeters; 719,503 users who interacted with the posted tweets (by likes, retweets, or comments); for \#jenesuispascharlie: 7,698 tweets; 5,466 tweeters; 17,872 users who posted and interacted with the posted tweets (by likes, retweets, or comments).

These full datasets were later used to reconstruct the overall web graphs for the two hashtags. But the datasets were very different in size, and be able to color them with language markers, we needed to sample the users for coding having in mind the volume difference of the datasets.

\paragraph{3.2 Language Analysis: Sampling, Coding, and Graph Reconstruction} 
To answer H1a and H1b, we have coded the users for their language use and then applied these data to the datasets for web graph reconstruction.

After eliminating bots and bot-like users (those who posted over 60\% of doubled tweets) as well as hashtag-only tweets, we have followed the strategy developed by the research group for previous Twitter studies \cite{Authors2016a,Authors2016b,Authors2018}, namely uniting random sampling with detection of influential users (influencers) for taking them into account. Then, we have coded all the influencers (disregarding the number of tweets they posted; for \#jesuiuscharlie, 402 users, for \#jenesuispascharlie, 85 users) and ‘ordinary users’ sampled in the feasible and comparable way. For \#jenesuispascharlie that was substantially smaller, all the users with 3 and more tweets were coded (339 users); for \#jesuischarlie, the ‘ordinary users’ with 5 tweets or more were taken into account (9,090 users), and of them, each second was coded (4500 users).

All the sampled underwent expert reading and were coded manually marking the number of tweets in language 1, language 2, and other languages; thus, users posting on one, two, and three or more languages were defined. The languages were identified for each user; in case of rare languages, Yandex language identifier was employed.

To reconstruct the graphs, we use Gephi API algorithms openly available online. Of the available algorithms, two were chosen: Hu \cite{Hu} and OpenOrd \cite{MartinBrownKlavans}; here, YifanHu-based graphs are presented, as the OpenOrd graphs require more space for presentation. We colored both the nodes (users) and the edges (connections between users). To prove that the visual nebulae are not artifacts of subjective viewership, we calculated the percentage of edges between and inside language groups, eliminating the ‘loops’ of self-commenting/liking by the users.

\paragraph{3.3 Sentiment Analysis: Sampling, Vocabulary Building, and Graph Reconstruction}After we have proved that the French-speaking users show non-random grouping in both cases (see below), we have taken them for sentiment analysis. The number of users for \#jesuischarlie included 1291 user; for \#jenesuispascharlie, 117 users.

Our strategy for French-language sentiment analysis was the following. We have united three sources in our vocabulary: the existing French dictionary with sentiment marking, machine translation from an additional Wordnet vocabulary, and the case-based vocabulary created from the collected tweets and manually marked for positive, negative, and neutral sentiment regardless of the case-specific meanings.

This vocabulary was applied to each tweet of the abovementioned French-speaking users; for each tweet, the sentiment was calculated. Then, the thresholds were defined: positive and negative were the users with positive(+neutral) and negative(+neutral) tweets, respectively; neutral were those with neutral tweets only; mixed were those with positive + negative(+neutral) tweets.

Then, we also checked the groupings with by calculating the percentage of edges both between and inside language groups, eliminating the ‘loops’ of self-commenting/liking by the users.

\subsubsection{4. Results and Discussion}

Our results are described below with regard to the hypotheses stated above.

\textit{H1a/H1b.} To assess the user groupings in both hashtagged discussions, we have reconstructed the web graphs for the coded users (see Fig.~\cref{fig:yifanHuGraphs-2} for \#jesuischarlie and Fig.~\cref{fig:yifanHuGraphs-2} for \#jenesuispascharlie, respectively). What we see on the graphs are three nebulae for~\cref{fig:yifanHuGraphs-1}: French, English, and other European, and two for~\cref{fig:yifanHuGraphs-2}: French and Engish. But the results of calculations of percentage of edges between and inside groups tell that the actual grouping is slightly different from what we see with unaided eyes. For \#jesuischarlie, the nebulae with density higher than the inter-group ones are French, English, and French/English (52.1\%, 16.7\%, and 16.9\%, respectively, against 6.17\% for inter-group edges) and not other European. For \#jenesuispascharlie, the graph is much denser (26\%), but still the same three clusters show up, with 26.2\%, 22.4\%, and 18.8\%, respectively; in both cases, other language clusters are virtually non-existent and do not mount to 3\%. As we stated in our earlier investigations \cite{Authors2018}, we have not seen a sign of ‘civilizational clashes’ in any of the hashtags.

\begin{figure}[ht]
	\centerfloat{
		\hfill
		\subcaptionbox[List-of-Figures entry]{\label{fig:yifanHuGraphs-1}}{%
			\includegraphics[width=0.419\linewidth]{yifanHuGraphs1}}
		\subcaptionbox{\label{fig:yifanHuGraphs-2}}{%
			\includegraphics[width=0.4\linewidth]{yifanHuGraphs2}}
		\hfill
	}
	\caption{The YifanHu graphs (fragments) for language distribution in \cref{fig:yifanHuGraphs-1} \#jesuischarlie and \cref{fig:yifanHuGraphs-2} \#jenesuispascharlie. Red: French; blue: English; lilac: French/English; green: other European. (Color figure online)}\label{fig:yifanHuGraphs-12}
\end{figure}

Thus, H1a is proven; H1b is proven too but not due to ‘civilizational clashes’.

\textit{H2a/H2b.} To see the user grouping and sentiment cleavages within the French-speaking parts of the discussions, we have reconstructed the web graphs for them (see Fig.~\cref{fig:yifanHuGraphs-3} for \#jesuischarlie and Fig.~\cref{fig:yifanHuGraphs-4} for \#jenesuispascharlie, respectively).

\begin{figure}[ht]
	\centerfloat{
		\hfill
		\subcaptionbox[List-of-Figures entry]{\label{fig:yifanHuGraphs-3}}{%
			\includegraphics[width=0.419\linewidth]{yifanHuGraphs3}}
		\subcaptionbox{\label{fig:yifanHuGraphs-4}}{%
			\includegraphics[width=0.4\linewidth]{yifanHuGraphs4}}
		\hfill
	}
	\caption{The YifanHu graphs (fragments) for language distribution in \cref{fig:yifanHuGraphs-3} \#jesuischarlie and \cref{fig:yifanHuGraphs-4} \#jenesuispascharlie. Red: French; blue: English; lilac: French/English; green: other European. (Color figure online)}\label{fig:yifanHuGraphs-34}
\end{figure}

Here, H2a should be rejected for \#jesuischarlie and partly supported for the second hashtag. Both in the graph and in the edge percentage calculations, it is only users with mixed sentiment who form a group (38.33\% against 56.05\% for the inter-group connections) in \#jesuischarlie. But for \#jenesuispascharlie, both mixed and negative user groups seem to have a potential for grouping (19.5\% and 15\% against 62.1\% for inter-group connections). Thus, H2b is supported: the cases do differ.

Even with the cases of such different sizes, H3 is supported: thanks to the negative nebula in \#jenesuispascharlie, we can state that echo chambers are able to form on at least two levels of the trans-border conflictual discussions of global (or, more precisely, macro-regional) reach. This adds to our understanding of the nature of public discussions in social media, even if lowers hopes for all-encompassing public spheres.

\section{Тематическое моделирование контента}\label{sec:ch5/sect2}

\subsection{Topic modeling of conflict ad hoc discussions in social networks}\label{subsec:ch5/sec2/sub1}

\subsubsection{Introduction}

As a result of different media platforms achieving a steady user growth in a recent years more and more people begin to use different social networks as the main source of news on economical, political and social events. In particular, ad hoc discussions emerged which can be defined as a debate about a specific problem. In most cases such discussions appear in the case of controversial events and involve large number of participants.

Presence of such user activity raises the problem of analyzing large volumes of this type of data which has become one of the most important problems in many data analysis tasks, including topic modeling. Topic modeling algorithm in this case is an algorithm that, given the number of topics and the list of user messages can output two distributions: topics over documents and of words over topics. Such algorithm can help in the understanding of different points of view and highlight the main arguments. This can be useful in many ways one of which is the case in which the number of documents is big and we want to know their general content without reading them all. Another use case is data prepossessing, reducing the dimensions of data to use in other analysis tasks such semantic analysis. However directly applying traditional topic models like LDA and PLSA to short texts can be problematic primarily due to the sparsity of data given the specificity of short texts. In this paper we are studying the usage of different models on a large scale data which can effectively infer hidden topics in big discussions taking these features into account.

\subsubsection{Prior Work}

Early studies of the problem of topic modeling on short texts mainly concerned the use of external knowledge to improve the representation of text data. For example, Phan and others \cite{HoriguchiPhanNguyen} used the modeling of those short texts based on the traditional topic model, the effectiveness of which was tested on a large-scale data set for the purpose of short texts classification. In these works, it is assumed that the use of data derived from long texts could help improving the model for short texts. However, these methods are effective only when the auxiliary data are closely related to the original data. In the case of texts, obtained from social networks this task is impossible due to most of user messages being self-contained. Another assumption is based on the use of different aggregation methods. In the case of data, obtained from the Twitter, user messages can be aggregated by authors, publication time and hashtags. The \cite{WrayLexingRishabh} shows that the best performing method is based on hashtag based aggregation, however it can’t be applied then the texts themselves are collected using a series of hashtags (which is one of better ways for collecting data on big events). Author based aggregation can also be unreliable since the majority of users will have very few messages on any given topic. In this paper we are focusing on models, relying on statistical information about the data.

\subsubsection{Topic Models}

\paragraph{LDA.} Latent Dirichlet Allocation is a three-level hierarchical Bayesian model in which each element of the collection is modeled as a finite distribution over the set of topics. Each topic, in turn, is modeled as an infinite mixture by the set of topic probabilities \cite{MichaelJohnDavidAndrew}.

Lda assumes the following generative process:
\begin{itemize}
	\item Choose \(\theta_i \sim \textit{Dir}(\alpha)\)
	\item Choose \(\phi_i \sim \textit{Dir}(\beta)\)
	\item For every word position \(i, j\):
	\begin{itemize}
		\item Choose a topic \(z_{i, j} \sim \textit{Multinomial}(\theta_i)\)
		\item Choose a word \(w_{i, j} \sim \textit{Multinomial}({\phi_z}_{i,j})\)
	\end{itemize}
\end{itemize}
The model parameters \(\alpha\) and \(\beta\) are typically chosen sparse for better performance on short texts. In this paper, LDA is used as a baseline model, the effectiveness of which for standard texts has been proven both theoretically and in many experimental results.

\paragraph{BTM.} Biterm Topic Model performs topic modeling task by modeling a set of biterms (unordered word pair cooccurring in a short context). The main idea is that if two words co-occur more frequently, they are more likely to belong to a same topic \cite{YanyanJiafengXueqi}.

Btm assumes the following generative process:
\begin{itemize}
	\item Draw \(\theta_i \sim \textit{Dir}(\alpha)\)
	\item For each topic \(k\):
	\begin{itemize}
		\item draw \(\phi_k \sim \textit{Multinomial}(\beta)\)
	\end{itemize}
	\item For each biterm \(b_i\):
	\begin{itemize}
		\item draw \(z_i \sim \textit{Multinomial}(\theta)\)
		\item draw \(w_{i,1},w_{i,2} \sim \textit{Multinomial}({\phi_z}_i)\)
	\end{itemize}
\end{itemize}

As BTM does not model documents explicitly, we must provide a way to infer the topics in a document, i.e., evaluating the topic posterior. Using the chain rule the following equation was obtained:
\begin{equation}
	\label{eqn:29}
	P(z \mid b) = \sum P(z \mid b_i) P(b_i \mid d)
\end{equation}

Where \(P (z \mid b_i)\) can be obtained using via Bayes’ formula based on the parameters learned in BTM and \(P(b_i \mid d)\) can be calculated using empirical distribution of words in a document.

\paragraph{WNTM.} Word Network Topic Model’s idea is based on the following observations. When the texts are short, the word-document space is very sparse, but the word-word space still contains a large number of non-zero elements. Since the topic distribution for each doc- ument can not be recognized accurately in short or unbalanced texts, instead WNTM uses the topic distribution for each word \cite{KeYuanJichang}. Therefore, WNTM studies the distribution by topics for words, rather than those for documents. Studying the topics of the word, rather than those of the document make WNTM less sensitive to the length of the document. In addition, a network of words can be built with any type of text, which makes the WNTM model simple and universal in real applications, unlike other models, such as the mixture of unigrams \cite{ThrunMitchellNigam} and BTM.

The generative process of the model is in many respects similar to that of the LDA, but due to the use of a different distribution it has its own features:
\begin{itemize}
	\item For every latent word group \(z\) choose \(\phi \sim Dir(\beta)\)
	\item Choose \(\vartheta_i \sim \textit{Dir}(\alpha)\) distribution of a latent word group for adjacent word list \(L_i\) for word \(w_i\)
	\item For every word \(w_j \in L_i\):
	\begin{itemize}
		\item Choose a latent word group \(z_j \sim \vartheta_i\) 
		\item Choose an adjacent word \(w_j \sim {\phi_z}_j\)
	\end{itemize}
\end{itemize}

\begin{figure}[ht]
	\centerfloat{
		\includegraphics[scale=1.0]{wntmGeneration}
	}
	\caption{Generation of word network for WNTM model \cite{KeYuanJichang}.}\label{fig:wntmGeneration}
\end{figure}

Similarly to BTM this model can’t be directly applied to get topic distributions over documents. To get the topics in the document, we assume that the proportions of the words generated by the document is equal to the proportions of the document topics, that is:
\begin{equation}
	\label{eqn:30}
	P(z \mid d) = \sum P(z \mid w_i) P(w_i \mid d)
\end{equation}
Where \(P(z \mid w_i\)) equal \(\vartheta_{i,z}\), obtained in the generative process of WNTM and \(P(w_i \mid d)\) obtained as an empirical distribution of words in documents.

\subsubsection{Experiment}

The experiment was conducted to evaluate the quality of these three models, on three sets of ad hoc discussions. The following coherence measures were used to test the effectiveness of each model:

\begin{itemize}
	\item UMass \cite{MimnoWallachTalley} measures how often a common word of each topic is in average a good predictor for a less common word.
	\item NPMI \cite{StevesonAletras} is a normalized version of pointwise mutual information.
\end{itemize}

\paragraph{Data sets.} In this work models were tested on data, collected on three ad hoc discussions from Twitter social network: Riots in Biryulevo (Russia), October 2013 \cite{BodrunovaLitvinenkoBlekanov}, Ferguson unrest (USA), August 2014 \cite{SmoliarovaBlekanovBodrunova} Charlie Hebdo shooting (France), January 2015 \cite{SmoliarovaBlekanovLitvinenko}. The data was crawled based on hashtags in user messages.

\textit{Byrulevo. Riots in Biryulevo}
\begin{itemize}
	\item Total number of user messages: 10215
	\item Total number of users participated in the discussion: 11429
	\item Surveyed time period: 1.10.2013 - 31.10.2013
	\item Number of users who published tweets in the period under consideration: 3574
\end{itemize}

\textit{Ferguson. Ferguson unrest}
\begin{itemize}
	\item Total number of user messages: 193812
	\item Total number of users participated in the discussion: 169677
	\item Surveyed time period: 22.08.2014 - 31.08.2014
	\item Number of users who published tweets in the period under consideration: 70018
\end{itemize}

Charlie Hebdo. Charlie Hebdo shooting
\begin{itemize}
	\item Total number of user messages: 505069
	\item Total number of users participated in the discussion: 952615
	\item Surveyed time period: 07.01.2015 - 10.01.2015
	\item Number of users who published tweets in the period under consideration: 238491
\end{itemize}


\begin{table}[ht]%
	\centering
	\caption{Topics for Byrulevo data set.}%
	\label{tab:byrulevoTopics}% label всегда желательно идти после caption
	%	\begin{adjustbox}{width=1\textwidth}
		%		\small
		\begin{tabular}{ c  c  c  c }% Вертикальные полосы не используются принципиально, как и лишние горизонтальные (допускается по ГОСТ 2.105 пункт 4.4.5) % @{} позволяет прижиматься к краям
			\toprule
			Topic 1 & Topic 2 & Topic 3 & Topic 4 \\
			\hline
			\multicolumn{4}{c}{\makecell{LDA}} \\
			migrant & warehouse & broadcast & riot  \\
			Zeynalov & work & live & Manezhka \\
			police & man & moscow & moscow \\
			murder & boutique & photo & migrant \\
			Sherbakov & moscow & find & block \\
			\hline
			\multicolumn{4}{c}{\makecell{WNTM}} \\
			Moscow & news & Moscow & police \\
			event & Sherbakov & OMON & authorities \\
			Russia & migrant & Zeynalov & killer \\
			riot & murder & arrest & russian \\
			mayhem & killer & Sherbakov & meetings \\
			\hline
			\multicolumn{4}{c}{\makecell{BTM}} \\
			citizen & OMON & Sherbakov &  russian \\
			police & warehouse & Zeynalov & government \\
			local & police & killer & riot \\
			riot & arrest & arrest & migrant \\
			Moscow & killer & moscow & news \\
			\bottomrule
		\end{tabular}%
		%	\end{adjustbox}
\end{table}

\begin{table}[ht]%
	\centering
	\caption{Topics for Charlie Hebdo data set.}%
	\label{tab:charlieTopics}% label всегда желательно идти после caption
	%	\begin{adjustbox}{width=1\textwidth}
		%		\small
		\begin{tabular}{ c  c  c  c }% Вертикальные полосы не используются принципиально, как и лишние горизонтальные (допускается по ГОСТ 2.105 пункт 4.4.5) % @{} позволяет прижиматься к краям
			\toprule
			Topic 1 & Topic 2 & Topic 3 & Topic 4 \\
			\hline
			\multicolumn{4}{c}{\makecell{LDA}} \\
			policia & die & police & islam \\
			Paris & satire & shooting & religion \\
			terroristas & cartoonist & attack & youngest  \\
			sospechosos & frankreich & suspects & local \\
			ataque & attentater & update & extrimists \\
			\hline
			\multicolumn{4}{c}{\makecell{WNTM}} \\
			attack & suspects & cartoonists & media\\
			french & police & support & cartoons  \\
			today & two & editor & toxic \\
			terror & attack & respond & image \\
			killed & hostage & journalism & caricatures \\
			\hline
			\multicolumn{4}{c}{\makecell{BTM}} \\
			police & french & victims & muslims\\
			suspects & shooting & solidarity & islam \\
			hostage & gunman & attack & must \\
			killed & dead & France & say\\
			breaking & killed & jesuischarlie & religion \\
			\bottomrule
		\end{tabular}%
		%	\end{adjustbox}
\end{table}

\begin{table}[ht]%
	\centering
	\caption{Topics for Ferguson data set.}%
	\label{tab:fergusonTopics}% label всегда желательно идти после caption
	%	\begin{adjustbox}{width=1\textwidth}
		%		\small
		\begin{tabular}{ c  c  c  c }% Вертикальные полосы не используются принципиально, как и лишние горизонтальные (допускается по ГОСТ 2.105 пункт 4.4.5) % @{} позволяет прижиматься к краям
			\toprule
			Topic 1 & Topic 2 & Topic 3 & Topic 4 \\
			\hline
			\multicolumn{4}{c}{\makecell{LDA}} \\
			police & MikeBrown & militarization & Miami  \\
			life & black & police & overtown \\
			surrender & brown & law & America \\
			must & racism & reason & vote  \\
			dissa & justice & end & jail \\
			\hline
			\multicolumn{4}{c}{\makecell{WNTM}} \\
			MikeBrown & CNN & must & movement  \\
			amp & cops & surrender & speak \\
			police & Times & police & join  \\
			black & black & dissa & support \\
			people & shooting & see & now\\
			\hline
			\multicolumn{4}{c}{\makecell{BTM}} \\
			must & join & community & pd\\
			police & movement & Miami & look \\
			surrender & now & support & closer  \\
			dise & speak & lot & MikebBrown  \\
			click & die & overtown & msnbc \\
			\bottomrule
		\end{tabular}%
		%	\end{adjustbox}
\end{table}

\paragraph{Results.} Coherence measures were calculated and plotted to estimate the models’ effectiveness on different number of topics. The results (Figures~\cref{fig:charlieCoherence,fig:byrulevoCoherence,fig:fergusonCoherence}) allow us to talk about the approximate number of topics that is optimal for this task, but due to imperfection of quality indicators it requires manual clarification.

\begin{figure}[ht]
	\centerfloat{
		\includegraphics[scale=1.0]{charlieCoherence}
	}
	\caption{Coherence scores on Charlie Hebdo data set.}\label{fig:charlieCoherence}
\end{figure}

\begin{figure}[ht]
	\centerfloat{
		\includegraphics[scale=1.0]{byrulevoCoherence}
	}
	\caption{Coherence scores on Byrulevo data set.}\label{fig:byrulevoCoherence}
\end{figure}

\begin{figure}[ht]
	\centerfloat{
		\includegraphics[scale=1.0]{fergusonCoherence}
	}
	\caption{Coherence scores on Ferguson data set.}\label{fig:fergusonCoherence}
\end{figure}

For each data set and topic model a few most coherent topics (topics focused on specific words which have the higher probability to occur in a specific topics) were chosen and can be seen in Tables~\cref{tab:byrulevoTopics,tab:charlieTopics,tab:fergusonTopics}.

As a result of the analysis of the given conflict ad hoc discussions, Biterm Topic Model came out to be the best performing and most stable method based on all coherence measures, baseline Lda model not specialized for working with short text has shown to not be suitable without the additional preprocessing of data mainly due to the data sparsity. However topics identified using this model can still be used to analyze key moments of discussions, participants’ arguments, and as a basis for other data analysis tasks.

Future work involves continuing the work on peer review, the first part of which was done for the Byrulevo data set by students and researchers from the journalism department of SPBU university. It has shown that the best results were obtained by the WNTM model. Comparing results of the models to the manual evaluation will allow us to measure not only the effectiveness of each model but also to compare coherence measures and find which one correlates better with human perception. The different prepocessing methods can also be used to improve the final result, tf-idf can be used instead of regular bag words approach to omit less frequent words more effectively.

\subsection{Topics in the Russian Twitter and relations between their interpretability and sentiment}\label{subsec:ch5/sec2/sub4}

\subsubsection{1. Introduction}

After the principal possibility of topic modelling on such short and noisy texts as tweets has been demonstrated over ten years ago \cite{RamageDumaisLiebling}, it has become a focus of scholarly attention as a method of detection of hidden substance of the tweet corpora, tracking user behavior, and pre-processing to reducing the dimensionality of the corpora. For English, an extensive number of studies have been made, and several algorithms have been proposed. For other languages, though, there have been much fewer experiments, especially of comparative thought.

Despite, by the nature of topic modelling, short and noisy natural-language texts are virtually the worst object for topic detection, and despite the diminishing relevance of Twitter in the media ecosystems of many countries, the attractiveness of Twitter as a source of datasets for modelling remains high -- again, mostly for the English-language cases.

Russian social media have so far received moderate attention in terms of experiments with topic modelling, and the Russian Twitter has not been its primary focus. In particular, topic interpretability has not received, to our viewpoint, enough scholarly attention. Despite topic quality is quite an expansive research area (for a review, see \cite{MavrinFilchenkovKoltcov}), only several studies have addressed the divergence between the objective and subjective topic quality assessment, especially rarely for Russian \cite{BodrunovaKoltcovKoltsova}. The causes of low interpretability of topics for short texts have virtually escaped proper discussion, and no studies have been dedicated to linking topic semantics and their interpretability.

Our work aims at partly covering the gaps described above. We model the topics for a dataset of a conflictual discussion of as early as 2013, as at that moment, according to our previous studies \cite{BodrunovaLitvinenkoBlekanov2017}, the Russian Twitter was not as bot-occupied as it has been since 2016 \cite{StukalSanovichBonneau}. We have chosen to work with conflicts online, as they provide for emotionally loaded texts and link the discussions to a variety of wider issues of user interest.

Earlier, we have tested three topic modelling algorithms, namely unsupervised LDA, BTM, and WNTM, on three raw-data Twitter datasets in English, French, and Russian, including the aforementioned dataset \cite{BlekanovTarasovMaksimov}. But, as it often happens in topic modelling studies, the results must be tested also by human coders. Below, we will describe in short what and why we have done in terms of algorithm development and what we have tested additionally to provide more freedom to human coders.

The remainder of the paper is organized as follows. In Section 2, we review the literature on topic modelling and Twitter studies in Russia and beyond. In Section 3, we formulate the hypotheses, tell of the case under scrutiny, our previous topic modelling experience with the three algorithms, and suggest data amplification for human coders. In Section 4, we describe the method and the research procedures. Section 5 provides the results and discusses them for both the process of human coding and the results based on sentiment analysis. The concluding remarks reformulate our findings against prior knowledge and provide the guidelines for further research.

\subsubsection{2. Topic Modelling for Russian Social Media: A Literature Review}

To our best knowledge, there has so far been no extensive review of how topic modeling has developed for the Russian language. Not aspiring for a truly representative one, we anyway need to review the field to show how scarce attention has so far been given to the relations between the interpretability of modelling results and the topics’ inherent features. This, at least partly, is also true for topic modelling studies for other languages.

For Russian, topic modelling studies may be divided into methodological (that develop, compare, and extend models as well as evaluate their quality), applied (that apply topic modelling to extract the meanings from datasets), and relational (that relate topic modelling results to other features of the datasets or external factors).

\paragraph{A. Methodological studies of topic modelling} 
There are several groups within Russia who have been focusing on various topic modelling algorithms. Thus, in a sequence of influential works, Koltsova and colleagues develop LDA \cite{KoltcovKoltsovaNikolenko2014} and a range of extensions to it. The latter include semi-supervised interval LDA, or ISLDA \cite{BodrunovaKoltcovKoltsova}, granulated LDA \cite{KoltcovNikolenkoKoltsova0516} and LDA with local density regularization \cite{KoltcovNikolenkoKoltsova0916}. In relation to this work, Koltsov and colleagues have optimized Gibbs sampling \cite{KoltcovNikolenkoKoltsova16} and the model itself applying Rényi and Tsallis entropies \cite{Koltcov}. This group of authors has linked the use of topic modelling to qualitative studies of social media and beyond \cite{NikolenkoKoltcovKoltsova} and applied their instruments to several corpora of longer and shorter texts (see below). In these works, Russian is used more as an example of ‘a language as such’, just as English is used in topic modeling, often without discussing inherent linguistic or contextual limitations. Similarly, the works by Vorontsov and colleagues (e.g. \cite{VorontsovFreiApishev1015,VorontsovPotapenko,VorontsovFreiApishev0415}) have been influential in pLSA and its modifications based on non- Bayesian regularization. Among the rest, they have tested two algorithms for the Russian-language short texts, namely biterm topic modelling (BTM) and word network topic model (WNTM) \cite[p.~191]{KochedykovApishevGolitsyn} which we have also employed for our Twitter study (see \cite{BlekanovTarasovMaksimov} and below). The two research groups have collaborated on additive topic models \cite{ApishevKoltcovKoltsova1016,ApishevKoltcovKoltsova16} and have published important methodological papers in Russian (which we do not review here due to the scarcity of space). We will also omit from our review the works that suggest text clustering instruments alternative to topic modelling.

Several other authors have amplified this corpus of works by adding automated labeling to Russian-language topics \cite{MirzagitovaMitrofanova}, showing the possibility of single term extraction by topic modelling \cite{BolshakovaLoukachevitchNokel}, and multi-model tests on optimization of the number of topics \cite{KrasnovSen}.

\paragraph{B. Use of topic modelling for content interpretation: applied and relational works}
Content-exploring research has scrutinized both social media and text corpora beyond them. Thus, topic modelling has been employed to map the agenda of the Russian Livejournal of 2013 \cite{KoltsovaKoltcov} and ethnic contents of the Russian blogs \cite{Nagornyy}, including detection of most hated ethnicities \cite{BodrunovaKoltsovaKoltcov}.

Beyond the social networking realm, LDA has been applied to TV topics \cite{KoltsovaPashakhin}, Russian prose \cite{SedovaMitrofanova}, a corpus of musicological texts \cite{Mitrofanova}, newspaper news texts on climate change \cite{BoussalisCoanPoberezhskaya}, and, along with BTM, to Q\&A queries \cite{VolskeBraslavskiHagen}, with reasonable success.

The works that, above, we have called ‘relational’ are almost exclusively written by Koltsova and colleagues. They have shown how activity in political blogs in 2011-2012 correlated with the politicians’ ratings \cite{KoltsovaShcherbak}; the topicality of Livejournal top blogs corresponded to the structure of the Livejournal co-commenting communities \cite{KoltsovaKoltcovNikolenko}; and the linkage between news topics on TV and user feedback \cite{KoltsovPashakhinDokuka}, among other works.

\paragraph{C. Topic modelling for the Russian Twitter}
There are three methodological works except ours that explore topic modelling for the Russian Twitter \cite{MimnoWallachNaradowsky,Sridhar,GutierrezShutovaLichtenstein}. They all use Russian datasets for developing multilingual modelling tools. The first two do not discuss individual results for any single language, and the third only observes one difference in description of sports between Russian- and English-language Twitter. Both works assess the quality of the models but lack the discussion on human interpretability of the topics.

Similarly, only a small handful of works applies topic modelling to Russian Twitter to detect substantial meanings or discussion features. Thus, one work \cite{ChewTurnley} has shown the divergence between Russian- and English-language ‘master narratives’ on Russian cyber-operations.

Our works appear to the only continuous effort (since 2013) to apply automated text analysis to the Russian Twitter. Thus, we have tested three topic models \cite{BlekanovTarasovMaksimov} (see the details below) and have also applied BTM to detect the dynamics of topicality in conflictual discussions \cite{SmoliarovaBodrunovaYakunin}.

But, so far, there has been no discussion on whether topic models work well for the Russian Twitter in terms of human interpretability and what would be the features that could help in raising it.

\paragraph{D. Quality assessment and interpretability of the Russian-language topics}
An extensive study of nine automated metrics juxtaposed to the human-coding baseline was performed in \cite{Nikolenko}. Based on word2vec approaches suggested earlier, the author shows that normalized PMI (NPMI) outperforms PMI as well as other conventional metrics like tf-idf, and that vector-based metrics work better than all others. But, for short texts, the question remains how wor2vec metrics work with pooled short texts (what is most often done for tweets), this is why we use NPMI, as well as Umass \cite{BodrunovaKoltcovKoltsova}. Another important attempt to introduce a quality metric for longer texts was made in \cite{MavrinFilchenkovKoltcov}.

But none of these works has primarily focused on the issues in human (non-)interpretability of the topics, mostly seeing human coding as a baseline -- perhaps because, for longer texts, when interpretability was at stake, the models performed well enough. Thus, the authors \cite{KoltsovaKoltcov} have shown that, for long texts like Livejournal posts, circa two-thirds of the topics are interpretable after LDA has been applied. They have also identified three types of uninterpretable topics: ‘language’ (other than Russian), ‘style’ (writing styles, including offensive language), and ‘noise’ (uninterpretable texts / combinations of texts) \cite[p.~218]{KoltsovaKoltcov}.

In our pilot studies, though, we have seen that topics for Twitter are less interpretable, and we explore the coders’ experience below. The general features of Russian as a highly inflected language are well-known \cite{Whittaker}; for such languages, several solutions have been suggested \cite{MaucecKacicHorvat}, but it is not the nature of Russian alone that seems to be causing lower topic interpretability in the case of Russian Twitter.

Also, no studies have been done to understand the features of the topics that could raise the topic interpretability. In other languages, first and foremost English, relations between sentiment and topicality of discussions in social media have been explored in a range of works (for a short review, see \cite{XiangZhou}). But, so far, as soon as we know, there have been no works that would explore topic sentiment in relation to human interpretability. Our experiments with sentiment on Twitter in three languages \cite{BlekanovKukarkinMaksimov,BodrunovaBlekanovKukarkin2018}, including Russian \cite{BodrunovaLitvinenkoBlekanov2017,NigmatullinaBodrunova}, show that sentiment of tweets is tightly linked to topicality and user status as influencer. Our intuition is that more interpretable topics are more sentiment-loaded, in particular -- negativity-loaded.

\subsubsection{3. Our Hypotheses and the Case Under Scrutiny}

\paragraph{A. The research hypotheses}
Previously, we have applied three topic modelling algorithms to three datasets from Twitter, all featuring nation- or world-scale conflictual discussions \cite{BlekanovTarasovMaksimov}. We have tested the results with two topic coherence metrics: Umass (as suggested by \cite{MimnoWallachTalley}) and NPMI (as suggested by \cite{StevesonAletras}). There, we have shown, in particular, that, for all the three cases that: 1) BTM outperforms LDA and WNTM for our data; 2) coherence metrics show that topics are extracted well; 3) that the best number of topics for our datasets is 100.

But, as we have stated above, later pilot tests of the models on human coders have demonstrated that the model runs labeled as good by automated metrics do not perform that well when read by humans. Thus, for this study, we turn the testing logic upside down and question the ‘human baseline’ approach, to discuss the shortcomings of the algorithms noted by the human coders. For this, we amplify our coding interface a word-to-topic relevance metric \(\lambda\) suggested by \cite{SievertShirley} and check its application upon a small number of topics with human coders. ‘Playing’ with this function allows to lower the most relevant (and, thus, most frequent) terms among the topic descriptors and bring more specific terms up.

We also link human interpretability to sentiment load of the topic descriptors (30 words).

Thus, we hypothesize that:
\begin{itemize}
	\item H1. For all the three algorithms, lowering \(\lambda\) leads to higher interpretability.
	\item H2. With the best relevance level, BTM outperforms LDA and WNTM in human coding, just as it does by objective metrics.
	\item H3. More interpretable topics are more negativity-loaded.
	\item H4. The correlation between human interpretability and sentiment will be higher for the model that performs best by automated quality metrics.
	\item H5. The correlation between human interpretability and sentiment will be higher for the model that performs best by human coders’ assessment.
\end{itemize}

The two last hypotheses become mutually exclusive if H2 is not proven.

\paragraph{B. The case under scrutiny} 
The case that we work with is a month-long discussion that dates back to October 2013 and describes a conflict between locals and re-settlers from Central Asia in the Moscow district of Biryulevo. The discussion unfolded after a killing of a Russian Egor Scherbakov by an Uzbek re-settler Orham Zeinalov and opened up a nationwide discussion on migration issues like illegal dwelling of immigrants, visas for immigrant workers, cultural differences, and ‘radical white’ Russia-for-Russians opposition to re-settling. The Twitter part of this discussion has immediately reached national trending topics and has sparked participation of local authorities, police special troops, diasporas, international commentators, and NGOs.

The collected corpus of tweets is not very large; it covers October 1 to 31, 2013, and, after cleaning of spam, tweets in languages other than Russian, and links-only tweets, includes 10215 tweets from 3574 users who published them.

It suits our goals by several reasons. First, its dating back to 2013 ensures that it is almost bot-free. Second, it has high conflicting potential, and its tweets are highly emotionally loaded. Third, half of our coders have previously worked with this dataset and could link the issues they experienced in coding to the nature of the discussion already studied in other aspects.

\subsubsection{4. The Research Methods and Conduct of the Study}

For this part of the study, our methods look simple, but we still consider this part of our research crucial for further development. We will describe the methods and the conduct of the research together, to be able to comment on methods use.

\begin{enumerate}
	\item To select the proper meaning of word relevance for further assessment, we have tested the relevance metric \(\lambda\) for the three algorithms on a minor number of topics (30 for each algorithm).
	\item We have hand-coded the 100-topic runs for the three algorithms. The coders were not familiar with the dataset and were just explained the task and provided some knowledge on the details of the case; the trainers belonged to the group that was familiar with the data. Then, for each topic, we have calculated the interpretability score \(K\) of 0 (both coders could not interpret it), 1 (one coder managed to interpret it), or 2 (both coders interpreted it) and checked the Cohen’s Kappa for inter-rater reliability.
	\item We applied our sentiment assigning algorithm \cite{BlekanovKukarkinMaksimov} to the topic descriptors and hand-corrected it. What we were focusing upon was negativity, very characteristic for this discussion. This is why we coded the sentiment in the following way: positive, neutral, mixed, and undefined terms = 0, negative terms (by dictionary) = 1, hate speech including harsh pejoratives and obscene lexicon = 2. For each topic, we have calculated the descriptor negativity score \((N)\), dividing the negativity score by 30 (the number of word descriptors assessed).
	\item We calculated Spearman’s rho for each algorithm juxtaposing the interpretability score and the negativity indices for the topics, to see if there is any correlation between them.
\end{enumerate}

We need to state that this stage of research is preliminary, and our work in progress involves identical tasks for the datasets on similar inter-ethnic conflicts in English, German, and French.

\subsubsection{5. The Results and Their Discussion}

\paragraph{A. H1: term relevance and interpretability}
H1. We have pre-tested \(\lambda\) for our data, to be able to choose its proper meaning for further coding. The results are shown in Table~\cref{tab:threeAlgorithmRelevance}. We have conducted the tests with \(\lambda\) changing from 1 to 0.4 with the step of 0.1, but show only the results for \(\lambda = 1\) and \(\lambda = 0.5\), as 0.5 has behaved as a threshold changing the word descriptors picture the most. Codings for various meanings \(\lambda\) were made with significant time intervals (5 to 8 days), to avoid results skewing.

\begin{table}[ht]%
	\centering
	\caption{Pre-testing the relevance metric \(\lambda\) for the three algorithms, \% of 30 topics.}%
	\label{tab:threeAlgorithmRelevance}% label всегда желательно идти после caption
	%	\begin{adjustbox}{width=1\textwidth}
		%		\small
		\begin{tabular}{ c  c  c  c  c }% Вертикальные полосы не используются принципиально, как и лишние горизонтальные (допускается по ГОСТ 2.105 пункт 4.4.5) % @{} позволяет прижиматься к краям
			\toprule
			\makecell[c]{Model} & \multicolumn{2}{c}{\makecell{\(\lambda = 1\)}} & \multicolumn{2}{c}{\makecell{\(\lambda = 0.5\)}} \\
			\cline{2-5}
			 & Coder1 & Coder2 & Coder1 & Coder2 \\
			 \hline
			 LDA & 53\% & 63\% & 40\% & 33\% \\
			 WNTM & 76\% & 73\% & 70\% & 67\%\\
			 BTM & 76\% & 83\% & 69\% & 75\% \\
			\bottomrule
		\end{tabular}%
		%	\end{adjustbox}
\end{table}

As Table~\cref{tab:threeAlgorithmRelevance} shows, H1 proves wrong for all the models. The general line in interpretability is that it either gradually declines with lowering  \(\lambda\) or keeps steady and then sharply drops at circa 0.55 to 0.45. This is perfectly logical if we consider the idea of word relevance itself, but it does not correspond to how the instrument is intended to work. In most cases, well-interpretable topics remained in place; only rarely, one or another coder found some dubious topics more interpretable than before, but mostly, the results were negative, For some good topics, unfortunately, the most frequent words remained in place, while the words \#10 to \#20 changed, completely altering the meaning of the topic to the worse in terms of interpretability. We need more experiments with bigger number of topics and coders, to detect the optimal  \(\lambda\) that would help avoid the shifts of meaning and at the same time eliminate the frequent terms. So far, we will use \(\lambda = 1\) for further coding, despite a relatively big number of frequent terms in the top of the topic descriptors. For a number of topics in WNTM, though, we have seen significant improvement in interpretability where the term relevance change worked exactly as expected.

\paragraph{B. H2: interpretability of the topics with \(\lambda = 1\)}
The aggregated results of the human coding of the topics are represented in Table~\cref{tab:topicHumanInterpretability}.

\begin{table}[ht]%
	\centering
	\caption{Human interpretability of the twitter topics, \% of 100 topics.}%
	\label{tab:topicHumanInterpretability}% label всегда желательно идти после caption
	%	\begin{adjustbox}{width=1\textwidth}
		%		\small
		\begin{tabular}{ c  c  c  c  c }% Вертикальные полосы не используются принципиально, как и лишние горизонтальные (допускается по ГОСТ 2.105 пункт 4.4.5) % @{} позволяет прижиматься к краям
			\toprule
			Model & Coder1 & Coder2 & Metacoding \((K = 2)\) &  Cohen’s Kappa \\
			\hline
			LDA & 66\% & 52\% & 46\% & 60\%; 0,109 \\
			WNTM & 66\% & 56\% & 34\% & 64\%; 0,277 \\
			BTM & 68\% & 53\% & 43\% & 72\%; 0,437 \\
			\bottomrule
		\end{tabular}%
		%	\end{adjustbox}
\end{table}

As we see from Table~\cref{tab:topicHumanInterpretability}, the difference between the coders in terms of the number of the interpretable topics was not that big (14\% of topics maximum). Coder 1 has shown the levels of interpretability similar to those reported for longer texts \cite{KoltsovaKoltcov}, while Coder 2 could not reach 60\% with any of the algorithms.

For the number of well-interpretable topics, H2 must be rejected as well, as LDA outperformed both WNTM and BTM in \(K = 2\). But, at the same time, BTM shows much higher inter-coder agreement, and this makes us conclude that, for H2, we have mixed evidence. For BTM and WNTM, there were more topics that were filtered out by both coders (14 for LDA; 27 for WNTM; 33 for BTM, 5 of which by language). This makes us pose a question whether high number of well-interpretable topics must always be the goal of topic modelling, especially for short texts. Instead, a smaller number of ‘ideal’ (robust, salient, and well- interpretable) topics might be sought for.

\paragraph{C. Qualitative topic assessment: interpretability issues}
Before going any further, we feel necessary to describe the difficulties that the coders have encountered. Some of them fall into the categories of topic fallacies described previously for LDA \cite{BoydGraberMimnoNewman}. Among them, there are general words, identical topics and mixed/chained topics that involve several chains of semantically related terms.

But, for Twitter, these problems have a specific face. Thus, for an issue-based discussion composed of short texts, even well-interpretable topics are problematic. They seem to be mostly based on the corpus of frequent terms of the discussion, like ‘migrant’, ‘situation’, ‘power’, ‘killer’, ‘detained’ etc., and some additional topic descriptors attracted to them by the model. The frequent lexicon is responsible for the coder’s feeling of comprehensibility, while the attracted lexicon contributes to the new meaning. But altogether this creates the sense of high similarity of the topics; the coders have complained on similarity of multiple topics rather than on non-comprehension. Also, the frequent terms blur the meaning of topics. Here, we see a possible decision in defining topic clusters (sub-themes) instead of individual topics, based on topic similarity metrics such as Jaccard or Kullback-Leibler indices.

Another issue is that, often enough, the topics seem to be formed by one popular (highly retweeted) tweet. Such tweets, allegedly, drag the users’ pools to each other and start the topics, further attracting to the topics the tweets of the respective users, rather than the topically similar tweets. Here, the problem might lie in the pooling technique, as we have used all tweets by a given user as ‘one text’. To resolve this issue, pooling vs. non-pooling must be tested.

And yet another problem lies in the case specificity. Working with a particular event- or issue-based discussion demands a certain level of contextual knowledge, without which one cannot assign meanings to the topic descriptors. Thus, during pre-tests, the members of our working group could interpret more topics than the coders, as the nature of the topics was details-dependent. This poses a wider problem of how the research community should treat interpretability of a topic in general. For longer texts from non-case-specific corpora, topics appear to be more distinguishable from each other, as they, indeed, relate to very different themes in users’ online speech. But defining sub-topics within a highly thematized corpus demands a different understanding of topic interpretability, coherence, and distance. Also, we recommend that, for coding thematic discussions, the coders become familiar with the theme and its context, and not by preliminary reading but from other sources.

\paragraph{D. Negativity in topics and its relation to interpretability}
To test H3, we have conducted Spearman’s test between topic interpretability score \((K)\) and topic negativity score \((N)\). For all the three algorithms, H3 is proven, with relatively high Spearman’s correlation values: for LDA, 0,375**; for WNTM, 0,383**; for BTM, 0,494. This proves H3 and demands further research on whether negatively loaded topics show higher interpretability for datasets of other nature -- non-thematic, non-conflictual, platform-diverse.

We also see that the Spearman’s value is the highest for BTM which proves our H4. It also proves H5 if we consider inter-coder reliability, but H5 is rejected if we look at \(K\) for each algorithm.

\subsubsection{6. Conclusion}

This paper describes the work in progress upon a dataset from the Russian Twitter; we amplify earlier topic modelling results and objective topic quality measurement with a discussion on human interpretation of the modelling results. Our results contribute to the field of the topic modelling studies in three ways. We, first, show that term relevance tool works only partly the way that could help raise interpretability. Second, we see that automated quality assessment does not always correspond with how coders assess the topics, and designate a range of problematic issues in coding. Third, we show that negativity in topic descriptors might be related to interpretability of the topics for human coders, which is not captured by automated assessment of topic detection quality.

More generally, we can cautiously claim that the difficulties with Russian as a highly inflected language are not what cause the problems in topic detection and interpretation. Rather, it is the algorithmic imperfection, dataset preparation technique, and context dependence that altogether cause lower interpretability of short texts that, in addition, are noisy and ‘oral-written’ \cite{Lutovinova}.

We, at least partly, support what was earlier reported on the number of interpretable topics in Russian-language datasets but pose the question of what is more important for Twitter -- a higher number of interpretable (but identical) topics or a better-defined (but smaller in quantity) topics in the run, as this question seems crucial for Twitter studies.

Our research will continue in the areas designated above, including stretching our method to three other languages, experimenting with term relevance, and exploring the nature of impact of negativity upon topic interpretation.

\section{Методы анализа тональности контента}\label{sec:ch5/sect3}

\section{Методы суммаризации}\label{sec:ch5/sect4}

\FloatBarrier

