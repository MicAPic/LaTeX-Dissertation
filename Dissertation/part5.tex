\chapter{Интеллектуальных анализ данных Web-ресурсов}\label{ch:ch5}

\section{Анализ Web-графов}\label{sec:ch5/sect1}

\subsection{Beyond left and right: Real-world political polarization in twitter discussions on inter-ethnic conflicts}\label{subsec:ch5/sec1/sub1}

\subsection{Please Follow Us: Media roles in Twitter discussions in the United States, Germany, France, and Russia}\label{subsec:ch5/sec1/sub2}

\subsection{Multi-dimensional echo chambers: Language and sentiment structure of Twitter discussions on the Charlie Hebdo case}\label{subsec:ch5/sec1/sub3}

\subsubsection{1. Introduction}

Public discussions on social networks potentially have trans-border and multilingual nature. This comes true in heated conflictual discussions that reach global trending topics. Such discussions are expected to demonstrate ‘civilizational clashes’ \cite{AnKwakMejova}.

Being part of the global public sphere, since the 1990s, such discussions were expected by many observers to be more horizontal, all-involving, and democratically efficient \cite{Fuchs} than the traditional mass-mediated discussions \cite{McQuail}. But, with time, criticism towards the democratic quality of discussions in social media arose, with many works discovering the patterns of echo chambering and discourse polarization in social networks \cite{Sunstein2001,Sunstein2002,BarberaJostNagler,BastosMerceaBaronchelli,ColleoniRozzaArvidsson,ConoverRatkiewiczFrancisco}, which lowered the capacities of inter-group discussions and, thus, just formed an additional line of social segregation.

Object-oriented hashtagged discussions have been thoroughly studied in the 2010s, including those on political and social conflicts. But there is still scarce knowledge on whether affective hashtags \cite{Papacharissi} that convey emotions -- either of solidarity with or of anger towards a particular social group -- work in terms of user clusterization. Also, there is no clear understanding of comparative democratic quality of emotionally ‘positive’ and ‘negative’ hashtags in terms of echo chambering.

In this paper, we address these gaps by analyzing the Twitter discussion on the \textit{Charlie Hebdo} massacre of 2015. In the discussion upon the mass killings, the Twittershpere has created \#jesuischarlie and \#jenesuispascharlie -- two emotionally differing discussion clusters with, allegedly, opposite sentiments towards the journal’s ethics and freedom of speech; the hashtags soon became ‘role models’ for online solidarity towards the victims of terrorist attacks and anthropogenic disasters.

To analyze the echo chambering patterns in the two discussions, we have focused upon two levels of echo chambers. We were wondering whether echo chambers formed on the level of a hashtag (based on language use) and within a particular language (based on user sentiment of French-speaking users).
The remainder of the paper is organized as follows. Section 2 reviews the literature on echo chambering in social media. Section 3 presents our methodology and the conduct of the research. Section 4 presents our results and discusses them.

\subsubsection{2. User Groupings on Twitter and the Efficacy of Public Sphere}

\paragraph{2.1 Social Media and the Public Sphere: Echo Chambers vs. Opinion Crossroads}
Public sphere as a spatial metaphor for a complex of discussions and procedures with a public status and decision-making goals \cite{Kleinstuber} has been amplified by the appearance of social media in the 2000s. By 1990s, it had been established in the academic literature that mediatized public sphere with traditional media playing the role of information hubs was uneven and hardly efficient in terms of access to opinion expression, as whole social groups remained under-represented, and newsmakers privileged in comparison to \textit{vox populi}. With the appearance of social media, hopes arose that the new communicative milieus would foster horizontalization of communication and provide for democratization and higher political participation \cite{Fuchs}. Also, hopes for better understanding and resolution of non-political inter-group conflicts existed.

But with time, these hopes fainted, as offline disparities seemed to reproduce online, including political interests, race, gender, and other inequalities \cite{Daniels}; emotion and affect proved to rule the discourse \cite{Papacharissi}, with publics even in most democratically developed countries moving from diverse in opinion to dissonant and disconnected \cite{Pfetsch}. With the development of social network analysis (SNA) and its application to social media research, the question of the efficacy of the public discussions in social media \cite{BrunsHighfield} became linked to network and structural features of the discussions, such as the influencer status \cite{BodrunovaBlekanovMaksimov,BodrunovaLitvinenkoBlekanov2016} and clusterization of users also known as user polarization \cite{BarberaJostNagler,BastosMerceaBaronchelli} and echo chambering \cite{ColleoniRozzaArvidsson,ConoverRatkiewiczFrancisco}. Some evidence was also gathered on discussion sphericules forming on the global scale just as well as nationally \cite{CammaertsAudenhove}.

\paragraph{2.2 Why the Twitter Discussions Fragment: Linguistic Properties of Speech as Catalyzers of Echo Chambering}
In early studies of social networks and its users, authors interested in testing the ability of networks to pull together users from distant locations and weak ties linked geo- graphical distance with factors like residence of users and their language profile \cite{TakhteyevGruzdWellman}. This is why Twitter that enabled the (arguably) quickest possible information spread across locations and languages became a major attractor of scholarly attention \cite{LotanGraeffAnanny,HongConvertinoChi}. But despite the global reach of the platform, several studies have found that people were still connected locally on Twitter \cite{CammaertsAudenhove,YardiBoyd}.

Along with locality and residence, linguistic factors, arguably, play a major role in user grouping on the global scale. Thus, the language(s) used by the discussion participants is the first natural barrier that is expected to make users group together and communicate within their language-based echo chambers \cite{ChenTuZheng}, both on Twitter on the whole and within particular hashtags \cite{BastosPuschmannTravitzki}.

Other factors have also been discussed as the catalyzers of user grouping on Twitter. Among those, political attitudes lead the research agenda \cite{BarberaJostNagler,BastosMerceaBaronchelli}. Here, several ways to detect user clusterization exist. Of them, use of network or semantic proxies like friendship ties \cite{BarberaRivero}, patterns of following \cite{Rivero} and retweeting \cite{CalaisGuerraMeiraJrCardie}, content sharing \cite{ColleoniRozzaArvidsson,BakshyMessingAdamic} etc. is till today the most prominent; another is automated analysis of user sentiment, either in general or toward an issue/actor in question (object-oriented) \cite{ConoverGoncalvesRatkiewicz}.

But all these studies depict user groupings within a single dimension; our idea is to try and trace user groupings multi-dimensionally -- both on the level of a hashtag (based on language use) and within a language nebula (based on user sentiment).

\paragraph{2.3 The \textit{Charlie Hebdo} Case: Emotional Hashtags and User Groupings}
To search for multi-dimensional echo chambers, we have chosen the case of the \textit{Charlie Hebdo} massacre of 2015. Here, we could hypothesize that the existence of emotionally opposite hashtags (\#jesuischarlie and \#jenesuispascharlie) already creates enclaves within the general discussion on the case. Then, within the hashtags, several clusters based on language structure may exist. Then, on the third level, we will look whether within the language clusters sub-clusters of sentiment form. In general, our idea is to see how exactly the language clusters correspond to the sentiment clusters in each language and whether ‘positive’ users within one language are linked to such in another language, while ‘negative’ users also group across languages in a similar way. But here we present only preliminary results that check if the user clusters may at all be detected based on language and on sentiment within a language.

\paragraph{2.4 Research Hypotheses}

Thus, our hypotheses are the following:
\begin{itemize}
	\item H1a. Non-random user groups will be detected for both hashtags, as based on
	language use.
	\item H1b. \#jesuischarie will not differ from \#jenesuispascharlie in their language
	structure, as both hashtags have reached global trending topics and are expected to show ‘civilizational clashes’.
	\item H2a. Non-random user groups will be detected within one language (French), as based on positive and negative sentiment.
	\item H2b. \#jesuischarlie will differ from \#jenesuispascharlie in the grouping based on user sentiment, due to the emotional opposition of the hashtags themselves.
	\item H3. Multi-layer echo chambering may be detected in trans-border hashtagged discussions of global reach.
\end{itemize}

\subsubsection{3. Data Collection and Conduct of Research}

\paragraph{3.1 Data Collection and the Datasets}
Using a web crawler developed especially for the Twitter data collection, we gathered all the tweets published openly under the hashtags \#jesuischarlie and \#jenesuispascharlie (by separate crawls) within January 7 to 9, 2015, as these three days covered the active conflict (from the killings in the editorial office to the assailants’ death) when the users provided virtually millions of tweets for collection.

The collected datasets included: for \#jesuischarlie: 420,080 tweets; 266,904 tweeters; 719,503 users who interacted with the posted tweets (by likes, retweets, or comments); for \#jenesuispascharlie: 7,698 tweets; 5,466 tweeters; 17,872 users who posted and interacted with the posted tweets (by likes, retweets, or comments).

These full datasets were later used to reconstruct the overall web graphs for the two hashtags. But the datasets were very different in size, and be able to color them with language markers, we needed to sample the users for coding having in mind the volume difference of the datasets.

\paragraph{3.2 Language Analysis: Sampling, Coding, and Graph Reconstruction} 
To answer H1a and H1b, we have coded the users for their language use and then applied these data to the datasets for web graph reconstruction.

After eliminating bots and bot-like users (those who posted over 60\% of doubled tweets) as well as hashtag-only tweets, we have followed the strategy developed by the research group for previous Twitter studies \cite{Authors2016a,Authors2016b,Authors2018}, namely uniting random sampling with detection of influential users (influencers) for taking them into account. Then, we have coded all the influencers (disregarding the number of tweets they posted; for \#jesuiuscharlie, 402 users, for \#jenesuispascharlie, 85 users) and ‘ordinary users’ sampled in the feasible and comparable way. For \#jenesuispascharlie that was substantially smaller, all the users with 3 and more tweets were coded (339 users); for \#jesuischarlie, the ‘ordinary users’ with 5 tweets or more were taken into account (9,090 users), and of them, each second was coded (4500 users).

All the sampled underwent expert reading and were coded manually marking the number of tweets in language 1, language 2, and other languages; thus, users posting on one, two, and three or more languages were defined. The languages were identified for each user; in case of rare languages, Yandex language identifier was employed.

To reconstruct the graphs, we use Gephi API algorithms openly available online. Of the available algorithms, two were chosen: Hu \cite{Hu} and OpenOrd \cite{MartinBrownKlavans}; here, YifanHu-based graphs are presented, as the OpenOrd graphs require more space for presentation. We colored both the nodes (users) and the edges (connections between users). To prove that the visual nebulae are not artifacts of subjective viewership, we calculated the percentage of edges between and inside language groups, eliminating the ‘loops’ of self-commenting/liking by the users.

\paragraph{3.3 Sentiment Analysis: Sampling, Vocabulary Building, and Graph Reconstruction}After we have proved that the French-speaking users show non-random grouping in both cases (see below), we have taken them for sentiment analysis. The number of users for \#jesuischarlie included 1291 user; for \#jenesuispascharlie, 117 users.

Our strategy for French-language sentiment analysis was the following. We have united three sources in our vocabulary: the existing French dictionary with sentiment marking, machine translation from an additional Wordnet vocabulary, and the case-based vocabulary created from the collected tweets and manually marked for positive, negative, and neutral sentiment regardless of the case-specific meanings.

This vocabulary was applied to each tweet of the abovementioned French-speaking users; for each tweet, the sentiment was calculated. Then, the thresholds were defined: positive and negative were the users with positive(+neutral) and negative(+neutral) tweets, respectively; neutral were those with neutral tweets only; mixed were those with positive + negative(+neutral) tweets.

Then, we also checked the groupings with by calculating the percentage of edges both between and inside language groups, eliminating the ‘loops’ of self-commenting/liking by the users.

\subsubsection{4. Results and Discussion}

Our results are described below with regard to the hypotheses stated above.

\textit{H1a/H1b.} To assess the user groupings in both hashtagged discussions, we have reconstructed the web graphs for the coded users (see Fig.~\cref{fig:yifanHuGraphs-2} for \#jesuischarlie and Fig.~\cref{fig:yifanHuGraphs-2} for \#jenesuispascharlie, respectively). What we see on the graphs are three nebulae for~\cref{fig:yifanHuGraphs-1}: French, English, and other European, and two for~\cref{fig:yifanHuGraphs-2}: French and Engish. But the results of calculations of percentage of edges between and inside groups tell that the actual grouping is slightly different from what we see with unaided eyes. For \#jesuischarlie, the nebulae with density higher than the inter-group ones are French, English, and French/English (52.1\%, 16.7\%, and 16.9\%, respectively, against 6.17\% for inter-group edges) and not other European. For \#jenesuispascharlie, the graph is much denser (26\%), but still the same three clusters show up, with 26.2\%, 22.4\%, and 18.8\%, respectively; in both cases, other language clusters are virtually non-existent and do not mount to 3\%. As we stated in our earlier investigations \cite{Authors2018}, we have not seen a sign of ‘civilizational clashes’ in any of the hashtags.

\begin{figure}[ht]
	\centerfloat{
		\hfill
		\subcaptionbox[List-of-Figures entry]{\label{fig:yifanHuGraphs-1}}{%
			\includegraphics[width=0.419\linewidth]{yifanHuGraphs1}}
		\subcaptionbox{\label{fig:yifanHuGraphs-2}}{%
			\includegraphics[width=0.4\linewidth]{yifanHuGraphs2}}
		\hfill
	}
	\caption{The YifanHu graphs (fragments) for language distribution in \cref{fig:yifanHuGraphs-1} \#jesuischarlie and \cref{fig:yifanHuGraphs-2} \#jenesuispascharlie. Red: French; blue: English; lilac: French/English; green: other European. (Color figure online)}\label{fig:yifanHuGraphs-12}
\end{figure}

Thus, H1a is proven; H1b is proven too but not due to ‘civilizational clashes’.

\textit{H2a/H2b.} To see the user grouping and sentiment cleavages within the French-speaking parts of the discussions, we have reconstructed the web graphs for them (see Fig.~\cref{fig:yifanHuGraphs-3} for \#jesuischarlie and Fig.~\cref{fig:yifanHuGraphs-4} for \#jenesuispascharlie, respectively).

\begin{figure}[ht]
	\centerfloat{
		\hfill
		\subcaptionbox[List-of-Figures entry]{\label{fig:yifanHuGraphs-3}}{%
			\includegraphics[width=0.419\linewidth]{yifanHuGraphs3}}
		\subcaptionbox{\label{fig:yifanHuGraphs-4}}{%
			\includegraphics[width=0.4\linewidth]{yifanHuGraphs4}}
		\hfill
	}
	\caption{The YifanHu graphs (fragments) for language distribution in \cref{fig:yifanHuGraphs-3} \#jesuischarlie and \cref{fig:yifanHuGraphs-4} \#jenesuispascharlie. Red: French; blue: English; lilac: French/English; green: other European. (Color figure online)}\label{fig:yifanHuGraphs-34}
\end{figure}

Here, H2a should be rejected for \#jesuischarlie and partly supported for the second hashtag. Both in the graph and in the edge percentage calculations, it is only users with mixed sentiment who form a group (38.33\% against 56.05\% for the inter-group connections) in \#jesuischarlie. But for \#jenesuispascharlie, both mixed and negative user groups seem to have a potential for grouping (19.5\% and 15\% against 62.1\% for inter-group connections). Thus, H2b is supported: the cases do differ.

Even with the cases of such different sizes, H3 is supported: thanks to the negative nebula in \#jenesuispascharlie, we can state that echo chambers are able to form on at least two levels of the trans-border conflictual discussions of global (or, more precisely, macro-regional) reach. This adds to our understanding of the nature of public discussions in social media, even if lowers hopes for all-encompassing public spheres.

\section{Тематическое моделирование контента}\label{sec:ch5/sect2}

\subsection{Topic modeling of conflict ad hoc discussions in social networks}\label{subsec:ch5/sec2/sub1}

\subsubsection{Introduction}

As a result of different media platforms achieving a steady user growth in a recent years more and more people begin to use different social networks as the main source of news on economical, political and social events. In particular, ad hoc discussions emerged which can be defined as a debate about a specific problem. In most cases such discussions appear in the case of controversial events and involve large number of participants.

Presence of such user activity raises the problem of analyzing large volumes of this type of data which has become one of the most important problems in many data analysis tasks, including topic modeling. Topic modeling algorithm in this case is an algorithm that, given the number of topics and the list of user messages can output two distributions: topics over documents and of words over topics. Such algorithm can help in the understanding of different points of view and highlight the main arguments. This can be useful in many ways one of which is the case in which the number of documents is big and we want to know their general content without reading them all. Another use case is data prepossessing, reducing the dimensions of data to use in other analysis tasks such semantic analysis. However directly applying traditional topic models like LDA and PLSA to short texts can be problematic primarily due to the sparsity of data given the specificity of short texts. In this paper we are studying the usage of different models on a large scale data which can effectively infer hidden topics in big discussions taking these features into account.

\subsubsection{Prior Work}

Early studies of the problem of topic modeling on short texts mainly concerned the use of external knowledge to improve the representation of text data. For example, Phan and others \cite{HoriguchiPhanNguyen} used the modeling of those short texts based on the traditional topic model, the effectiveness of which was tested on a large-scale data set for the purpose of short texts classification. In these works, it is assumed that the use of data derived from long texts could help improving the model for short texts. However, these methods are effective only when the auxiliary data are closely related to the original data. In the case of texts, obtained from social networks this task is impossible due to most of user messages being self-contained. Another assumption is based on the use of different aggregation methods. In the case of data, obtained from the Twitter, user messages can be aggregated by authors, publication time and hashtags. The \cite{WrayLexingRishabh} shows that the best performing method is based on hashtag based aggregation, however it can’t be applied then the texts themselves are collected using a series of hashtags (which is one of better ways for collecting data on big events). Author based aggregation can also be unreliable since the majority of users will have very few messages on any given topic. In this paper we are focusing on models, relying on statistical information about the data.

\subsubsection{Topic Models}

\paragraph{LDA.} Latent Dirichlet Allocation is a three-level hierarchical Bayesian model in which each element of the collection is modeled as a finite distribution over the set of topics. Each topic, in turn, is modeled as an infinite mixture by the set of topic probabilities \cite{MichaelJohnDavidAndrew}.

Lda assumes the following generative process:
\begin{itemize}
	\item Choose \(\theta_i \sim \textit{Dir}(\alpha)\)
	\item Choose \(\phi_i \sim \textit{Dir}(\beta)\)
	\item For every word position \(i, j\):
	\begin{itemize}
		\item Choose a topic \(z_{i, j} \sim \textit{Multinomial}(\theta_i)\)
		\item Choose a word \(w_{i, j} \sim \textit{Multinomial}({\phi_z}_{i,j})\)
	\end{itemize}
\end{itemize}
The model parameters \(\alpha\) and \(\beta\) are typically chosen sparse for better performance on short texts. In this paper, LDA is used as a baseline model, the effectiveness of which for standard texts has been proven both theoretically and in many experimental results.

\paragraph{BTM.} Biterm Topic Model performs topic modeling task by modeling a set of biterms (unordered word pair cooccurring in a short context). The main idea is that if two words co-occur more frequently, they are more likely to belong to a same topic \cite{YanyanJiafengXueqi}.

Btm assumes the following generative process:
\begin{itemize}
	\item Draw \(\theta_i \sim \textit{Dir}(\alpha)\)
	\item For each topic \(k\):
	\begin{itemize}
		\item draw \(\phi_k \sim \textit{Multinomial}(\beta)\)
	\end{itemize}
	\item For each biterm \(b_i\):
	\begin{itemize}
		\item draw \(z_i \sim \textit{Multinomial}(\theta)\)
		\item draw \(w_{i,1},w_{i,2} \sim \textit{Multinomial}({\phi_z}_i)\)
	\end{itemize}
\end{itemize}

As BTM does not model documents explicitly, we must provide a way to infer the topics in a document, i.e., evaluating the topic posterior. Using the chain rule the following equation was obtained:
\begin{equation}
	\label{eqn:29}
	P(z \mid b) = \sum P(z \mid b_i) P(b_i \mid d)
\end{equation}

Where \(P (z \mid b_i)\) can be obtained using via Bayes’ formula based on the parameters learned in BTM and \(P(b_i \mid d)\) can be calculated using empirical distribution of words in a document.

\paragraph{WNTM.} Word Network Topic Model’s idea is based on the following observations. When the texts are short, the word-document space is very sparse, but the word-word space still contains a large number of non-zero elements. Since the topic distribution for each doc- ument can not be recognized accurately in short or unbalanced texts, instead WNTM uses the topic distribution for each word \cite{KeYuanJichang}. Therefore, WNTM studies the distribution by topics for words, rather than those for documents. Studying the topics of the word, rather than those of the document make WNTM less sensitive to the length of the document. In addition, a network of words can be built with any type of text, which makes the WNTM model simple and universal in real applications, unlike other models, such as the mixture of unigrams \cite{ThrunMitchellNigam} and BTM.

The generative process of the model is in many respects similar to that of the LDA, but due to the use of a different distribution it has its own features:
\begin{itemize}
	\item For every latent word group \(z\) choose \(\phi \sim Dir(\beta)\)
	\item Choose \(\vartheta_i \sim \textit{Dir}(\alpha)\) distribution of a latent word group for adjacent word list \(L_i\) for word \(w_i\)
	\item For every word \(w_j \in L_i\):
	\begin{itemize}
		\item Choose a latent word group \(z_j \sim \vartheta_i\) 
		\item Choose an adjacent word \(w_j \sim {\phi_z}_j\)
	\end{itemize}
\end{itemize}

\begin{figure}[ht]
	\centerfloat{
		\includegraphics[scale=1.0]{wntmGeneration}
	}
	\caption{Generation of word network for WNTM model \cite{KeYuanJichang}.}\label{fig:wntmGeneration}
\end{figure}

Similarly to BTM this model can’t be directly applied to get topic distributions over documents. To get the topics in the document, we assume that the proportions of the words generated by the document is equal to the proportions of the document topics, that is:
\begin{equation}
	\label{eqn:30}
	P(z \mid d) = \sum P(z \mid w_i) P(w_i \mid d)
\end{equation}
Where \(P(z \mid w_i\)) equal \(\vartheta_{i,z}\), obtained in the generative process of WNTM and \(P(w_i \mid d)\) obtained as an empirical distribution of words in documents.

\subsubsection{Experiment}

The experiment was conducted to evaluate the quality of these three models, on three sets of ad hoc discussions. The following coherence measures were used to test the effectiveness of each model:

\begin{itemize}
	\item UMass \cite{MimnoWallachTalley} measures how often a common word of each topic is in average a good predictor for a less common word.
	\item NPMI \cite{StevesonAletras} is a normalized version of pointwise mutual information.
\end{itemize}

\paragraph{Data sets.} In this work models were tested on data, collected on three ad hoc discussions from Twitter social network: Riots in Biryulevo (Russia), October 2013 \cite{BodrunovaLitvinenkoBlekanov}, Ferguson unrest (USA), August 2014 \cite{SmoliarovaBlekanovBodrunova} Charlie Hebdo shooting (France), January 2015 \cite{SmoliarovaBlekanovLitvinenko}. The data was crawled based on hashtags in user messages.

\textit{Byrulevo. Riots in Biryulevo}
\begin{itemize}
	\item Total number of user messages: 10215
	\item Total number of users participated in the discussion: 11429
	\item Surveyed time period: 1.10.2013 - 31.10.2013
	\item Number of users who published tweets in the period under consideration: 3574
\end{itemize}

\textit{Ferguson. Ferguson unrest}
\begin{itemize}
	\item Total number of user messages: 193812
	\item Total number of users participated in the discussion: 169677
	\item Surveyed time period: 22.08.2014 - 31.08.2014
	\item Number of users who published tweets in the period under consideration: 70018
\end{itemize}

Charlie Hebdo. Charlie Hebdo shooting
\begin{itemize}
	\item Total number of user messages: 505069
	\item Total number of users participated in the discussion: 952615
	\item Surveyed time period: 07.01.2015 - 10.01.2015
	\item Number of users who published tweets in the period under consideration: 238491
\end{itemize}


\begin{table}[ht]%
	\centering
	\caption{Topics for Byrulevo data set.}%
	\label{tab:byrulevoTopics}% label всегда желательно идти после caption
	%	\begin{adjustbox}{width=1\textwidth}
		%		\small
		\begin{tabular}{ c  c  c  c }% Вертикальные полосы не используются принципиально, как и лишние горизонтальные (допускается по ГОСТ 2.105 пункт 4.4.5) % @{} позволяет прижиматься к краям
			\toprule
			Topic 1 & Topic 2 & Topic 3 & Topic 4 \\
			\hline
			\multicolumn{4}{c}{\makecell{LDA}} \\
			migrant & warehouse & broadcast & riot  \\
			Zeynalov & work & live & Manezhka \\
			police & man & moscow & moscow \\
			murder & boutique & photo & migrant \\
			Sherbakov & moscow & find & block \\
			\hline
			\multicolumn{4}{c}{\makecell{WNTM}} \\
			Moscow & news & Moscow & police \\
			event & Sherbakov & OMON & authorities \\
			Russia & migrant & Zeynalov & killer \\
			riot & murder & arrest & russian \\
			mayhem & killer & Sherbakov & meetings \\
			\hline
			\multicolumn{4}{c}{\makecell{BTM}} \\
			citizen & OMON & Sherbakov &  russian \\
			police & warehouse & Zeynalov & government \\
			local & police & killer & riot \\
			riot & arrest & arrest & migrant \\
			Moscow & killer & moscow & news \\
			\bottomrule
		\end{tabular}%
		%	\end{adjustbox}
\end{table}

\begin{table}[ht]%
	\centering
	\caption{Topics for Charlie Hebdo data set.}%
	\label{tab:charlieTopics}% label всегда желательно идти после caption
	%	\begin{adjustbox}{width=1\textwidth}
		%		\small
		\begin{tabular}{ c  c  c  c }% Вертикальные полосы не используются принципиально, как и лишние горизонтальные (допускается по ГОСТ 2.105 пункт 4.4.5) % @{} позволяет прижиматься к краям
			\toprule
			Topic 1 & Topic 2 & Topic 3 & Topic 4 \\
			\hline
			\multicolumn{4}{c}{\makecell{LDA}} \\
			policia & die & police & islam \\
			Paris & satire & shooting & religion \\
			terroristas & cartoonist & attack & youngest  \\
			sospechosos & frankreich & suspects & local \\
			ataque & attentater & update & extrimists \\
			\hline
			\multicolumn{4}{c}{\makecell{WNTM}} \\
			attack & suspects & cartoonists & media\\
			french & police & support & cartoons  \\
			today & two & editor & toxic \\
			terror & attack & respond & image \\
			killed & hostage & journalism & caricatures \\
			\hline
			\multicolumn{4}{c}{\makecell{BTM}} \\
			police & french & victims & muslims\\
			suspects & shooting & solidarity & islam \\
			hostage & gunman & attack & must \\
			killed & dead & France & say\\
			breaking & killed & jesuischarlie & religion \\
			\bottomrule
		\end{tabular}%
		%	\end{adjustbox}
\end{table}

\begin{table}[ht]%
	\centering
	\caption{Topics for Ferguson data set.}%
	\label{tab:fergusonTopics}% label всегда желательно идти после caption
	%	\begin{adjustbox}{width=1\textwidth}
		%		\small
		\begin{tabular}{ c  c  c  c }% Вертикальные полосы не используются принципиально, как и лишние горизонтальные (допускается по ГОСТ 2.105 пункт 4.4.5) % @{} позволяет прижиматься к краям
			\toprule
			Topic 1 & Topic 2 & Topic 3 & Topic 4 \\
			\hline
			\multicolumn{4}{c}{\makecell{LDA}} \\
			police & MikeBrown & militarization & Miami  \\
			life & black & police & overtown \\
			surrender & brown & law & America \\
			must & racism & reason & vote  \\
			dissa & justice & end & jail \\
			\hline
			\multicolumn{4}{c}{\makecell{WNTM}} \\
			MikeBrown & CNN & must & movement  \\
			amp & cops & surrender & speak \\
			police & Times & police & join  \\
			black & black & dissa & support \\
			people & shooting & see & now\\
			\hline
			\multicolumn{4}{c}{\makecell{BTM}} \\
			must & join & community & pd\\
			police & movement & Miami & look \\
			surrender & now & support & closer  \\
			dise & speak & lot & MikebBrown  \\
			click & die & overtown & msnbc \\
			\bottomrule
		\end{tabular}%
		%	\end{adjustbox}
\end{table}

\paragraph{Results.} Coherence measures were calculated and plotted to estimate the models’ effectiveness on different number of topics. The results (Figures~\cref{fig:charlieCoherence,fig:byrulevoCoherence,fig:fergusonCoherence}) allow us to talk about the approximate number of topics that is optimal for this task, but due to imperfection of quality indicators it requires manual clarification.

\begin{figure}[ht]
	\centerfloat{
		\includegraphics[scale=1.0]{charlieCoherence}
	}
	\caption{Coherence scores on Charlie Hebdo data set.}\label{fig:charlieCoherence}
\end{figure}

\begin{figure}[ht]
	\centerfloat{
		\includegraphics[scale=1.0]{byrulevoCoherence}
	}
	\caption{Coherence scores on Byrulevo data set.}\label{fig:byrulevoCoherence}
\end{figure}

\begin{figure}[ht]
	\centerfloat{
		\includegraphics[scale=1.0]{fergusonCoherence}
	}
	\caption{Coherence scores on Ferguson data set.}\label{fig:fergusonCoherence}
\end{figure}

For each data set and topic model a few most coherent topics (topics focused on specific words which have the higher probability to occur in a specific topics) were chosen and can be seen in Tables~\cref{tab:byrulevoTopics,tab:charlieTopics,tab:fergusonTopics}.

As a result of the analysis of the given conflict ad hoc discussions, Biterm Topic Model came out to be the best performing and most stable method based on all coherence measures, baseline Lda model not specialized for working with short text has shown to not be suitable without the additional preprocessing of data mainly due to the data sparsity. However topics identified using this model can still be used to analyze key moments of discussions, participants’ arguments, and as a basis for other data analysis tasks.

Future work involves continuing the work on peer review, the first part of which was done for the Byrulevo data set by students and researchers from the journalism department of SPBU university. It has shown that the best results were obtained by the WNTM model. Comparing results of the models to the manual evaluation will allow us to measure not only the effectiveness of each model but also to compare coherence measures and find which one correlates better with human perception. The different prepocessing methods can also be used to improve the final result, tf-idf can be used instead of regular bag words approach to omit less frequent words more effectively.

\subsection{Topics in the Russian Twitter and relations between their interpretability and sentiment}\label{subsec:ch5/sec2/sub4}

\subsubsection{1. Introduction}

After the principal possibility of topic modelling on such short and noisy texts as tweets has been demonstrated over ten years ago \cite{RamageDumaisLiebling}, it has become a focus of scholarly attention as a method of detection of hidden substance of the tweet corpora, tracking user behavior, and pre-processing to reducing the dimensionality of the corpora. For English, an extensive number of studies have been made, and several algorithms have been proposed. For other languages, though, there have been much fewer experiments, especially of comparative thought.

Despite, by the nature of topic modelling, short and noisy natural-language texts are virtually the worst object for topic detection, and despite the diminishing relevance of Twitter in the media ecosystems of many countries, the attractiveness of Twitter as a source of datasets for modelling remains high -- again, mostly for the English-language cases.

Russian social media have so far received moderate attention in terms of experiments with topic modelling, and the Russian Twitter has not been its primary focus. In particular, topic interpretability has not received, to our viewpoint, enough scholarly attention. Despite topic quality is quite an expansive research area (for a review, see \cite{MavrinFilchenkovKoltcov}), only several studies have addressed the divergence between the objective and subjective topic quality assessment, especially rarely for Russian \cite{BodrunovaKoltcovKoltsova}. The causes of low interpretability of topics for short texts have virtually escaped proper discussion, and no studies have been dedicated to linking topic semantics and their interpretability.

Our work aims at partly covering the gaps described above. We model the topics for a dataset of a conflictual discussion of as early as 2013, as at that moment, according to our previous studies \cite{BodrunovaLitvinenkoBlekanov2017}, the Russian Twitter was not as bot-occupied as it has been since 2016 \cite{StukalSanovichBonneau}. We have chosen to work with conflicts online, as they provide for emotionally loaded texts and link the discussions to a variety of wider issues of user interest.

Earlier, we have tested three topic modelling algorithms, namely unsupervised LDA, BTM, and WNTM, on three raw-data Twitter datasets in English, French, and Russian, including the aforementioned dataset \cite{BlekanovTarasovMaksimov}. But, as it often happens in topic modelling studies, the results must be tested also by human coders. Below, we will describe in short what and why we have done in terms of algorithm development and what we have tested additionally to provide more freedom to human coders.

The remainder of the paper is organized as follows. In Section 2, we review the literature on topic modelling and Twitter studies in Russia and beyond. In Section 3, we formulate the hypotheses, tell of the case under scrutiny, our previous topic modelling experience with the three algorithms, and suggest data amplification for human coders. In Section 4, we describe the method and the research procedures. Section 5 provides the results and discusses them for both the process of human coding and the results based on sentiment analysis. The concluding remarks reformulate our findings against prior knowledge and provide the guidelines for further research.

\subsubsection{2. Topic Modelling for Russian Social Media: A Literature Review}

To our best knowledge, there has so far been no extensive review of how topic modeling has developed for the Russian language. Not aspiring for a truly representative one, we anyway need to review the field to show how scarce attention has so far been given to the relations between the interpretability of modelling results and the topics’ inherent features. This, at least partly, is also true for topic modelling studies for other languages.

For Russian, topic modelling studies may be divided into methodological (that develop, compare, and extend models as well as evaluate their quality), applied (that apply topic modelling to extract the meanings from datasets), and relational (that relate topic modelling results to other features of the datasets or external factors).

\paragraph{A. Methodological studies of topic modelling} 
There are several groups within Russia who have been focusing on various topic modelling algorithms. Thus, in a sequence of influential works, Koltsova and colleagues develop LDA \cite{KoltcovKoltsovaNikolenko2014} and a range of extensions to it. The latter include semi-supervised interval LDA, or ISLDA \cite{BodrunovaKoltcovKoltsova}, granulated LDA \cite{KoltcovNikolenkoKoltsova0516} and LDA with local density regularization \cite{KoltcovNikolenkoKoltsova0916}. In relation to this work, Koltsov and colleagues have optimized Gibbs sampling \cite{KoltcovNikolenkoKoltsova16} and the model itself applying Rényi and Tsallis entropies \cite{Koltcov}. This group of authors has linked the use of topic modelling to qualitative studies of social media and beyond \cite{NikolenkoKoltcovKoltsova} and applied their instruments to several corpora of longer and shorter texts (see below). In these works, Russian is used more as an example of ‘a language as such’, just as English is used in topic modeling, often without discussing inherent linguistic or contextual limitations. Similarly, the works by Vorontsov and colleagues (e.g. \cite{VorontsovFreiApishev1015,VorontsovPotapenko,VorontsovFreiApishev0415}) have been influential in pLSA and its modifications based on non- Bayesian regularization. Among the rest, they have tested two algorithms for the Russian-language short texts, namely biterm topic modelling (BTM) and word network topic model (WNTM) \cite[p.~191]{KochedykovApishevGolitsyn} which we have also employed for our Twitter study (see \cite{BlekanovTarasovMaksimov} and below). The two research groups have collaborated on additive topic models \cite{ApishevKoltcovKoltsova1016,ApishevKoltcovKoltsova16} and have published important methodological papers in Russian (which we do not review here due to the scarcity of space). We will also omit from our review the works that suggest text clustering instruments alternative to topic modelling.

Several other authors have amplified this corpus of works by adding automated labeling to Russian-language topics \cite{MirzagitovaMitrofanova}, showing the possibility of single term extraction by topic modelling \cite{BolshakovaLoukachevitchNokel}, and multi-model tests on optimization of the number of topics \cite{KrasnovSen}.

\paragraph{B. Use of topic modelling for content interpretation: applied and relational works}
Content-exploring research has scrutinized both social media and text corpora beyond them. Thus, topic modelling has been employed to map the agenda of the Russian Livejournal of 2013 \cite{KoltsovaKoltcov} and ethnic contents of the Russian blogs \cite{Nagornyy}, including detection of most hated ethnicities \cite{BodrunovaKoltsovaKoltcov}.

Beyond the social networking realm, LDA has been applied to TV topics \cite{KoltsovaPashakhin}, Russian prose \cite{SedovaMitrofanova}, a corpus of musicological texts \cite{Mitrofanova}, newspaper news texts on climate change \cite{BoussalisCoanPoberezhskaya}, and, along with BTM, to Q\&A queries \cite{VolskeBraslavskiHagen}, with reasonable success.

The works that, above, we have called ‘relational’ are almost exclusively written by Koltsova and colleagues. They have shown how activity in political blogs in 2011-2012 correlated with the politicians’ ratings \cite{KoltsovaShcherbak}; the topicality of Livejournal top blogs corresponded to the structure of the Livejournal co-commenting communities \cite{KoltsovaKoltcovNikolenko}; and the linkage between news topics on TV and user feedback \cite{KoltsovPashakhinDokuka}, among other works.

\paragraph{C. Topic modelling for the Russian Twitter}
There are three methodological works except ours that explore topic modelling for the Russian Twitter \cite{MimnoWallachNaradowsky,Sridhar,GutierrezShutovaLichtenstein}. They all use Russian datasets for developing multilingual modelling tools. The first two do not discuss individual results for any single language, and the third only observes one difference in description of sports between Russian- and English-language Twitter. Both works assess the quality of the models but lack the discussion on human interpretability of the topics.

Similarly, only a small handful of works applies topic modelling to Russian Twitter to detect substantial meanings or discussion features. Thus, one work \cite{ChewTurnley} has shown the divergence between Russian- and English-language ‘master narratives’ on Russian cyber-operations.

Our works appear to the only continuous effort (since 2013) to apply automated text analysis to the Russian Twitter. Thus, we have tested three topic models \cite{BlekanovTarasovMaksimov} (see the details below) and have also applied BTM to detect the dynamics of topicality in conflictual discussions \cite{SmoliarovaBodrunovaYakunin}.

But, so far, there has been no discussion on whether topic models work well for the Russian Twitter in terms of human interpretability and what would be the features that could help in raising it.

\paragraph{D. Quality assessment and interpretability of the Russian-language topics}
An extensive study of nine automated metrics juxtaposed to the human-coding baseline was performed in \cite{Nikolenko}. Based on word2vec approaches suggested earlier, the author shows that normalized PMI (NPMI) outperforms PMI as well as other conventional metrics like tf-idf, and that vector-based metrics work better than all others. But, for short texts, the question remains how wor2vec metrics work with pooled short texts (what is most often done for tweets), this is why we use NPMI, as well as Umass \cite{BodrunovaKoltcovKoltsova}. Another important attempt to introduce a quality metric for longer texts was made in \cite{MavrinFilchenkovKoltcov}.

But none of these works has primarily focused on the issues in human (non-)interpretability of the topics, mostly seeing human coding as a baseline -- perhaps because, for longer texts, when interpretability was at stake, the models performed well enough. Thus, the authors \cite{KoltsovaKoltcov} have shown that, for long texts like Livejournal posts, circa two-thirds of the topics are interpretable after LDA has been applied. They have also identified three types of uninterpretable topics: ‘language’ (other than Russian), ‘style’ (writing styles, including offensive language), and ‘noise’ (uninterpretable texts / combinations of texts) \cite[p.~218]{KoltsovaKoltcov}.

In our pilot studies, though, we have seen that topics for Twitter are less interpretable, and we explore the coders’ experience below. The general features of Russian as a highly inflected language are well-known \cite{Whittaker}; for such languages, several solutions have been suggested \cite{MaucecKacicHorvat}, but it is not the nature of Russian alone that seems to be causing lower topic interpretability in the case of Russian Twitter.

Also, no studies have been done to understand the features of the topics that could raise the topic interpretability. In other languages, first and foremost English, relations between sentiment and topicality of discussions in social media have been explored in a range of works (for a short review, see \cite{XiangZhou}). But, so far, as soon as we know, there have been no works that would explore topic sentiment in relation to human interpretability. Our experiments with sentiment on Twitter in three languages \cite{BlekanovKukarkinMaksimov,BodrunovaBlekanovKukarkin2018}, including Russian \cite{BodrunovaLitvinenkoBlekanov2017,NigmatullinaBodrunova}, show that sentiment of tweets is tightly linked to topicality and user status as influencer. Our intuition is that more interpretable topics are more sentiment-loaded, in particular -- negativity-loaded.

\subsubsection{3. Our Hypotheses and the Case Under Scrutiny}

\paragraph{A. The research hypotheses}
Previously, we have applied three topic modelling algorithms to three datasets from Twitter, all featuring nation- or world-scale conflictual discussions \cite{BlekanovTarasovMaksimov}. We have tested the results with two topic coherence metrics: Umass (as suggested by \cite{MimnoWallachTalley}) and NPMI (as suggested by \cite{StevesonAletras}). There, we have shown, in particular, that, for all the three cases that: 1) BTM outperforms LDA and WNTM for our data; 2) coherence metrics show that topics are extracted well; 3) that the best number of topics for our datasets is 100.

But, as we have stated above, later pilot tests of the models on human coders have demonstrated that the model runs labeled as good by automated metrics do not perform that well when read by humans. Thus, for this study, we turn the testing logic upside down and question the ‘human baseline’ approach, to discuss the shortcomings of the algorithms noted by the human coders. For this, we amplify our coding interface a word-to-topic relevance metric \(\lambda\) suggested by \cite{SievertShirley} and check its application upon a small number of topics with human coders. ‘Playing’ with this function allows to lower the most relevant (and, thus, most frequent) terms among the topic descriptors and bring more specific terms up.

We also link human interpretability to sentiment load of the topic descriptors (30 words).

Thus, we hypothesize that:
\begin{itemize}
	\item H1. For all the three algorithms, lowering \(\lambda\) leads to higher interpretability.
	\item H2. With the best relevance level, BTM outperforms LDA and WNTM in human coding, just as it does by objective metrics.
	\item H3. More interpretable topics are more negativity-loaded.
	\item H4. The correlation between human interpretability and sentiment will be higher for the model that performs best by automated quality metrics.
	\item H5. The correlation between human interpretability and sentiment will be higher for the model that performs best by human coders’ assessment.
\end{itemize}

The two last hypotheses become mutually exclusive if H2 is not proven.

\paragraph{B. The case under scrutiny} 
The case that we work with is a month-long discussion that dates back to October 2013 and describes a conflict between locals and re-settlers from Central Asia in the Moscow district of Biryulevo. The discussion unfolded after a killing of a Russian Egor Scherbakov by an Uzbek re-settler Orham Zeinalov and opened up a nationwide discussion on migration issues like illegal dwelling of immigrants, visas for immigrant workers, cultural differences, and ‘radical white’ Russia-for-Russians opposition to re-settling. The Twitter part of this discussion has immediately reached national trending topics and has sparked participation of local authorities, police special troops, diasporas, international commentators, and NGOs.

The collected corpus of tweets is not very large; it covers October 1 to 31, 2013, and, after cleaning of spam, tweets in languages other than Russian, and links-only tweets, includes 10215 tweets from 3574 users who published them.

It suits our goals by several reasons. First, its dating back to 2013 ensures that it is almost bot-free. Second, it has high conflicting potential, and its tweets are highly emotionally loaded. Third, half of our coders have previously worked with this dataset and could link the issues they experienced in coding to the nature of the discussion already studied in other aspects.

\subsubsection{4. The Research Methods and Conduct of the Study}

For this part of the study, our methods look simple, but we still consider this part of our research crucial for further development. We will describe the methods and the conduct of the research together, to be able to comment on methods use.

\begin{enumerate}
	\item To select the proper meaning of word relevance for further assessment, we have tested the relevance metric \(\lambda\) for the three algorithms on a minor number of topics (30 for each algorithm).
	\item We have hand-coded the 100-topic runs for the three algorithms. The coders were not familiar with the dataset and were just explained the task and provided some knowledge on the details of the case; the trainers belonged to the group that was familiar with the data. Then, for each topic, we have calculated the interpretability score \(K\) of 0 (both coders could not interpret it), 1 (one coder managed to interpret it), or 2 (both coders interpreted it) and checked the Cohen’s Kappa for inter-rater reliability.
	\item We applied our sentiment assigning algorithm \cite{BlekanovKukarkinMaksimov} to the topic descriptors and hand-corrected it. What we were focusing upon was negativity, very characteristic for this discussion. This is why we coded the sentiment in the following way: positive, neutral, mixed, and undefined terms = 0, negative terms (by dictionary) = 1, hate speech including harsh pejoratives and obscene lexicon = 2. For each topic, we have calculated the descriptor negativity score \((N)\), dividing the negativity score by 30 (the number of word descriptors assessed).
	\item We calculated Spearman’s rho for each algorithm juxtaposing the interpretability score and the negativity indices for the topics, to see if there is any correlation between them.
\end{enumerate}

We need to state that this stage of research is preliminary, and our work in progress involves identical tasks for the datasets on similar inter-ethnic conflicts in English, German, and French.

\subsubsection{5. The Results and Their Discussion}

\paragraph{A. H1: term relevance and interpretability}
H1. We have pre-tested \(\lambda\) for our data, to be able to choose its proper meaning for further coding. The results are shown in Table~\cref{tab:threeAlgorithmRelevance}. We have conducted the tests with \(\lambda\) changing from 1 to 0.4 with the step of 0.1, but show only the results for \(\lambda = 1\) and \(\lambda = 0.5\), as 0.5 has behaved as a threshold changing the word descriptors picture the most. Codings for various meanings \(\lambda\) were made with significant time intervals (5 to 8 days), to avoid results skewing.

\begin{table}[ht]%
	\centering
	\caption{Pre-testing the relevance metric \(\lambda\) for the three algorithms, \% of 30 topics.}%
	\label{tab:threeAlgorithmRelevance}% label всегда желательно идти после caption
	%	\begin{adjustbox}{width=1\textwidth}
		%		\small
		\begin{tabular}{ c  c  c  c  c }% Вертикальные полосы не используются принципиально, как и лишние горизонтальные (допускается по ГОСТ 2.105 пункт 4.4.5) % @{} позволяет прижиматься к краям
			\toprule
			\makecell[c]{Model} & \multicolumn{2}{c}{\makecell{\(\lambda = 1\)}} & \multicolumn{2}{c}{\makecell{\(\lambda = 0.5\)}} \\
			\cline{2-5}
			 & Coder1 & Coder2 & Coder1 & Coder2 \\
			 \hline
			 LDA & 53\% & 63\% & 40\% & 33\% \\
			 WNTM & 76\% & 73\% & 70\% & 67\%\\
			 BTM & 76\% & 83\% & 69\% & 75\% \\
			\bottomrule
		\end{tabular}%
		%	\end{adjustbox}
\end{table}

As Table~\cref{tab:threeAlgorithmRelevance} shows, H1 proves wrong for all the models. The general line in interpretability is that it either gradually declines with lowering  \(\lambda\) or keeps steady and then sharply drops at circa 0.55 to 0.45. This is perfectly logical if we consider the idea of word relevance itself, but it does not correspond to how the instrument is intended to work. In most cases, well-interpretable topics remained in place; only rarely, one or another coder found some dubious topics more interpretable than before, but mostly, the results were negative, For some good topics, unfortunately, the most frequent words remained in place, while the words \#10 to \#20 changed, completely altering the meaning of the topic to the worse in terms of interpretability. We need more experiments with bigger number of topics and coders, to detect the optimal  \(\lambda\) that would help avoid the shifts of meaning and at the same time eliminate the frequent terms. So far, we will use \(\lambda = 1\) for further coding, despite a relatively big number of frequent terms in the top of the topic descriptors. For a number of topics in WNTM, though, we have seen significant improvement in interpretability where the term relevance change worked exactly as expected.

\paragraph{B. H2: interpretability of the topics with \(\lambda = 1\)}
The aggregated results of the human coding of the topics are represented in Table~\cref{tab:topicHumanInterpretability}.

\begin{table}[ht]%
	\centering
	\caption{Human interpretability of the twitter topics, \% of 100 topics.}%
	\label{tab:topicHumanInterpretability}% label всегда желательно идти после caption
	%	\begin{adjustbox}{width=1\textwidth}
		%		\small
		\begin{tabular}{ c  c  c  c  c }% Вертикальные полосы не используются принципиально, как и лишние горизонтальные (допускается по ГОСТ 2.105 пункт 4.4.5) % @{} позволяет прижиматься к краям
			\toprule
			Model & Coder1 & Coder2 & Metacoding \((K = 2)\) &  Cohen’s Kappa \\
			\hline
			LDA & 66\% & 52\% & 46\% & 60\%; 0,109 \\
			WNTM & 66\% & 56\% & 34\% & 64\%; 0,277 \\
			BTM & 68\% & 53\% & 43\% & 72\%; 0,437 \\
			\bottomrule
		\end{tabular}%
		%	\end{adjustbox}
\end{table}

As we see from Table~\cref{tab:topicHumanInterpretability}, the difference between the coders in terms of the number of the interpretable topics was not that big (14\% of topics maximum). Coder 1 has shown the levels of interpretability similar to those reported for longer texts \cite{KoltsovaKoltcov}, while Coder 2 could not reach 60\% with any of the algorithms.

For the number of well-interpretable topics, H2 must be rejected as well, as LDA outperformed both WNTM and BTM in \(K = 2\). But, at the same time, BTM shows much higher inter-coder agreement, and this makes us conclude that, for H2, we have mixed evidence. For BTM and WNTM, there were more topics that were filtered out by both coders (14 for LDA; 27 for WNTM; 33 for BTM, 5 of which by language). This makes us pose a question whether high number of well-interpretable topics must always be the goal of topic modelling, especially for short texts. Instead, a smaller number of ‘ideal’ (robust, salient, and well- interpretable) topics might be sought for.

\paragraph{C. Qualitative topic assessment: interpretability issues}
Before going any further, we feel necessary to describe the difficulties that the coders have encountered. Some of them fall into the categories of topic fallacies described previously for LDA \cite{BoydGraberMimnoNewman}. Among them, there are general words, identical topics and mixed/chained topics that involve several chains of semantically related terms.

But, for Twitter, these problems have a specific face. Thus, for an issue-based discussion composed of short texts, even well-interpretable topics are problematic. They seem to be mostly based on the corpus of frequent terms of the discussion, like ‘migrant’, ‘situation’, ‘power’, ‘killer’, ‘detained’ etc., and some additional topic descriptors attracted to them by the model. The frequent lexicon is responsible for the coder’s feeling of comprehensibility, while the attracted lexicon contributes to the new meaning. But altogether this creates the sense of high similarity of the topics; the coders have complained on similarity of multiple topics rather than on non-comprehension. Also, the frequent terms blur the meaning of topics. Here, we see a possible decision in defining topic clusters (sub-themes) instead of individual topics, based on topic similarity metrics such as Jaccard or Kullback-Leibler indices.

Another issue is that, often enough, the topics seem to be formed by one popular (highly retweeted) tweet. Such tweets, allegedly, drag the users’ pools to each other and start the topics, further attracting to the topics the tweets of the respective users, rather than the topically similar tweets. Here, the problem might lie in the pooling technique, as we have used all tweets by a given user as ‘one text’. To resolve this issue, pooling vs. non-pooling must be tested.

And yet another problem lies in the case specificity. Working with a particular event- or issue-based discussion demands a certain level of contextual knowledge, without which one cannot assign meanings to the topic descriptors. Thus, during pre-tests, the members of our working group could interpret more topics than the coders, as the nature of the topics was details-dependent. This poses a wider problem of how the research community should treat interpretability of a topic in general. For longer texts from non-case-specific corpora, topics appear to be more distinguishable from each other, as they, indeed, relate to very different themes in users’ online speech. But defining sub-topics within a highly thematized corpus demands a different understanding of topic interpretability, coherence, and distance. Also, we recommend that, for coding thematic discussions, the coders become familiar with the theme and its context, and not by preliminary reading but from other sources.

\paragraph{D. Negativity in topics and its relation to interpretability}
To test H3, we have conducted Spearman’s test between topic interpretability score \((K)\) and topic negativity score \((N)\). For all the three algorithms, H3 is proven, with relatively high Spearman’s correlation values: for LDA, 0,375**; for WNTM, 0,383**; for BTM, 0,494. This proves H3 and demands further research on whether negatively loaded topics show higher interpretability for datasets of other nature -- non-thematic, non-conflictual, platform-diverse.

We also see that the Spearman’s value is the highest for BTM which proves our H4. It also proves H5 if we consider inter-coder reliability, but H5 is rejected if we look at \(K\) for each algorithm.

\subsubsection{6. Conclusion}

This paper describes the work in progress upon a dataset from the Russian Twitter; we amplify earlier topic modelling results and objective topic quality measurement with a discussion on human interpretation of the modelling results. Our results contribute to the field of the topic modelling studies in three ways. We, first, show that term relevance tool works only partly the way that could help raise interpretability. Second, we see that automated quality assessment does not always correspond with how coders assess the topics, and designate a range of problematic issues in coding. Third, we show that negativity in topic descriptors might be related to interpretability of the topics for human coders, which is not captured by automated assessment of topic detection quality.

More generally, we can cautiously claim that the difficulties with Russian as a highly inflected language are not what cause the problems in topic detection and interpretation. Rather, it is the algorithmic imperfection, dataset preparation technique, and context dependence that altogether cause lower interpretability of short texts that, in addition, are noisy and ‘oral-written’ \cite{Lutovinova}.

We, at least partly, support what was earlier reported on the number of interpretable topics in Russian-language datasets but pose the question of what is more important for Twitter -- a higher number of interpretable (but identical) topics or a better-defined (but smaller in quantity) topics in the run, as this question seems crucial for Twitter studies.

Our research will continue in the areas designated above, including stretching our method to three other languages, experimenting with term relevance, and exploring the nature of impact of negativity upon topic interpretation.

\section{Методы анализа тональности контента}\label{sec:ch5/sect3}

\section{Методы суммаризации}\label{sec:ch5/sect4}

\FloatBarrier

