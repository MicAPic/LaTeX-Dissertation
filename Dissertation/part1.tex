\chapter{Web 1.0}\label{ch:ch1}

\section{Концептуальное назначение}\label{sec:ch1/sec1}

\[
--
\]

\section{Обобщенная структура}\label{sec:ch1/sec2}

\subsection{Анализ топологии крупных сегментов Веба с помощью модели Бродера “Bow-tie”}\label{subsec:ch1/sec2/sub1}

\paragraph{Введение.} В настоящее время все больше организаций, стремясь отразить свою деятельность в Вебе, создают отдельные веб-ресурсы (сайты, группы сайтов) и заботятся об улучшении их рейтингов в информационно-поисковых системах с целью накопления символического капитала, повышения прямых продаж. Важную роль в формировании рейтинга сайтов и локализированных веб-сегментов в целом играют поисковые машины.

С 2006 года основные поисковые машины (например, в мире -- Google, в России -- Яндекс) в своих методах ранжирования наряду с классическими факторами \cite{Kleinberg, BrinPage, Chakrabarti} (частота встречаемости слов, объем цитирования и авторитетность сайтов, частота обновления сайтов и др.) по-новому используют факторы, связанные с поведением пользователя на веб-ресурсах \cite{GuhaKunduBhadra,AntoniouPlegasTsakalidis,FeuerSavevAslam}. Так, действия пользователя, ранее приводившие к повышению рейтингов сайтов (например, частота захода пользователей), ныне могут приводить к “пессимизации” показателей, поскольку поисковые системы становятся более “социальными” и учитывают уже не только профиль пользователя и частоту захода на ресурсы, но также мотивацию посетителей и стратегии их поведения (частоту повторного захода, время нахождения на странице и сайте, логику и маршруты переходов, пользовательские интересы, тип потребляемого контента и многие иные факторы). При ранжировании современные поисковые машины в совокупности учитывают около 300-500 таких факторов.

С другой стороны, существует небольшое количество структурных характеристик Веб-сегментов \cite{ChoRoy}, которые существенно влияют практически на все эти факторы. Имеется значительное число исследований \cite{Kleinberg,ChoRoy,BroderKumarMaghoul,AguilloGranadinoOrtega,StuartThelwallHarries,Chakrabarti,Thelwall,Pechnikov,PechnikovNwohiri}, в которых строится модель фрагментов веба, описывающаяся сравнительно небольшим набором параметров.

Общим направлением исследований авторов является изучение связей между рейтингом сайта и его глобальными характеристиками. По-видимому, эти связи различны для сайтов разных категорий, например, естественнонаучной направленности и гуманитарных. Для выявления существенных различий между категориями сайтов, необходимо исследовать большое количество сайтов с последующей статистической обработкой материала. Авторы располагают разработанной ими аналитической системой для вебометрических исследований \cite{BlekanovSergeevMaksimov,BlekanovSergeevMartynenko}, способной выполнить эту работу. Ввиду чрезвычайно больших затрат машинных ресурсов на подобное исследование, представляет интерес машинный эксперимент, заключающийся в определении затрат на полное исследование типичных сайтов, описание которого и является темой данной статьи.

\paragraph{Основная часть.} В эксперименте ставилась задача построения топологии на основе модели Бродера “Bow-tie” \cite{BroderKumarMaghoul,Thelwall} для типичных крупных гуманитарно-ориентированных и естественнонаучно-ориентированных сегментов Веба с последующим вычислением их структурных характеристик и определением произведенных затрат ресурсов.

В качестве гуманитарно-ориентированного веб-сегмента был выбран сайт Факультета журналистики (JF) Санкт-Петербургского государственного университета (СПбГУ) -- www.jf.spbu.ru, естественнонаучно-ориентированного -- сайт факультета Прикладной математики -- процессов управления (APMATH) СПбГУ -- www.apmath.spbu.ru.

С помощью специализированной сборки аналитической системы для вебометрических исследований на основе ядра поискового робота, успешно апробированной в исследованиях \cite{BlekanovSergeev,MaksimovBlekanov} , были выявлены гиперссылочные структуры обоих сайтов, которые описываются в виде двух ориентированных веб-графов \(G_{apmath}\) и \(G_{jf}\), в которых вершины соответствуют страницам, а дуги – соединяющим эти страницы гиперссылкам. Для получения глобальных характеристик использовалась модель Бродера “Bow-tie” \cite{BroderKumarMaghoul}, которая во множестве вершин исследуемого графа выделяет четыре основных подмножества: 1) центральное ядро (компонента сильной связности SCC); 2) истоки (IN); 3) стоки (OUT); 4) “TENDRILS” и “TUBES”. Поиск сильно-связной компоненты выполнялся с помощью алгоритма Косарайю \cite{Sedgewick} (Kosaraju's algorithm), время выполнения которого для разреженных графов (а такими являются исследуемые веб-графы) пропорционально сумме всех их вершин и направленных ребер.

В эксперименте вычислялись следующие характеристики веб-сайтов:
\begin{itemize}
	\item размер центрального ядра \(S_{scc}\);
	\item размер стоков \(S_{out}\) и истоков \(S_{in}\);
	\item размер “TENDRILS” и “TUBES” \(S_{tubes}\);
	\item отношение \({\lvert S_{scc} \rvert + \lvert S_{in} \rvert}\) к \(\lvert S_{scc} \rvert + \lvert S_{out} \rvert\).
\end{itemize}

Кроме того, для каждой страницы центрального ядра был введен показатель ее связности с другими страницами из ядра:
\[
	\text{\textit{VC}}(u, v) = 
	\begin{cases} 
		1, & \text{\textit{if }} u \rightarrow v,\\
		0, & \text{\textit{else}}
	\end{cases},
	\forall u, v \in S_{scc}
\]

Этот показатель лег в основу усредненной меры связности каждой страницы ядра \(\text{\textit{MA\text{\textit{VC}}}}(u)\) и усредненной меры связности центрального ядра \(\text{\textit{MAC}}(S_{scc})\):
\[
	\text{\textit{MAVC}}(u) = \frac{1}{\lvert S_{scc} \rvert - 1} \sum_{\forall v \in S_{scc}, \; v \neq u}\text{\textit{VC}}(u, v),
\] 
\[
	\text{\textit{MAC}}(S_{scc}) = \frac{1}{|S_{scc}|} \sum_{\forall u \in S_{scc}}\text{\textit{MAVC}}(u).
\]

Приведенные меры были введены для оценки качества связанности каждой топологии, полученной в ходе поставленного эксперимента.

Для оценки трудоемкости эксперимента по каждому исследуемому сайту использовались следующие характеристики:
\begin{itemize}
	\item время (\(T_{total}^{js}\) и \(T_{total}^{apmath}\)), необходимое для выявления топологии и вычисления указанных обобщенных характеристик;
	\item объем памяти (\(\text{\textit{VMem}}_{js}\) и \(\text{\textit{VMem}}_{apmath}\)), требуемый для хранения обработанных данных;
	\item объем Веб трафика \(\text{\textit{Traffic}}_{js}\) и \(\text{\textit{Traffic}}_{apmath}\)).
\end{itemize}

\paragraph{Результаты эксперимента.} Эксперимент проводился на тестовой персональной электронно-вычислительной машине (ПЭВМ) с процессором \textit{Intel Core i7 CPU 950 @ 3.07 GHz x 8}, \textit{12 GB} объемом жесткого диска и операционной системой \textit{Ubuntu 13.10}. Загрузка данных с Интернета выполнялась посредством канала с пропускной способностью до \textit{50 Мбит/с}.

В ходе эксперимента было выявлено, что Веб-граф \(G_{apmath}\) содержит \textit{26 148} вершин и \textit{2 025 909} направленных ребер, а Веб-граф \(G_{jf}\) -- \textit{27 361} вершин и \textit{4 252 424} направленных ребра. На рис.~\cref{fig:webGraphs} приведены топологии, построенные для каждого полученного графа с помощью модели Бродера “Bow-tie”.

\begin{figure}[ht]
    \centerfloat{
	        \hfill
	        \subcaptionbox[List-of-Figures entry]{\label{fig:webGraphs-1}}{%
		            \includegraphics[width=0.5\linewidth]{webGraph(apmath)}}
	        \subcaptionbox{\label{fig:webGraphs-2}}{%
		            \includegraphics[width=0.5\linewidth]{webGraph(jf)}}
	        \hfill
	    }
    \caption{Топологии Веб-графов \(G_{apmath}\) и \(G_{jf}\), соответственно.}\label{fig:webGraphs}
\end{figure}

Значительную долю в топологии Веб-графа \(G_{apmath}\) среди всех ее компонент занимает SCC (\textit{65.56\%} страниц от общего их числа), \textit{8.8\%} занимают истоки (IN), - \textit{25.5\%} стоки (OUT), \textit{3.96\%} -- “TENDRILS” и “TUBES” (рис.~\cref{fig:webGraphs-1}). В то время как доли компонент топологии Веб-графа (рис.~\cref{fig:webGraphs}\subcaptionref*{fig:webGraphs-2}) приблизительно одинаковые (более \textit{46\%} от общего числа всех страниц) за исключением компоненты OUT (\textit{0.76\%}).

В табл.~\cref{tab:webGraphTable} приведены полученные численные значения структурных характеристик, которые были введены в эксперименте для анализа исследуемых сайтов.

\begin{table} [htbp]%
    \centering
    \caption{Структурные характеристики Веб-графов \(G_{apmath}\) и \(G_{jf}\).}%
    \label{tab:webGraphTable}% label всегда желательно идти после caption
    \renewcommand{\arraystretch}{1.5}%% Увеличение расстояния между рядами, для улучшения восприятия.
    \begin{SingleSpace}
    	\begin{tabulary}{\textwidth}{@{}>{\zz}L >{\zz}C >{\zz}C@{}} %Вертикальные полосы не используются принципиально, как и лишние горизонтальные (допускается по ГОСТ 2.105 пункт 4.4.5) % @{} позволяет прижиматься к краям
		            \toprule     %%% верхняя линейка
		            Характеристика & Топология сайта APMATH &Топология сайта JS \\
		            \midrule %%% тонкий разделитель. Отделяет названия столбцов. Обязателен по ГОСТ 2.105 пункт 4.4.5
		            Мощность множества ядра \(\lvert S_{scc} \rvert\)  & 17142 & 13893    \\
		            Мощность множества IN \(\lvert S_{in} \rvert\)       & 2302     & 13165    \\
		            Мощность множества OUT \(\lvert S_{out} \rvert\)       & 6669     & 206    \\
					Мощность множества \newline TENDRILS и TUBES  \(\lvert S_{tubes} \rvert\)      & 1035    & 12809     \\
					Отношение \(\frac{\lvert S_{scc} \rvert + \lvert S_{in} \rvert}{\lvert S_{scc} \rvert + \lvert S_{out} \rvert}\) & 0.82 & 1.92 \\
					Усредненная мера связности центрального ядра \(\text{\textit{MAC}}(S_{scc})\) & 0.00244 & 0.00193 \\
					Усредненная мера связности всего Веб-графа \textit{MAC(G)} & 0.00106 & 0.00098 \\
		            \bottomrule %%% нижняя линейка
		        \end{tabulary}%
	    \end{SingleSpace}
\end{table}

Затраты ресурсов, требуемых для полного анализа исследуемых в эксперименте сегментов Веба, приведены в табл.~\cref{tab:webGraphCostTable}.

\begin{table}[ht]%
	\caption{Характеристики, оценивающие трудоемкость поставленного эксперимента.}%
	\label{tab:webGraphCostTable}% label всегда желательно идти после caption
    \renewcommand{\arraystretch}{1.6}%% Увеличение расстояния между рядами, для улучшения восприятия.
    \def\tabularxcolumn#1{m{#1}}
    \begin{tabularx}{\textwidth}{@{}>{\raggedright}X>{\centering}m{3.5cm}  >{\centering\arraybackslash}m{3.5cm}@{}}% Вертикальные полосы не используются принципиально, как и лишние горизонтальные (допускается по ГОСТ 2.105 пункт 4.4.5) % @{} позволяет прижиматься к краям
			\toprule     %%% верхняя линейка
			Характеристика & Сайт APMATH & Сайт JS \\
			\midrule %%% тонкий разделитель. Отделяет названия столбцов. Обязателен по ГОСТ 2.105 пункт 4.4.5
			Время, необходимое для построения топологии и вычисления структурных характеристик, \textit{min.}  & \(T_{total}^{apmath} = 672\) &  \(T_{total}^{js} = 271\)  \\
			Среднее время загрузки одной страницы, \textit{min.} & 0.026 & 0.001 \\
			Объем памяти на жестком диске для хранения данных, \textit{MB} & \(\text{\textit{VMem}}_{apmath} = 154.743\) & \(\text{\textit{VMem}}_{js} = 318.463\) \\
			Минимальный объем \newline оперативной памяти \newline для обработки данных, \textit{GB} & \(\approx 1.5\) & \(\approx 1.5\) \\
			Средний объем памяти, требуемый  для полной обработки \newline одной веб-страницы, \textit{kB} & 6.06 & 11.92 \\
			Объем Веб трафика, \textit{GB} &  \(\text{\textit{Traffic}}_{apmath} = 9.66\) &  \(\text{\textit{Traffic}}_{js} = 3.1\) \\
			\bottomrule %%% нижняя линейка
	    \end{tabularx}%
\end{table}

\paragraph{Выводы.} Результаты эксперимента подтвердили существенные различия между гиперссылочной структурой представителя сайтов естественнонаучной направленности и структурой представителя гуманитарно-ориентированных сайтов. Так, например, в топологии первого сайта (рис.~\cref{fig:webGraphs-1}) большая доля всех веб-страниц сосредоточена в центральном ядре, которое имеет большую связность между своими элементами по сравнению с элементами ядра второго сайта (рис.~\cref{fig:webGraphs}\subcaptionref*{fig:webGraphs-2}). Кроме, того значительное различие компоненты OUT первой топологии в сравнении со второй объясняется наличием в ней большого числа полнотекстовых документов (в формате PDF, DOC, DOCX и др.). Однако, количество гиперссылок в Веб-графе \(G_{jf}\) и характеристик размеров компонент SCC, IN, TUBES/TENDRILS (Таб.~\cref{tab:webGraphTable}) говорят о хорошей коммуникабельности между компонентами топологии Веб-графа \(G_{jf}\).

Для получения статистически достоверных результатов необходимо провести экспериментальные исследования топологий большого числа сайтов для каждой выделенной категории. Опираясь на полученные значения характеристик (Таб.~\cref{tab:webGraphCostTable}), оценивающие трудоемкость поставленного в данной статье эксперимента, можно приблизительно оценить ресурсные затраты на полное исследование любого количества сайтов.

\subsection{Research of University Sites Internal Links Distribution}\label{subsec:ch1/sec2/sub2}

\paragraph{Intoduction.} The paper is concerned with the university sites structural properties study. An oriented graph is used as a generally accepted website structure representation. Nodes of the graph correspond to webpages, while arcs correspond to links between webpages. Consider that webpage does not have its own graph structure in terms of this graph.

Any site is a part of the World Wide Web, so we have to consider how to lay emphasis on a particular site.

First of all, technically speaking, the site has a domain name and actually only webpages corresponding to the domain can be referred to the organization website. Secondly, we consider subdomains to be external in relation to the website main domain. Thirdly, all links that are relevant to the main site webpages and subdomain webpages or other domain webpages are also considered to be external and are not part of the website.

University websites are usually both extremely large and significantly vary in size. A typical website has hundreds or thousands of pages, whereas there are also sites including 25, 10 thousand and 1 million pages (an approximate number of website pages can be estimated using Google's «site:» search term). Since the websites are so large, the task to determine structural, thematic, ergonomic characteristics of the site becomes truly laborious.

Knowledge about these characteristics will allow owners of large scientific and educational organizations’ websites to analyze the quality of their web resources presence in Web. Particularly, studies of structural characteristics will allow to monitor connections between web resources of organizations, to capture target groups of users and to evaluate site relevancy for them, to organize effective logistics of hyperlink structure \cite{StuartThelwallHarries,Thelwall,PechnikovNwohiri,PechnikovNwohiri2}. Ergonomic characteristics are estimating the adaptation of web resources of organizations to relevancy and ranking evaluation criteria of top search engines \cite{Huang,BodrunovaYakuninSmolin,LangvilleMeyer}. Enhancement of these characteristics will help stimulating user interest for every site and extend the time users spend on the sites which is the most important factor of relevancy in search engines rankings. However, estimating these characteristics for big sites is not an easy task in terms of time, computing power and system resources.

It seems natural to try to find an approximate solution to this issue, looking at a relatively small part of the site. Though, when you try to do this, you will get stuck at the point where you need to know what part of the site has already been looked through. To answer this question, you still need to look through the whole site to find out the total number of pages.

In our paper \cite{SergeevBlekanovMaksimov} we offered a method for website size (the number of webpages and links) estimation by its fraction (10\%). By collecting 11 university websites it has been shown that this method gives an estimate site webpages number with an average error of 40\%. The analysis was based on studies in \cite{BarabasiAlbert,BroderKumarMaghoul}, revealing that the pages are distributed according to the power law by the number of incoming links. We developed the idea that university sites are special in some way and that they have a different exponent. The preliminary analysis revealed that the changes of the exponent of the distribution law in our formulas \cite{SergeevBlekanovMaksimov} could have a significant influence on the site’s webpages and hyperlinks count estimation error. So, we performed an experiment using hyperlink structure of 97 universities from top 500 Webometrics  \cite{RankingWeb} rating. The information has been collected by means of authors' specialized web crawler \cite{BlekanovSergeevMartynenko}, adapted for webometric studies. Search engines like Google, Yahoo, etc. do not provide this kind of information.

\paragraph{Experiment.} \textit{Goal.} The experiment specifies the distribution law of university sites webpages by the number of incoming links. We assume that the distribution law is a power law. We need to find out the exponent for a power law.

\textit{Selecting university websites.} Since the websites of the world's leading universities are in great interests of the Webometrics rating, we decided to take first 500 sites into account. In order to reduce the huge amount of work, we have randomly chosen 100 university sites (from 1 to 500 of Webometrics rating), about 2-3 sites from every dozen.

Using web crawler designed by authors all pages and websites internal links of 97 universities have been collected and processed \cite{WebometricsLab}. We were unable to collect data from three sites due to their special layout. As a result of random selection universities were distributed by region as follows:
\begin{itemize}
	\item North America -- 42
	\item South America -- 2
	\item Europe -- 36
	\item Asia -- 13
	\item Australia and New Zealand -- 4
\end{itemize}

\textit{Experimental results.} Preliminary analysis displayed that university sites can be divided into several distinct groups by the number of webpages (Table~\cref{tab:sitesGroupedByNoOfPages}). This distinction caused some dubiety, whether or not it is possible to calculate the universal distribution exponent. 

\begin{table} [htbp]%
	\centering
	\caption{Sites grouped by the number of pages.}%
	\label{tab:sitesGroupedByNoOfPages}% label всегда желательно идти после caption
	\renewcommand{\arraystretch}{1.5}%% Увеличение расстояния между рядами, для улучшения восприятия.
	\begin{SingleSpace}
		\begin{tabulary}{\textwidth}{@{}>{\zz}L >{\zz}C@{}} %Вертикальные полосы не используются принципиально, как и лишние горизонтальные (допускается по ГОСТ 2.105 пункт 4.4.5) % @{} позволяет прижиматься к краям
			\toprule     %%% верхняя линейка
			Pages count & Sites count  \\
			\midrule %%% тонкий разделитель. Отделяет названия столбцов. Обязателен по ГОСТ 2.105 пункт 4.4.5
			<1 300 & \textit{12} \\
			2 000-9 200 & \textit{10} \\
			10 000-25 000 & \textit{16} \\ 
			25 000-61 000 & \textit{12} \\
			61 000-100 000 &  \textit{7} \\
			100 000-400 000 & \textit{15} \\
			400 000-1 000 000  & \textit{9} \\ 
			>1 000 000  & \textit{16} \\
			\bottomrule %%% нижняя линейка
		\end{tabulary}%
	\end{SingleSpace}
\end{table}

We formed a plane for each university site, where horizontal coordinate is the number of incoming links, and vertical coordinate is the proportion of the page’s number. A set of points was applied to the plane. A power approximating curve was drawn, that helped to determine the exponent. We omitted some of the points while drawing the curve. These were points with the proportion of page count less than \(10^{-5}\). A
typical picture is shown in figure~\cref{fig:uniCollegeLondonCurve}. 

\begin{figure}[ht]
	\centerfloat{
		\includegraphics[scale=0.5]{uniCollegeLondonCurve}
	}
	\caption{Approximation curve for University College London site.}\label{fig:uniCollegeLondonCurve}
\end{figure}

We created a plane for each exponent of all university websites pages’ power distribution law where X axis represents the number of pages and Y axis represents exponent module. (Fig.~\cref{fig:sitesExponent}). 

\begin{figure}[ht]
	\centerfloat{
		\includegraphics[scale=0.4]{sitesExponent}
	}
	\caption{The exponent for different sites (X axis represents the number of pages of these sites). Black dot separates sites from first two groups from the rest of the sites.}\label{fig:sitesExponent}
\end{figure}

Figure 2 shows that sites from first and second groups have a vastly varying exponent that makes them stand out from other sites. This is most likely because their structure is influenced by two factors: management decisions regarding the structure and random spontaneous structure influences by active users. When a website is small (less than 10 thousand pages), management decisions influence is prevailing. And since every university has its own unique administrative unit, sites have different structural characteristics and different incoming link distributions. But when a website is big, the number of random structure influences becomes so large, that the role of management becomes less significant. 

\begin{figure}[ht]
	\centerfloat{
		\includegraphics[scale=0.4]{bigUniAveragedDistribution}
	}
	\caption{Averaged distribution for big university sites. Solid line represents power law distribution from A. Broder and R. Kumar (2000), Barabasi and Albert (1999) papers. Circles show variation areas of the averaged distribution values.}\label{fig:bigUniAveragedDistribution}
\end{figure}

\begin{figure}[ht]
	\centerfloat{
		\includegraphics[scale=0.4]{allUniAveragedDistribution}
	}
	\caption{Averaged distribution for all university websites. Solid line represents power law distribution from A. Broder and R. Kumar (2000), Barabasi and Albert (1999) papers.}\label{fig:allUniAveragedDistribution}
\end{figure}

We calculated two average exponents (Fig.~\cref{fig:bigUniAveragedDistribution},~\cref{fig:allUniAveragedDistribution}): \(k_\textit{uni\_all} = 1.8\) for all 97 sites and \(k_\textit{uni\_max} = 1.9\) just for big sites (75 sites with more than 10 thousand pages). These exponents differ from those obtained in \cite{BarabasiAlbert,BroderKumarMaghoul}. 

Figure~\cref{fig:bigUniAveragedDistribution} shows averaged distribution for all big sites. Two splashes (highlighted with circles) attract our attention: large deviations from the approximating curve. The first -- from 8 to 10 incoming links, the second -- around 69-71. The second can obviously be explained by the fact that the main page for most sites is here. The first is harder to explain. Perhaps, such a number of links is typical for main pages of different structural divisions of universities.

Figure~\cref{fig:allUniAveragedDistribution} shows that despite the fact there are many splashes due to considering small sized sites in terms of page count, the average exponent stays the same -- 1.8.

\paragraph{Conclusions and future work.} Our main results: 

\begin{itemize}
	\item All pages of 97 sites have been collected
	\item All internal incoming links for each page have been
	collected
	\item Exponent of power distribution for each university site
	has been calculated.
	\item The proximity of large (more than 10 thousand pages)
	sites distributions (\(k_\textit{uni\_max} = 1.9\)) and difference in
	distributions of small sites have been revealed (with
	the exponent between 0.0 and 5.0).
	\item The averaged approximating distribution curve (the
	exponent \(k_\textit{uni\_all} = 1.8\)) has been constructed and
	analyzed.
	\item Average exponent values have been calculated.
	Hyperlink structure of university sites was found to be
	different from \cite{BarabasiAlbert,BroderKumarMaghoul} in terms of the exponent.
\end{itemize}

The final results will be used to improve the algorithms for estimation of the size of any university site (page and hyperlink count) by studying just a part of the site. In the future, we plan to estimate structural, thematic and ergonomic characteristics of university websites by studying a randomly selected part. 

\subsection{Индекс для обработки и хранения гиперссылочных структур крупных Веб-сегментов}\label{subsec:ch1/sec2/sub3}

\paragraph{Введение.} В настоящее время все больше организаций заботятся о качестве собственных веб-ресурсов в сети Интернет, а именно: мониторят и увеличивают рейтинги в поисковых системах; увеличивают индекс цитирования в Вебе; увеличивают вебометрический рейтинг \cite{RankingWeb} (в случае научно-общеобразовательных учреждений); улучшают коммуникабельность гиперссылочной структуры веб-ресурсов; улучшают эргономические свойства веб-ресурсов (удобочитаемость шрифтов, удобство глубины экрана, продолжительность пребывания пользователя на сайте, доступности целевого контента, эффективности брендинга в визуальном оформлении сайта и т. п.).

Исследованиями количественных и качественных характеристик гиперссылочной структуры занимается активно-развивающееся научное направление -- вебометрика \cite{Pechnikov,PechnikovNwohiri}. Такие исследования вебресурсов называются вебометрическими. Местом апробации различных вебометрических методов, алгоритмов и программно-аналитических инструментов является университетский Веб \cite{Pechnikov,MaksimovBlekanov,BlekanovSergeevMaksimovBOWTIE}, который включает крупные веб-сегменты, состоящие из сайтов научно-образовательных учреждений.

Важной особенностью сайтов университетского Веба является сложная разветвленная гиперссылочная структура большой размерности. В среднем, подобные веб-структуры ссылок содержат более 50 000 узлов и более 2 000 000 связей между ними \cite{Pechnikov,BlekanovSergeevMaksimov}. В связи с этим одной из актуальных задач вебометрики является задача индексирования гиперссылочных структур больших Веб-сегментов, решение которой в целом позволило бы существенно ускорить различные вебометрические методы и алгоритмы анализа крупных сайтов, а также систематизировать хранение таких структур.

В данной работе в качестве решения поставленной выше задачи разработана индексная структура, основанная на использовании принципов инвертированного индекса.

\paragraph{Теоретическая часть.} Идея разработанной индексной структуры схожа с идеей реализации прямого и обратного индекса для полнотекстового поиска \cite{ManningRaghavanSchutze}. Индекс состоит из следующих частей:

\begin{enumerate}
	\item Словарь, содержащий множество страниц исследуемого веб-сегмента, которым при обработке присваиваются уникальные целочисленные идентификаторы. Ключом в словаре является хэш от URL-адреса, а значением -- идентификатор для этого адреса. Данная структура позволяет осуществлять поиск и добавление идентификатора для заданного URL-адреса страницы в среднем за \(O(1\)). Для эффективного поиска (за время \(O(1)\)) URL-адреса любой страницы по идентификатору вместе со словарем в индексе используется специальный массив, в котором ключом является идентификатор, а значением -- адрес страницы. Эти структуры расходуют \(O(n)\) памяти, где n -- число индексируемых страниц.
	\item Множества входящих и исходящих ссылок для каждой страницы. Множество исходящих ссылок заранее известно, а для нахождения множества входящих ссылок выполняется следующая процедура: для каждой веб-страницы \(T\) пробегаем по всем исходящим ссылкам и для каждой исходящей ссылки \(L\) добавляем для страницы \(L\) входящую ссылку \(T\). Данные множества ссылок хранятся в массивах переменной длины.
\end{enumerate}

Кроме того, в индексе реализована возможность мониторинга гиперссылочной структуры исследуемого сайта во времени, которая основана на хранении в базе данных различных сессий индексирования данного сайта.

Теоретическая оценка времени построения разработанного индекса составляет \(O(n)\).

\paragraph{Эксперимент.} Ставился эксперимент, в котором разработанный индекс для обработки и хранения гиперссылочных структур апробировался на крупных сайтах научно-образовательных учреждений. В качестве сайтов из мирового вебометрического рейтинга вузов \cite{RankingWeb} были выбраны два сайта российских университетов с наивысшим рейтингом:

\begin{enumerate}
	\item Сайт Московского государственного университета.
	\item Сайт Санкт-Петербургского государственного университета.
\end{enumerate}

Для сбора и выявления гиперссылочной структуры веб-ресурсов использовался прототип программно-аналитической системы для вебометрических исследований, основанной на обобщенном ядре поискового робота и успешно апробированной в исследованиях \cite{MaksimovBlekanov,BlekanovSergeevMaksimovBOWTIE}.

Эксперимент условно был разделен на три части. В первой части оценивалось время построения индекса для сайтов МГУ и СПбГУ. Во второй -- время работы алгоритма Косарайю (алгоритм нахождения компонент сильной связности графа), использующего данные из построенного индекса. В третьей -- производилась оценка скорости нахождения изменений в гиперссылочной структуре сайта МГУ (информация об удаленных и добавленных ссылках), собранной программным комплексом в разные моменты времени (в первой сессии сайт был проиндексирован полностью, во второй -- проиндексирована лишь некоторая его часть, чтобы гарантировать наличие изменений в гиперссылочной структуре сайта).

\paragraph{Результаты эксперимента.} Результаты оценки времени построения индексов для сайтов МГУ и СПбГУ приведены в таблице~\cref{tab:kosarajuMSUAndSPbU}.

\begin{table} [htbp]%
	\centering
	\caption{Оценка времени построения индекса и работы алгоритма Косарайю.}%
	\label{tab:kosarajuMSUAndSPbU}% label всегда желательно идти после caption
	\renewcommand{\arraystretch}{1.5}%% Увеличение расстояния между рядами, для улучшения восприятия.
	\begin{SingleSpace}
		\begin{tabulary}{\textwidth}{@{}>{\zz}L >{\zz}C >{\zz}C@{}} %Вертикальные полосы не используются принципиально, как и лишние горизонтальные (допускается по ГОСТ 2.105 пункт 4.4.5) % @{} позволяет прижиматься к краям
			\toprule     %%% верхняя линейка
			Параметры оценки & Сайт МГУ & Сайт СПбГУ  \\
			\midrule %%% тонкий разделитель. Отделяет названия столбцов. Обязателен по ГОСТ 2.105 пункт 4.4.5
			Количество обработанных индексом веб-страниц & 45 379 & 127 682 \\
			Количество обработанных индексом ссылок & 3 330 258 & 16 713 138 \\
			Индексирование исходящих ссылок, сек. & 31,4 & 101,4 \\
			Индексирование входящих ссылок, сек. & 0,71 & 3,7 \\
			Время выполнения алгоритма Косарайю, сек. & 0,051 & 0,12 \\
			\bottomrule %%% нижняя линейка
		\end{tabulary}%
	\end{SingleSpace}
\end{table}

Результаты анализа изменения гиперссылочной структуры сайта
МГУ между двумя сессиями сбора приведены в таблице~\cref{tab:timeMSUAndSPbU}.

\begin{table} [htbp]%
	\centering
	\caption{Оценка времени построения индекса и работы алгоритма Косарайю.}%
	\label{tab:timeMSUAndSPbU}% label всегда желательно идти после caption
	\renewcommand{\arraystretch}{1.5}%% Увеличение расстояния между рядами, для улучшения восприятия.
	\begin{SingleSpace}
		\begin{tabulary}{\textwidth}{@{}>{\zz}L >{\zz}C >{\zz}C@{}} %Вертикальные полосы не используются принципиально, как и лишние горизонтальные (допускается по ГОСТ 2.105 пункт 4.4.5) % @{} позволяет прижиматься к краям
			\toprule     %%% верхняя линейка
			Параметры оценки & Сессия сбора №1 & Сессия сбора №2  \\
			\midrule %%% тонкий разделитель. Отделяет названия столбцов. Обязателен по ГОСТ 2.105 пункт 4.4.5
			Количество веб-страниц & 45 379 & 41 314 \\
			Количество ссылок & 3 330 258 & 3 120 322 \\
			Количество добавленных ссылок & & 641 563\\
			Количество удаленных ссылок & & 851 499 \\
			Поиск добавленных и удаленных ссылок, сек. & & 5,18 \\
			\bottomrule %%% нижняя линейка
		\end{tabulary}%
	\end{SingleSpace}
\end{table}

\paragraph{Заключение.} Таким образом, результаты эксперимента показывают, что разработанный индекс имеет высокую скорость построения. Существенное различие скорости индексирования исходящих и скорости индексирования входящих ссылок (табл.~\cref{tab:kosarajuMSUAndSPbU}) объясняется тем, что при индексировании исходящих ссылок приходится обращаться к базе данных, хранящей предварительно собранную программным комплексом веб-структуру сайта, в то время, как при построении индекса для входящих ссылок используется уже построенный индекс исходящих ссылок. Созданная авторами индексная структура также показала высокую скорость взаимодействия с алгоритмом Косарайю, что в результате обеспечило быстрый поиск компонент сильной связности (табл.~\cref{tab:kosarajuMSUAndSPbU}).
	
Процедура получение информации из индекса по двум сессиям (табл.~\cref{tab:timeMSUAndSPbU}), хранящим состояние гиперссылочной структуры сайта МГУ, для определение разницы между ними, работает несколько медленнее, чем при взаимодействии индекса с алгоритмом Касарайю. Это связанно с загрузкой сессий из базы данных. Однако, это несущественно влияет на анализ гиперссылочных структур крупных сегментов веба.

\section{Сбор данных}\label{sec:ch1/sec3}

\subsection{Построение тематико-ориентированных веб-краулеров с использованием обобщенного ядра}\label{subsec:ch1/sec3/sub1}

За последние несколько лет в области информационного Веб-поиска все чаще проводятся исследования \cite{Pechnikov,PechnikovLugovayaChuiko,PechnikovChirkovChuiko}, связанные с развивающимся научным направлением вебометрика (webometrics) \cite{PechnikovLugovayaChuiko}. В частности, методами вебометрики исследуются такие вопросы, как оценка присутствия информационных веб-ресурсов в Вебе, ранжирование веб-сайтов университетов и научных учреждений \cite{Pechnikov,PechnikovChirkovChuiko} или повышение вебометрического рейтинга, введенного испанской группой Cybermetrics Lab (http://www.webometrics.info), различных университетов мира.

К актуальным направлениям вебометрики относятся исследования по анализу и выявлению гиперссылочных структур различных сегментов Веб-пространства (например, академический сегмент Веба, корпоративный и др.). Для получения больших объемов информации о гиперссылках используются Веб-краулеры. Общей задачей таких инструментов является специализированный обход Веба с целью сбора информации или определения гиперссылочной структуры и полезности каких-либо информационных ресурсов \cite{BlekanovBondarenko1,BlekanovBondarenko2}. Имеющиеся в настоящее время Веб-краулеры можно разделить на две группы "--- универсальные и специализированные (тематико-ориентированные). Универсальные обычно избыточны, сильно нагружают сетевые ресурсы и проигрывают специализированным по таким показателям, как время обхода Веб-пространства, производительность обработки информации, а также возможность направленного поиска информационных источников в рамках определенной метрики значимости. В этом смысле преимущества специализированных систем очевидны, однако, их минусом являются значительные затраты на изготовление -- для каждой специализации нужен свой краулер.

Вместе с тем, значительное число функций всех краулеров совпадает \cite{ArasuChoGM,Bar-Ilan,ChoGM,NajorkHeydon}.

В данной статье предлагается модель специализированных Веб-краулеров, состоящих из обобщенного ядра и специализированных дополнений. При таком подходе затраты на изготовление специализированного Веб-краулера минимизируются. Предлагается архитектура обобщенного ядра поискового робота и ее реализация. Ставится эксперимент, в котором сравнивается качество работы Веб-краулеров на основе обобщенного ядра с качеством работы зарубежных аналогов, на примере Heritrix, OpenWebSpider и Methanol Web Crawler.

\paragraph{Архитектура обобщенного ядра.} Масштабность Веб-пространства порождают ряд проблем, влияющих на эффективность работы Веб-краулеров любых типов. Анализ известных публикаций \cite{ArasuChoGM,ChoGM,NajorkHeydon,RaghavanGM}, в которых затрагиваются проблемы эффективной работы поисковых роботов, показывает, что любой специализированный поисковый робот должен решать следующие основные проблемы:
\begin{itemize}
	\item \textit{построение специализированного алгоритма обхода Веб-пространства;}
	\item \textit{обновление информационных Веб-ресурсов в общей коллекции документов, собранной Веб-краулером;}
	\item \textit{учет разновидностей форматов Веб-ресурсов;}
	\item \textit{организация информационного поиска по скрытому Вебу} \cite{RaghavanGM}\textit{;}
	\item \textit{минимизация нагрузки на информационный источник;}
	\item \textit{распараллеливание и масштабирование процесса сбора информационных Веб-ресурсов;}
	\item \textit{минимизация нагрузки на каналы связи в Веб-пространстве.}
\end{itemize}

Для их решения была разработана и реализована архитектура обобщенного ядра (рис.~\cref{fig:kernelArchitecture}), которая может применяться для создания любых типов Веб-краулеров. В основе этой архитектуры лежит краулер-процесс. По структуре \textit{краулер-процесс} является классом, содержащим текущую информацию о состоянии поискового робота (текущий список проанализированных гиперссылок, содержимое Веб-станиц, кэш адресов Веб-страниц, список значимых источников информации), а также набор методов для осуществления поиска и обновления информации в индексе. Данные методы позволяют запускать и останавливать процесс поиска.

Под процессом поиска в рамках рассматриваемой архитектуры понимается итеративный процесс, останавливающийся при выполнении заданных условий (например, число пройденных итераций, количество полученных гиперссылок, сходимость результатов), которые зависят от настроек конфигурации. Каждая итерация данного процесса заключается в последовательном выполнении следующих основных модулей (рис.~\cref{fig:kernelArchitecture}).

\textit{Многопоточный загрузчик Веб-страниц.} Данный модуль исполняет роль загрузчика Веб-страниц с сервера, на котором они расположены в Веб-пространстве. Также он является менеджером безопасности, который контролирует количество потоков, выделяемое для загрузки всех информационных источников в рамках одной итерации, и не обрабатывает Веб-ресурсы, время отклика которых превышает заданное. Все параметры менеджера загрузок настраиваемые, по умолчанию число потоков равно \textit{10}, а время отклика -- \textit{2 секунды}.

\textit{Модуль извлечения гиперссылок (URLs extraction)}, который ищет дочерние элементы из заданного набора Веб-страниц. Поиск таких элементов осуществляется путем извлечения всех гиперссылок, находящихся в начальном множестве веб-страниц, и их добавления в очередь гиперссылок. Процесс обработки элементов очереди выполняется с помощью набора параллельных и синхронизированных между собой потоков, которые независимо извлекают ссылки и добавляют результаты обработки в общую коллекцию полученных дочерних Веб-страниц.

\textit{Модуль синтаксического анализа Веб-страниц (HTML parser)} -- извлекает текст из загруженных Веб-страниц и определяет его кодировку.

\textit{Нормализация гиперссылок (URLs normalization).} После того, как из начального множества Веб-страниц извлечены все гиперссылки, необходимо отфильтровать их адреса. За данную операцию в обобщенном ядре отвечает модуль фильтрации адресов для Веб-страниц. Основная задача такого модуля заключается в приведении адреса каждого Веб-ресурса к стандартизированному виду по заданным критериям \cite{LeeKimHong} (например, имя хоста и протокола каждого ресурса должны состоять из символов нижнего регистра или адреса всех страниц должны приводиться к одинаковой канонической форме, то есть, заканчиваться символом «/»). Другими словами, модуль фильтрации отсеивает Веб-страницы с некорректными и дублированными адресами, тем самым улучшая производительности работы поискового робота.

\textit{Модуль кэширования.} Операция извлечения содержимого Веб-страниц затрачивает ресурсы системы (интенсивность затрат ресурсов системы зависит от роста числа Веб-страниц), что значительным образом влияет на ее производительность. Поэтому была разработана система кэширования, предназначенная для ускорения процесса извлечения гиперссылок с Веб-страниц за счет повторного использования ранее извлеченного контента. Хранение контента продолжается до тех пор, пока не превышен допустимый порог памяти компьютера, используемой приложением. Данный порог приложение определяет динамически, исходя из имеющегося объема и требований по использованию ресурсов компьютера, а также производительности. При превышении порога система освобождает память путем удаления из кэша контента Веб-страниц, которые реже всего используются.

\textit{Коллекция найденных документов.} Данный модуль хранит информацию обо всех веб-ресурсах и их гиперссылочных структурах, полученных Веб-краулером на каждой итерации краулер-процесса, и предоставляет ее пользователю для дальнейших вебометрических исследований.

\begin{figure}[ht]
	\centerfloat{
		\includegraphics[scale=0.6]{kernelArchitecture}
	}
	\caption{Архитектура обобщенного ядра.}\label{fig:kernelArchitecture}
\end{figure}

Данная архитектура обобщенного ядра предоставляет конфигурируемые настройки интеграции и интерфейсы, использование которых существенно упрощают процесс и минимизируют время добавления в архитектуру новых модулей (рис.~\cref{fig:kernelModuleLink}), для создания потенциальных внешних модулей (новые алгоритм обхода, синтаксический и семантический анализаторы и др.).

Описанная реализация поискового робота является базовым инструментом для вебометрических исследований, поэтому на ее основе могут создаваться Веб-краулеры со специализированными алгоритмами обхода Веба.

\begin{figure}[ht]
	\centerfloat{
		\includegraphics[scale=0.6]{kernelModuleLink}
	}
	\caption{Связь обобщенного ядра с внешними модулями.}\label{fig:kernelModuleLink}
\end{figure}

\paragraph{Архитектурные особенности обобщенного ядра.} Перед экспериментом в данных исследованиях проводилась оценка архитектурных возможностей обобщенного ядра и зарубежных аналогов Веб-краулеров (Heritrix, OpenWebSpider, Methanol Web Crawler) по наличию следующей функциональности (табл.~\cref{tab:webCrawlerComparison}):
\begin{itemize}
	\item \textit{возможность многопоточной загрузки Веб-страниц;}
	\item \textit{масштабирования процесса сбора Веб-страниц (на программно-аппаратном уровне);}
	\item \textit{минимизация нагрузки на информационные Веб-ресурсы;}
	\item \textit{минимизации нагрузки на каналы связи;}
	\item \textit{гибкость архитектуры (возможность добавлять новые модули, алгоритмы обхода);}
	\item \textit{возможность специализированного обновления информационных Веб-ресурсов в индексе;}
	\item \textit{учет разновидностей форматов Веб-ресурсов;}
	\item \textit{обработка текста Веб-страниц;}
	\item \textit{приспособленность к российскому сегменту Веба;}
	\item \textit{приспособленность к англоязычному сегменту Веба.}
\end{itemize}

\begin{table} [htbp]%
	\centering
	\caption{Сравнение особенностей архитектуры Веб-краулеров.}%
	\label{tab:webCrawlerComparison}% label всегда желательно идти после caption
	\renewcommand{\arraystretch}{1.5}%% Увеличение расстояния между рядами, для улучшения восприятия.
	\begin{SingleSpace}
		\begin{tabulary}{\textwidth}{@{}>{\zz}L >{\zz}C >{\zz}C >{\zz}C >{\zz}C@{}}%Вертикальные полосы не используются принципиально, как и лишние горизонтальные (допускается по ГОСТ 2.105 пункт 4.4.5) % @{} позволяет прижиматься к краям
			\toprule     %%% верхняя линейка
			Функциональность &Heri-\linebreak trix & OpenWeb Spider & Methanol Web Crawler & Обобщенное ядро \\
			\midrule %%% тонкий разделитель. Отделяет названия столбцов. Обязателен по ГОСТ 2.105 пункт 4.4.5
			Многопоточная загрузка &+ &+ &+ &+ \\				
			Масштабирования процесса сбора Веб-ресурсов & -- & -- & -- &+ \\
			Минимизация нагрузки на Веб-ресурсы & + &+ &+ &+ \\
			Минимизации нагрузки на каналы связи &+ & -- & -- &+\\
			Гибкость архитектуры & --& -- & -- &+ \\
			Обновление индекса & -- & -- & -- &+ \\
			Учет разновидностей форматов Веб-ресурсов &+ &+ &+ &+ \\
			Обработка текста Веб-страниц &  -- & -- & -- &+ \\
			Адаптированность к рос. Вебу & -- & -- & -- &+ \\
			Адаптированность к англ. Вебу &+ &+ &+ &+ \\
			\bottomrule %%% нижняя линейка
		\end{tabulary}%
	\end{SingleSpace}
\end{table}

Оценка архитектурных возможностей показала, что обобщенное ядро и Heritrix имеют гибкие настройки (например, объем скачиваемой информации с одного источника, время доступа к серверу Веб-ресурса, количество итераций краулер-процесса и др.), которые при получении и обработке информационных Веб-ресурсов легко позволяют контролировать нагрузку на ресурсы и каналы связи (табл.~\cref{tab:webCrawlerComparison}). Веб-краулеры OpenWebSpider и Methanol Web Crawler плохо настраиваются (OpenWebSpider конфигурируется в течение \textit{34 минут}, а Methanol Web Crawler -- \textit{30 минут} (табл.~\cref{tab:crawlersEnglish},~\cref{tab:crawlersRussian})) и при обработке информации источников максимально нагружают каналы связи и используют ресурсы обрабатываемого источника. Вместе с этим зарубежные аналоги плохо адаптированы к российскому Вебу, что подтверждается далее в эксперименте.

\paragraph{Эксперимент.} В эксперименте ставилась задача сравнения эффективности работы четырех реализаций Веб-краулеров:
\begin{itemize}
	\item \textit{Веб-краулер на основе обобщенного ядра} с тремя дополнительными модулями: \textit{Module\_BDD}, \textit{Module\_MDD} и \textit{Module\_SDD}, ориентированные на загрузку большого количества Веб-страниц, среднего и малого, соответственно;
	\item \textit{Heritrix (http://crawler.archive.org)} \cite{MohrKimptonStack}, имеющий хорошую производительность при обработке большого количества Веб-страниц;
	\item \textit{Methanol Web Crawler (http://metha-sys.org)}, имеющий хорошую производительность при обработке малого количества с Веб-страниц;
	\item \textit{Open WebSpider (http://www.openwebspider.org)}, имеющий хорошую производительность при обработке небольшого количества с Веб-страниц.
\end{itemize}

Сравнивалась производительность работы и время конфигурирования Веб-краулеров на основе обобщенного ядра с зарубежными аналогами на примере загрузки \textit{2000}, \textit{300000} и \textit{500000} Веб-страниц.

Производительность поисковых роботов проверялась в российском и англоязычном сегментах Веба при одинаковом тестовом наборе данных (ЭВМ, ОС, начальное множество ссылок в очереди и др.). В качестве начальных параметров были заданы начальное множество Веб-страниц для каждого из сегмента (по \textit{10} Веб-страниц), и частота запуска, равная \textit{10}.

\paragraph{Результаты эксперимента.} В ходе сравнительного анализа производительности Веб-краулера на основе обобщенного ядра (с подключенными дополнительными модулями \textit{Module\_BDD}, \textit{Module\_MDD} и \textit{Module\_SDD}) с зарубежными аналогами (Heritrix, OpenWebSpider, Methanol Web Crawler) были получены следующие результаты (табл.~\cref{tab:crawlersEnglish},~\cref{tab:crawlersRussian}):

\begin{table} [htbp]%
	\centering
	\caption{Средняя производительность обработки информации Веб-краулерами в англоязычном сегменте Веб-пространства и время их настройки.}%
	\label{tab:crawlersEnglish}% label всегда желательно идти после caption
	\renewcommand{\arraystretch}{1.5}%% Увеличение расстояния между рядами, для улучшения восприятия.
	\begin{SingleSpace}
		\begin{tabulary}{\textwidth}{@{}>{\zz}L >{\zz}C >{\zz}C >{\zz}C >{\zz}C@{}}%Вертикальные полосы не используются принципиально, как и лишние горизонтальные (допускается по ГОСТ 2.105 пункт 4.4.5) % @{} позволяет прижиматься к краям
			\toprule     %%% верхняя линейка
			Веб-краулеры & Время загрузки 2000 страниц (мин.) & Время загрузки 30000 страниц (мин.) & Время загрузки 50000 страниц (мин.) & Время настройки Веб-краулера (мин.) \\
			\midrule %%% тонкий разделитель. Отделяет названия столбцов. Обязателен по ГОСТ 2.105 пункт 4.4.5
			Heritrix & 1.9 & 8.0 & 11.2 & 20.0 \\				
			OpenWebSpider & 3.0 & 8.4 & 13.0 & 34.0 \\
			Methanol & 1.9 & 8.6 & 14.1 & 30.0 \\			
			Обобщенное ядро с \textit{Module\_BDD} модулем & -- & -- & \textbf{9.1} & 4.0\\
			Обобщенное ядро с \textit{Module\_MDD} модулем & -- & \textbf{6.0} & -- & 4.0\\			
			Обобщенное ядро с \textit{Module\_SDD} модулем & \textbf{1.7} & -- & -- & 4.0\\		
			Обобщенное ядро с модулем анализа текста и ссылок & \textbf{2.0} & \textbf{8.5} & \textbf{11.5} & 4.0\\		
			\bottomrule %%% нижняя линейка
		\end{tabulary}%
	\end{SingleSpace}
\end{table}

\begin{table} [htbp]%
	\centering
	\caption{Средняя производительность обработки информации Веб-краулерами в российском сегменте Веб-пространства и время их настройки.}%
	\label{tab:crawlersRussian}% label всегда желательно идти после caption
	\renewcommand{\arraystretch}{1.5}%% Увеличение расстояния между рядами, для улучшения восприятия.
	\begin{SingleSpace}
		\begin{tabulary}{\textwidth}{@{}>{\zz}L >{\zz}C >{\zz}C >{\zz}C >{\zz}C@{}}%Вертикальные полосы не используются принципиально, как и лишние горизонтальные (допускается по ГОСТ 2.105 пункт 4.4.5) % @{} позволяет прижиматься к краям
			\toprule     %%% верхняя линейка
			Веб-краулеры & Время загрузки 2000 страниц (мин.) & Время загрузки 30000 страниц (мин.) & Время загрузки 50000 страниц (мин.) & Время настройки Веб-краулера (мин.) \\
			\midrule %%% тонкий разделитель. Отделяет названия столбцов. Обязателен по ГОСТ 2.105 пункт 4.4.5
			Heritrix & 5.0 & 12.9 & 22.3 & 20.0 \\				
			OpenWebSpider & 5.5 & 13.0 & 25.0 & 34.0 \\
			Methanol & 4.2 & 13.5 & 26.5 & 30.0 \\			
			Обобщенное ядро с \textit{Module\_BDD} модулем & -- & -- & \textbf{10.5} & 4.0\\
			Обобщенное ядро с \textit{Module\_MDD} модулем & -- & \textbf{6.4} & -- & 4.0\\			
			Обобщенное ядро с \textit{Module\_SDD} модулем & \textbf{2.0} & -- & -- & 4.0\\		
			Обобщенное ядро с модулем анализа текста и ссылок & \textbf{3.5} & \textbf{9.0} & \textbf{13.0} & 4.0\\		
			\bottomrule %%% нижняя линейка
		\end{tabulary}%
	\end{SingleSpace}
\end{table}

\begin{itemize}
	\item получено небольшое время добавления внешних модулей (алгоритмы обхода Веб-пространства, синтаксические анализаторы, методы обновления индекса и др.) к обобщенному ядру и их конфигурирования (табл.~\cref{tab:webCrawlerComparison},~\cref{tab:crawlersEnglish}) по сравнению с зарубежными аналогами, архитектура которых не предусматривает возможность масштабирования и добавления в нее других модулей и время настройки в среднем занимает \textit{28 минут}.
	\item Веб-краулер с обобщенным ядром имеет возможность добавления в него специализированных модулей и их адаптации для тематического поиска (рис.~\cref{fig:crawlerModes}): классический (рекурсивно обходит все встречающиеся гиперссылки); интересо-ориентированный; ориентированный на популярность; ориентированный на интересы и популярность. В исследованиях \cite{BlekanovBondarenko1,BlekanovBondarenko2} проверялась качество поиска и полезность этих модулей, а также их интеграция с обобщенным ядром. В то время как Heritrix, OpenWebSpider и Methanol Web Crawler используют только классический режим обхода Веб-пространства и не обрабатывают текст источников информации в Вебе, но поддерживают возможность поиска музыкальных, видео файлов, файлов PDF.
	
	\begin{figure}[ht]
		\centerfloat{
			\includegraphics[scale=0.6]{crawlerModes}
		}
		\caption{Режимы поиска Веб-краулера с обобщенным ядром.}\label{fig:crawlerModes}
	\end{figure}
	
	\item Веб-краулеры на основе обобщенного ядра с разными модулями загрузки Веб-страниц (\textit{Module\_BDD}, \textit{Module\_MDD} и \textit{Module\_SDD}) в классическом режиме (обрабатывая информацию только о ссылках) показали высокую производительность среди зарубежных аналогов (табл.~\cref{tab:crawlersEnglish},~\cref{tab:crawlersRussian}), как в российском, так и зарубежном сегменте Веба. Вместе с этим поисковый робот на основе обобщенного ядра с модулем анализа текста не уступает по производительности зарубежным аналогам, которые не обрабатывают текстовую информацию Веб-страниц, в англоязычном сегменте Веба, а в российском Вебе зарубежные реализации Веб-краулеров сильно уступают в производительности. 
	\item зарубежные аналоги показали низкую производительность обработки информационных Веб-ресурсов в российском сегменте Веба (табл.~\cref{tab:crawlersEnglish}), а \textit{70\%} ресурсов из этого сегмента Веб-краулеры Heritrix, OpenWebSpider и Methanol Web Crawler не смогли загрузить и обработать. Данный факт объясняется низкой приспособленностью зарубежных аналогов к российскому Вебу.
	\item Среди зарубежных аналогов неплохую эффективность загрузки и обработки \textit{2000}, \textit{300000} и \textit{500000} Веб-страниц показал Веб-краулер Heritrix. Его можно использовать как неплохой инструмент для поиска (с классическим алгоритмом обхода Веба) разнородной информации и добавления ее в индекс.
\end{itemize}

Таким образом, из поставленного эксперимента следует, что Веб-краулер с обобщенным ядром эффективней собирает и обрабатывает информацию с Веб-ресурсов, чем зарубежные аналоги, которые сильно уступают по производительности, гибкости и масштабируемости архитектуры, а также приспособленностью к обработке информации в российском сегменте Веба.

В дальнейшем созданный Веб-краулер с обобщенным ядром планируется использовать в прикладных исследованиях по повышению вебометрических рейтингов университетов, научных институтов и других научных организаций.

\subsection{Веб-краулер как инструмент для вебометрических исследований на примере анализа Веб-пространства СПбГУ}\label{subsec:ch1/sec3/sub2}

\paragraph{Введение.} За последние несколько лет в области информационного Веб-поиска все чаще появляются задачи, связанные с развивающимся научным направлением вебометрика (webometrics) \cite{HolmbergThelwall,Pechnikov,PechnikovChirkovChuiko,BlekanovSergeevPechnikov}. К актуальным направлениям вебометрических исследований относятся задачи анализа и выявления гиперссылочных структур различных сегментов Веб-пространства (например, академический сегмент Веба, университетский, и др.), решение которых влияет на качество присутствия этих сегментов в Вебе, на результаты ранжирования поисковых машин (Google, Yandex и др.) или, в случае университетского Веба, на вебометрический рейтинг (http://www.webometrics.info) различных университетов мира \cite{BlekanovSergeevPechnikov}.

 Для получения и обработки больших объемов информации о веб-сайтах и их гиперссылках используются Веб-краулеры (поисковые роботы), общей задачей которых является специализированный обход Веба с целью сбора информации или определения гиперссылочной структуры и полезности каких-либо информационных ресурсов.
 
\paragraph{Эксперимент.} В эксперименте ставилась задача анализа и выявления гиперссылочной структуры Веб-пространства Санкт-Петербургского государственного университета (СПбГУ). 
 
 Для эксперимента использовался программный комплекс обобщенного ядра поискового робота, который обладает высокой гибкостью и масштабируемостью в сравнении с зарубежными аналогами, сильно уступающими в производительности собора и обработки веб-ресурсов и имеющими слабую приспособленность к анализу российского сегмента Веба \cite{BlekanovSergeevMartynenko}. 
 
 К Веб-краулеру с обобщенным ядром дополнительно был разработан и добавлен специализированный алгоритм обхода веб-страниц, который собирает и обрабатывает только страницы из Веб-пространства СПбГУ. В свою очередь пространство СПбГУ состоит из веб-сайта главного домена и сайтов всех его поддоменов (рис.~\cref{fig:spbuWebSpace}).
 
\begin{figure}[ht]
    \centerfloat{
	        \includegraphics[scale=0.5]{spbuWebSpace}
	    }
    \caption{Веб-пространство СПбГУ.}\label{fig:spbuWebSpace}
\end{figure}

Используя программный комплекс на основе обобщенного ядра поискового робота со специализированным алгоритмом, запущенного с начального множества веб-страниц, требовалось в автоматизированном режиме получить значения следующих показателей, характеризующих гиперссылочную структуру Веб-пространства СПбГУ:
\begin{itemize}
	\item объем Веб-пространства СПбГУ (количество всех различных веб-страниц из Веб-пространства СПбГУ);
	\item количество всех поддоменов из Веб-пространства СПбГУ; 
	\item количество тупиковых (не имеющих ссылок) веб-страниц; 
	\item количество неработающих гиперссылок; 
	\item количество гиперссылок на внешние веб-ресурсы; 
	\item количество поддоменов, связанных с «Главной страницей» главного домена; 
	\item количество поддоменов, несвязанных с «Главной страницей» главного домена; 
	\item количество страниц, имеющие гиперссылки на «Главную страницу» главного домена; 
	\item количество страниц, не имеющие гиперссылки на «Главную страницу» главного домена; 
	\item гиперссылочная структура Веб-пространства СПбГУ в виде матрицы смежности. 
\end{itemize}

В качестве начального множества веб-страниц, с которого Веб-краулер запускал процесс сбора и обработки веб-ресурсов, брался URL-адрес главного веб-сайта СПбГУ -- «http://www.spbu.ru/».

\paragraph{Результаты эксперимента.} В ходе эксперимента всего Веб-краулером было обработано и проанализировано \textit{6\space429\space963} гиперссылки, которые содержались на страницах Веб-пространства СПбГУ. Из них: объем ссылок на внешние источники информации равен \textit{507\space168}, а объем внутренних ссылок (на страницы главного домена сайта СПбГУ и его поддоменов) -- \textit{5\space922\space795}. Кроме того, были получены следующие результаты (табл.~\cref{tab:indicators}):

\begin{table} [htbp]%
	\centering
	\caption{}%
	\label{tab:indicators}% label всегда желательно идти после caption
	\renewcommand{\arraystretch}{1.5}%% Увеличение расстояния между рядами, для улучшения восприятия.
	\begin{SingleSpace}
		\begin{tabulary}{\textwidth}{@{}>{\zz}L >{\zz}C@{}} %Вертикальные полосы не используются принципиально, как и лишние горизонтальные (допускается по ГОСТ 2.105 пункт 4.4.5) % @{} позволяет прижиматься к краям
			\toprule     %%% верхняя линейка
			Показатель & Значение показателя  \\
			\midrule %%% тонкий разделитель. Отделяет названия столбцов. Обязателен по ГОСТ 2.105 пункт 4.4.5
			объем Веб-пространства СПбГУ & 71688 \\				
			количество всех поддоменов из Веб-пространства СПбГУ & 315 \\
			количество поддоменов, сильно-связанных с «Главной страницей» главного домена & 280 \\
			количество поддоменов, несвязанных с «Главной страницей» главного домена & 35 \\
			количество тупиковых веб-страниц & 12516 \\
			количество неработающих гиперссылок & 680 \\
			количество страниц, имеющие гиперссылки на «Главную страницу» главного домена & 10632 \\
			количество страниц, не имеющие гиперссылки на «Главную страницу» главного домена & 61056 \\
			\bottomrule %%% нижняя линейка
		\end{tabulary}%
	\end{SingleSpace}
\end{table}

Также стоит отметить, что для простоты исследований и получения численных значений показателей было предложено хранить выявленную гиперссылочную структуру Веб-пространства СПбГУ в виде матрицы смежности A размерностью [71688\(\times\)71688]. С помощью нее и ее транспонированной формы удобно определять связанность между страницами и в дальнейшем оптимизировать структуру ссылок. 

\paragraph{Общие выводы.} По результатам эксперимента видно, что \textit{17,5\%} веб-страниц из всех страниц Веб-пространства СПбГУ занимают тупиковые страницы, которые, например, не учитывает поисковая машина Google в своем алгоритме ранжирования PageRank, что негативно влияет на коммуникабельность всего Веб-пространство университета в целом. 

Также из эксперимента наблюдается плохая связанность страниц Веб-пространства СПбГУ и его поддоменов со страницами главного сайта университета: только \textit{14,8\%} всех веб-страниц имеет ссылки на страницы главного сайта, а \textit{35} доменов со всеми своими страницами и вовсе никак не связаны с ним. 

Стоит отметить и большое количество неработающих гиперссылок (\textit{680} уникальных гиперссылок), которые неоднократно повторяются по всему Веб-пространству СПбГУ, тем самым снижают доверие пользователей к ресурсам сайтов университета. 

Таким образом, проведенный эксперимент демонстрирует слабую связанность и коммуникабельность внутренних ресурсов Веб-пространства СПбГУ, что влечет и его слабую позицию в вебометрическом рейтинге сайтов университетов мира.

\subsection{Вебометрические исследования сегмента университетского Веба с помощью поискового робота}\label{subsec:ch1/sec3/sub3}

\paragraph{Введение.} За последние годы все больше российских ВУЗов следят за позициями своих веб-сайтов в вебометрическом рейтинге испанской группы Cybermetrics Lab \cite{RankingWeb}, который входит в пятерку мировых рейтингов наряду с такими рейтингами, как Times, QS, шанхайский и т. п. Этот рейтинг оценивает лишь качество сайта университета, но качество веб-сайта оказывает большое влияние на «видимость» университета в целом. Ведь по публикациям ученых видны только они, но не виден университет.

Для повышения рейтинга университетского сайта необходимо работать в таких направлениях, как увеличение объема, улучшение качества контента, увеличение количества гиперссылок на сайт с авторитетных веб-сайтов, а также оптимизация внутренней гиперссылочной структуры сайта. Исследованиям в этой области посвящены работы \cite{HolmbergThelwall,Pechnikov,Pechnikov2,PechnikovChirkovChuiko,BlekanovSergeevPechnikov,BlekanovSergeevMaksimov,BlekanovSergeev}. Мы же решили сосредоточиться на вопросах внутренней гиперссылочной структуры как целого Веб-пространства университета, так и отдельных сегментов, из которых оно состоит. Правильно организованная веб-структура может значительно увеличить коммуникабельность сайта и повысить его показатель цитируемости в поисковых системах, увеличить количество внешних ссылок, улучшить позиции университетского веб-сайта в вебометрическом рейтинге.

\paragraph{Эксперимент.} В эксперименте ставилась задача анализа и выявления гиперссылочной структуры одного из сегментов, составляющих Веб-пространство СПбГУ, -- веб-сайт факультета ПМ–ПУ, а также получения рекомендаций по улучшению этой структуры.

В качестве инструмента для достижения поставленной в эксперименте цели использовалась аналитическая система на основе специ- ализированного поискового робота, которая была успешно апробирована на исследованиях зарубежного и российского сегментов Веба \cite{BlekanovSergeevPechnikov,BlekanovSergeevMaksimov,BlekanovSergeev,BlekanovSergeevMartynenko}.

Специализация используемого поискового робота была сконфигурирована таким образом, чтобы робот собирал и обрабатывал только страницы в рамках сегмента Веба, содержащего веб-сайт факультета ПМ–ПУ СПбГУ, и гиперссылки с этого сайта на внешние источники информации (рис.~\cref{fig:spbuApmathWebSpace}).

\begin{figure}[ht]
	\centerfloat{
		\includegraphics[scale=1]{spbuApmathWebSpace}
	}
	\caption{Веб-пространство СПбГУ и включенный в него исследуемый сегмент Веба.}\label{fig:spbuApmathWebSpace}
\end{figure}

С помощью аналитической системы на основе специализированного поискового робота, запущенной с начальным URL-адресом http://www.apmath.spbu.ru/, требовалось выявить гиперссылочную структуру сайта ПМ–ПУ в виде матрицы смежности и определить значения параметров, приведенных в таблице~\cref{tab:researchedParameters}.

\begin{table} [htbp]%
	\centering
	\caption{Исследуемые параметры.}%
	\label{tab:researchedParameters}% label всегда желательно идти после caption
	\renewcommand{\arraystretch}{1.5}%% Увеличение расстояния между рядами, для улучшения восприятия.
	\begin{SingleSpace}
		\begin{tabulary}{\textwidth}{@{}>{\zz}L >{\zz}C@{}} %Вертикальные полосы не используются принципиально, как и лишние горизонтальные (допускается по ГОСТ 2.105 пункт 4.4.5) % @{} позволяет прижиматься к краям
			\toprule     %%% верхняя линейка
			ID параметра & Параметр  \\
			\midrule %%% тонкий разделитель. Отделяет названия столбцов. Обязателен по ГОСТ 2.105 пункт 4.4.5
			1 & Общее количество уникальных внутренних страниц сайта ПМ–ПУ \\				
			2 & Количество страниц, связанных с «Главной страницей» сайта ПМ–ПУ \\
			3 & Количество страниц, не связанных с «Главной страницей» сайта ПМ–ПУ \\
			4 & Количество тупиковых веб-страниц \\
			5 & Количество уникальных гиперссылок сайта ПМ–ПУ \\
			6 & Количество неработающих гиперссылок0 \\
			7 & Количество гиперссылок на внешние веб-ресурсы \\
			8 & Количество внутренних гиперссылок на документы форматов *.doc (*.docx), *.pdf, *excel, *.ppt \\
			9 & Количество гиперссылок с переадресацией \\
			\bottomrule %%% нижняя линейка
		\end{tabulary}%
	\end{SingleSpace}
\end{table}

\paragraph{Результаты эксперимента.} В ходе эксперимента поисковым роботом на веб-сайте факультета ПМ–ПУ была обработана и проанализирована \textit{2 258 521} гиперссылка. Из них \textit{2 026 285} гиперссылок на внутренние веб-страницы сайта ПМ–ПУ, \textit{119 931} ссылка на страницы из Веб-пространства СПбГУ и \textit{112 305} ссылок на внешние источники информации. Кроме того, с помощью построенной матрицы смежности и ее транспонированной формы для параметров из таблицы~\cref{tab:researchedParameters} были получены числовые значения в виде гистограмм и круговых диаграмм (рис.~\cref{fig:histogramOfTab1},~\cref{fig:linkDiagram}).

\begin{figure}[ht]
	\centerfloat{
		\includegraphics[scale=0.5]{histogramOfTab1}
	}
	\caption{Гистограммы значений показателей таблицы~\cref{tab:researchedParameters}.}\label{fig:histogramOfTab1}
\end{figure}

\begin{figure}[ht]
	\centerfloat{
		\includegraphics[scale=0.5]{linkDiagram}
	}
	\caption{Круговая диаграмма внешних и внутренних ссылок.}\label{fig:linkDiagram}
\end{figure}

\paragraph{Общие выводы.} По результатам эксперимента видно, что тупиковые страницы (не имеющие никаких других ссылок), не являющиеся медиа-контентом, отсутствуют. В свою очередь, 16,2\% страниц не связаны с главной веб-страницей главного сайта. Однако если отбросить страницы с документами, то это отношение составит всего около 1\%.
Несмотря на относительно неплохую связность страниц сайта и отсутствие тупиковых страниц, ситуацию ухудшает огромное число гиперссылок, переадресующих пользователя в основном на главную страницу сайта или ее иностранные аналоги. Кроме того, не все гиперссылки приводятся к единой нормальной форме на стороне сервера. Большое количество таких гиперссылок влекут вероятность некорректного цитирования информационных ресурсов веб-сайта ПМ–ПУ в поисковых системах, а также оказывают негативное влияние на вебометрический рейтинг университета в целом.
Стоит отметить и большое количество веб-страниц, при переходе на которые возникает ошибка, вызванная отсутствием предполагаемых данных в указанном месте сервера или неверной обработкой адреса. Наличие таких страниц снижает доверие пользователей к ресурсам сайта факультета.
Таким образом, для улучшения присутствия веб-сайта факультета ПМ–ПУ в Вебе и улучшения его показателя цитируемости в поисковых системах необходимо разрешить выявленные в эксперименте проблемы на практике.

\subsection{Noise Removal Methods from Web Pages}\label{subsec:ch1/sec3/sub4}

\paragraph{Intoduction.} For today, in connection with the large number of websites it became necessary to analyze content of Web pages for further information processing (information retrieval, classification, clustering) \cite{Chakrabarti2}. Humans can easily distinguish the main page content from various noisy information such as navigation menus, header and footer elements, advertising and other text portions during website examining. This main content can be considered as semantically significant information. Normally, the Web crawlers that gather Web pages eliminate noise extract semantically significant information and store it for further analysis. There is a variety of techniques for noise detection in Web pages \cite{YiLiuLi}. In this paper we consider three approaches of noisy information removal that are based on the following tools: Boilerpipe library, HTML5 semantic markup, Headless browser with Selenium.

This paper is structured as follows. The Section 2 describes methods that can be used for extraction of main textual content. An experiment to assess the quality of the methods described is presented in the Section 3. The Section 4 is devoted to conclusion.

\subsubsection{Noise removal tools.}

\paragraph{Boilerpipe library.} The Boilerpipe library was written by Christian Kohlschutter for Java platform and released under the Apache License 2.0. It is based on the algorithms that provide detection and removal the surplus "clutter" (templates, boilerplate) which is placed around the main textual content of a Web page. Nowadays, the library supplies specific approaches for common task (e.g., extraction of the news articles) and can be easily enlarged for personal problem issues. The paper "Boilerplate Detection using Shallow Text Features" was the basis of the Boilerpipe library that uses and extends algorithms from it \cite{KohlschutterFankhauserNejdl}. Analysis of small set of shallow text features is the main approach for classifying the particular text elements of a Web page. The main components of the Boilerpipe library are listed below.

\textit{HTML parser.} The main purpose of the HTML parser is to transform an HTML page into set of text "blocks" that can be considered as an internal text-only document model. The parser is based on the third-party library, CyberNeko \cite{CyberNeko}. It transforms an HTML document into a TextDocument which consists of one or more TextBlocks. HTML parser can distinguish specific elements such as Script, Option, etc. and ignore them automatically. Each TextBlock contains a text portion from the HTML document and shallow text statistics for it (e.g., words number and words number in the anchor text).

\textit{Filters.} Every filter is applied to the TextDocument and normally iterates once over all TextBlocks from it. Every individual TextBlock is marked by filter as content or boilerplate. Also filter can assign additional textual label to it. Boilerpipe's filters are grouped as follows:

\begin{enumerate}
	\item Basic filters.
	\item English filters, that can be applied to English text (they might also be applied to the other Western languages, but some settings perhaps need to be changed).
	\item Heuristics Filters were not explored but will be investigated in the future.
\end{enumerate}

\textit{Extractors.} Extractors are formed by one or more Filters. Their main purpose is to obtain content from a Web page. For instance, extractor that uses "pipelines" filters takes the parsed HTML document and extracts the main textual content from it. There are several different extractors, from generic DefaultExtractor to special extractors (e.g., ArticleExtractor which is used for news articles).

\textit{HTML highlighter.} HTML highlighter is an additional tool to represent extracted main content of a Web page as an HTML document.

The algorithms used in Boilerpipe are quite content-independent. However, if the page contains insufficient HTML text (i.e., PDF, Flash or JavaScript) the correctness of the library work is not guaranteed.

\paragraph{HTML5 Semantic Markup.} Such tags as <div> and <span> have little meaning for those who examines an HTML code whether it is machine or human. <div> elements are typically designed for positioning the content on the page. <span> elements are responsible for special formatting of the content. Developers have been using <div> elements to combine page layout, and the developer usually provides the meaning of each <div> element which is based on its id or CSS class.

\begin{figure}[ht]
	\centerfloat{
		\includegraphics[scale=0.5]{blogLayout}
	}
	\caption{A blog site layout container using <div> elements.}\label{fig:blogLayout}
\end{figure}

HTML5 separates presentation, structure and behavior. Semantic is defined as the study of meaning of linguistic expressions \cite{Johnson}. The HTML5 standard introduces tags that do not serve for presentation purposes but provide meaning. Users usually do not read an HTML code during website surfing, but many machines read it to interpret the Web page. Moreover, Nonvisual Desktop Access (NVDA) devices can provide other means of Web pages processing.

HTML5 semantic elements can be used for creation of layout container which has elements that are meaningful to both the developer and the machine. The following are common elements by which an HTML5 layout container can be created:

\begin{itemize}
	\item <header> specifies a header section of HTML document and can be used as a page header. Moreover, it can be place at the <article> element.
	\item <footer> specifies a section that is placed at the bottom of the HTML document. Moreover, it can be placed at the bottom of the <article> tag content.
	\item <nav> specifies a section that contains a block of links used for navigational purposes.
	\item <aside> specifies a section that is normally used for sidebars.
	\item <section> specifies a section that contains <h1> to <h6> internal elements.
	\item <article> specifies a section that can be considered as a unit of content (e.g., blog posts, news articles).
\end{itemize}

In Figure~\cref{fig:blogLayoutNew}, all <div> elements have been replaced with the HTML5 semantic elements.

\begin{figure}[ht]
	\centerfloat{
		\includegraphics[scale=0.5]{blogLayoutNew}
	}
	\caption{Layout container example, using the new HTML5 elements.}\label{fig:blogLayoutNew}
\end{figure}

Web Accessible Initiative (WAI) specifies the Accessible Rich Internet Applications (ARIA) suite (WAI-ARIA). WAI-ARIA defines a role classes that can used for providing additional meaning for HTML page elements. For instance, screen readers can use it for accessibility purposes \cite{Thiessen}. There are several parent and child role classes. "Landmark" class is a parent role class that describes regions of the Web page and can be set as an attribute of the HTML tag. The following are child classes of the "Landmark" role class:

\begin{itemize}
	\item \textit{Application} defines an area that is declared as a Web application.
	\item \textit{Banner} defines an area with site-specific content (e.g., site name, logo).
	\item \textit{Complementary} defines an area for an additional page content that can have different meaning than main textual content. 
	\item \textit{Contentinfo} defines an area that contains information about document (copyright notices, links). It is typically footer content, one per Web page.
	\item \textit{Form} defines an area that contains input controls sending gathered data to server.
	\item \textit{Main} defines an area for the main content of Web page.
	\item \textit{Navigation} defines an area for navigational links.
	\item \textit{Search} defines an area of input controls for query entering and displaying corresponding information.
\end{itemize}

An <article> tag can be used for noise removal from Web pages. Furthermore, text that is placed into tags with Role attribute which have an appropriate value (i.e., Main) can be also considered as semantically significant.

\paragraph{Headless browser.} One of the methods of Web site content extraction is based on Headless browser that is a Web browser without a graphical user interface. Usage of the headless browser provides a control a Web page via a command line interface or other network communications in environment that is similar to popular browsers. Using Selenium framework that is designed for testing of Web applications user has access to various elements on the Web page \cite{Sirotkin}. Today, Selenium does not provide methods of noisy information removal. Without any additional methods Selenium framework can extract only all Web page textual content. Therefore the amount of noisy information can be determined among the all textual content of web page. Also Jaccard similarity for headless browser can be compared with Jaccard similarity for other noise reduction methods and determine if these methods should be used for extraction of semantically important text. For instance, method is not appropriate for extraction semantically significant information in case of Jaccard similarity evaluated for this method is less than Jaccard similarity evaluated for headless browser.

\paragraph{Methods evaluation.}

For comparative analysis of methods considered in Section 2 test documents collection was chosen which contains 30 documents with different design, topics and amount of semantically significant information. In these documents all the noise was removed by expertise (manually) and the main content was extracted. This content can be considered as the expected text in comparison with the text that is obtained using the methods of noise removal. In the current paper Jaccard similarity and Shingles algorithm are used to evaluate the similarity of two texts \cite{KorelinBlekanovSergeev}.

\paragraph{Jaccard similarity.} A document represents a string of characters. Shingling is a process that creates sets of \(k\)-shingles. Define a \(k\)-shingle for a document to be any substring of length \(k\) found within the document. For instance, \(k = 2\), \(\textit{doc} = \{a, b\}, \{b, c\}, \{c, a\}\).

Similar documents to each other will have a lot of equal shingles \cite{KorelinBlekanovSergeev}.

In this paper Jaccard similarity is used to determine the similarity of two documents. The Jaccard similarity of sets \(C_1\) and \(C_2\) is the ratio of the cardinality of the intersection of \(C_1\) and \(C_2\) to the cardinality of their union \cite{SingthongchaiNiwattanakul}.

\[
\textit{Sim}(C_1, C_2) = \frac{\lvert C_1 \cap C_2\rvert}{\lvert C_1 \cup C_2\rvert}
\]

Every document can be considered as a set of shingles where every shingle contains \(k\) words. Therefore two documents can be represented as boolean matrix with two columns. Rows correspond to the elements of the universal set which is the set of all \(k\)-shingles. Columns correspond to the documents. Value of \(1\) is in the row \(E\) and in the column \(S\) if and only if the document that corresponds the column \(S\) contains the single that corresponds the row \(E\). Otherwise, it is value of \(0\). Therefore, documents similarity is the Jaccard similarity of the sets of their shingles.

\begin{figure}[ht]
	\centerfloat{
		\includegraphics[scale=0.4]{docBooleanMatrix}
	}
	\caption{Boolean matrix representing two documents.}\label{fig:docBooleanMatrix}
\end{figure}

\(\textit{Sim}(C_1, C_2) = \frac{1}{3}\) -- similarity of two documents.

\paragraph{Experimental results.} Tables~\cref{tab:noiseRemovalSmall},~\cref{tab:noiseRemovalMiddle}, and~\cref{tab:noiseRemovalLarge} present the Jaccard similarity between the expected main textual content and the text that was extracted by the methods discussed above. Shingles of different lengths (2, 5, 10 and 15 words per shingle) are used. The test set of documents was divided into 3 groups: sites with small (Table~\cref{tab:noiseRemovalSmall}), middle (Table~\cref{tab:noiseRemovalMiddle}) and large (Table~\cref{tab:noiseRemovalLarge}) main textual content.

\begin{table} [htbp]%
	\centering
	\caption{Comparison of the noise removal results for small size main textual content.}%
	\label{tab:noiseRemovalSmall}% label всегда желательно идти после caption
	\renewcommand{\arraystretch}{1.5}%% Увеличение расстояния между рядами, для улучшения восприятия.
	\begin{SingleSpace}
		\begin{tabulary}{\textwidth}{@{}>{\zz}L >{\zz}C >{\zz}C >{\zz}C >{\zz}C >{\zz}C@{}} %Вертикальные полосы не используются принципиально, как и лишние горизонтальные (допускается по ГОСТ 2.105 пункт 4.4.5) % @{} позволяет прижиматься к краям
			\toprule     %%% верхняя линейка
			Method & \(k=2\) &\( k=5\) & \(k=10\) & \(k=15\) & Average \\
			\midrule %%% тонкий разделитель. Отделяет названия столбцов. Обязателен по ГОСТ 2.105 пункт 4.4.5
			Boilerpipe library & 73\% & 71\% & 69\% & 68\% & 70.25\% \\
			HTML5 Semantic markup & 49\% & 48\% & 48\% & 47\% & 48\% \\
			Headless browser + Selenium & 24\% & 23\% & 22\% & 21\% & 22.5\%\\
			\bottomrule %%% нижняя линейка
		\end{tabulary}%
	\end{SingleSpace}
\end{table}

\begin{table} [htbp]%
	\centering
	\caption{Comparison of the noise removal results for middle size main textual content.}%
	\label{tab:noiseRemovalMiddle}% label всегда желательно идти после caption
	\renewcommand{\arraystretch}{1.5}%% Увеличение расстояния между рядами, для улучшения восприятия.
	\begin{SingleSpace}
		\begin{tabulary}{\textwidth}{@{}>{\zz}L >{\zz}C >{\zz}C >{\zz}C >{\zz}C >{\zz}C@{}} %Вертикальные полосы не используются принципиально, как и лишние горизонтальные (допускается по ГОСТ 2.105 пункт 4.4.5) % @{} позволяет прижиматься к краям
			\toprule     %%% верхняя линейка
			Method & \(k=2\) &\( k=5\) & \(k=10\) & \(k=15\) & Average \\
			\midrule %%% тонкий разделитель. Отделяет названия столбцов. Обязателен по ГОСТ 2.105 пункт 4.4.5
			Boilerpipe library & 83\% & 82\% & 80\% & 78\% & 80.75\% \\
			HTML5 Semantic markup & 78\% & 76\% & 73\% & 71\% & 74.5\% \\
			Headless browser + Selenium & 71\% & 70\% & 69\% & 68\% & 69.5\%\\
			\bottomrule %%% нижняя линейка
		\end{tabulary}%
	\end{SingleSpace}
\end{table}

\begin{table} [htbp]%
	\centering
	\caption{Comparison of the noise removal results for large size main textual content.}%
	\label{tab:noiseRemovalLarge}% label всегда желательно идти после caption
	\renewcommand{\arraystretch}{1.5}%% Увеличение расстояния между рядами, для улучшения восприятия.
	\begin{SingleSpace}
		\begin{tabulary}{\textwidth}{@{}>{\zz}L >{\zz}C >{\zz}C >{\zz}C >{\zz}C >{\zz}C@{}} %Вертикальные полосы не используются принципиально, как и лишние горизонтальные (допускается по ГОСТ 2.105 пункт 4.4.5) % @{} позволяет прижиматься к краям
			\toprule     %%% верхняя линейка
			Method & \(k = 2\) &\( k = 5\) & \(k = 10\) & \(k = 15\) & Average \\
			\midrule %%% тонкий разделитель. Отделяет названия столбцов. Обязателен по ГОСТ 2.105 пункт 4.4.5
			Boilerpipe library & 93\% & 88\% & 83\% & 79\% & 88.75\% \\
			HTML5 Semantic markup & 87\% & 83\% & 78\% & 75\% & 80.75\% \\
			Headless browser + Selenium & 79\% & 75\% & 71\% & 68\% & 73.25\%\\
			\bottomrule %%% нижняя линейка
		\end{tabulary}%
	\end{SingleSpace}
\end{table}

Comparative analysis shows that the more semantically significant information is present on the Web page the more Jaccard similarity between the expected main textual content and the text obtained by described methods as the more textual content is on the Web page, the more shingles are in the intersection for both documents. Moreover Jaccard similarity grows as the number of words per shingle \((k)\) decreases because for small values of \(k\) there are more shingles in the intersection than for large \(k\) value. For large size main textual content and small \(k\) value Jaccard similarity is greatest because in large size text the shingle with 2 words is much more common for two documents than the shingle that contains 10 or 15 words. For documents with small and middle size textual content the number of shingles in intersection for \(k = 2\) is not much different from number of shingles in intersection for \(k = 15\). This behavior is typical for all methods.

The highest Jaccard similarity coefficient was shown by the method that is based on Boilerpipe library. Usage of the HTML5 semantic markup eliminates noisy information with less accuracy and can be applied only for those Web sites that use HTML5 semantic markup. Headless browser with Selenium framework renders the entire Web page but cannot remove noisy information. As expected, Jaccard similarity of the last method is significantly lower than Jaccard similarity of the other methods.

\paragraph{Conclusions.} This paper describes three methods of noisy text removal for extraction of semantically significant information from Web pages. These methods were tested on a collection of Web pages that have varying markup and amount of main textual content. Experiment shows that Boilerpipe library removes noisy information with the highest accuracy for all kinds of Web pages.

\subsection{Тематический краулинг на основе алгоритма HITS}\label{subsec:ch1/sec3/sub5}

Все системы поиска информации в Веб-пространстве условно делятся на два класса по признаку наличия собственного индекса, хранящего информацию о документах, опубликованных в сети. Например, Google и Яндекс имеют собственный индекс, тогда как Quintura (http://www.search.quintura.ru/), Grokker (http:// www.grokker.com/), Metacrawler (http://www. metacrawler.com/) и многие другие своего индекса не имеют и перенаправляют запросы пользователя в прочие системы (одну или несколько), но выполняют дополнительную обработку результатов и помогают пользователю формировать запрос. Для построения индекса существенной части Веб-пространства поисковая система должна иметь систему сетевых агентов (веб-краулеры), отслеживающих появление новых (и обновление старых) публикаций в Веб-пространстве и сохраняющих индексную информацию об этих документах в индексе системы поиска. В связи с быстрым ростом Веб-пространства \cite{Kahle,HubermanAdamic} проблема построения алгоритма его обхода (как первичного, так и с целью обновления индекса) является очень сложной и актуальной.

В данной статье предлагается алгоритм обхода Веб-пространства, целью которого является посещение в первую очередь наиболее авторитетных, полезных документов. Для этого предлагается использовать модифицированный алгоритм Клеинберга HITS (в свое время был конкурентом алгоритму PageRank -- базе для Google), который в относительно небольшой тематически сфокусированной части Веб-пространства может найти страницы двух типов: авторитетные и индексные. В предлагаемом алгоритме авторитетные страницы сразу же включаются в индекс, а индексные используются для расширения поиска новых авторитетных страниц. Также оценивается качество поиска данного алгоритма обхода Веб- пространства в сравнении со стандартным алгоритмом в совокупности с ранжированием его результатов.

\paragraph{Принципы и алгоритм работы системы.} Проведенное в 1999 г. Кумаром и Бродером исследование \cite{BroderKumarMaghoul} на примере 200 млн Веб-страниц выявило, что естественной моделью представления структуры Веб-пространства является ориентированный \textit{Веб-граф} \(G(V,E)\) \cite{BroderKumarMaghoul}, в котором вершины \(v \in V\) соответствуют веб-страницам, а дуги (направленные ребра) \(e(u, v), u, v \in V, e \in E\) -- соединяющим страницы гипер-ссылкам. Расстояние \(d(u, v)\) между узлами \(u\) и \(v\) в графе без учета весов ребер равно наименьшему числу гиперссылок, по которым можно пройти из \(u\) в \(v\). В рамках этой модели задача анализа структуры связей между страницами -- это задача анализа структуры графа. Исследование Веб-графа (обход Веб-пространства) -- это процесс поиска узлов и ребер графа, начиная с некоторого начального подмножества узлов (в рамках нашей терминологии узлы -- это Веб-страницы, а ребра -- гиперссылки, соединяющие Веб-страницы). За выполнение данной процедуры отвечает Веб-краулер (сетевой агент).

По своей сути, Веб-краулер -- программа, написанная на языке высокого уровня, взаимодействующая с окружающей средой посредством сетевых протоколов, например, интернет-протокола HTTP. Задачей сетевых агентов является обход Веб-графа определенным образом с целью сбора информации или понимания структуры и полезности каких-либо Веб-страниц. А также передача собранной информации для анализа другим приложениям поисковых систем.

Веб-краулеры упрощают поиск информации в Веб-пространстве и повышают его качество и эффективность. Как правило, поисковые системы использует некоторое множество сетевых агентов, которые собирают Веб-страницы, извлекая из них контент (содержание) и индексируя его \cite{ManningRaghavanSchutze}. Полученные сетевыми агентами результаты обхода Веб-графа обрабатываются поисковой системой на предмет выявления наиболее подходящих результатов для заданного пользователем поиска. Например, поисковая машина Google дополнительно использует алгоритм PageRank, предназначенный для ранжирования результатов поиска. В процессе ранжирования для каждой Веб-страницы вычисляется вес, страницы с наибольшими весами получают высокий рейтинг в результатах поиска.

В данной статье реализованы две модели Веб-краулеров: стандартная и сфокусированная. Стандартная модель Веб-краулера выполняет классический обход Веб-графа по всем найденным узлам, начиная с некоторого заданного множества узлов \cite{ArasuChoGM}. В результате своей работы он формирует коллекцию из найденных Веб-страниц. Сфокусированная модель Веб-краулера использует алгоритм Клеинберга HITS в качестве алгоритма обхода Веб-пространства. В результирущую коллекцию Веб-страниц для такой модели попадают только приоритетные по HITS.

В основе алгоритма Клеинберга HITS лежит понятие значимости страницы. Наиболее значимыми Веб-страницами в рамках заданной темы принято считать \textit{авторитетные страницы} (\textit{authorities}), на которые ссылаются другие страницы, относящиеся к данной теме. Это свойство позволяет выявить \textit{индексные страницы} (\textit{hubs}). Авторитетные страницы -- это страницы, на которые ссылаются индексные страницы, а индексные страницы -- это страницы, которые ссылаются на авторитетные страницы. Вместе оба типа значимых страниц образуют \textit{отношение взаимного усиления}, т. е. на хорошую авторитетную страницу ссылается много хороших индексных страниц, и хорошая индексная страница ссылается на хорошие авторитетные страницы. Целью алгоритма HITS является поиск наиболее качественных авторитетных и индексных страниц в подмножестве \(S_\sigma\) Веб-графа \(G(V, E)\). Качественность Веб-страниц оценивается числовыми величинами.

Подмножество \(S_\sigma\) Веб-графа строится, например, путем посылки запроса \(\sigma\) поисковой машине по ключевым словам, и берутся первые \(n\) результатов. В ходе анализа подграфа \(S_\sigma\) каждой Веб-странице сопоставляются два веса: \(x\) -- \textit{AP-вес}, показывающий качество страницы как авторитетной, и \(y\) – \textit{HP-вес}, показывающий качество страницы как индексной. Изначально весам присваивается максимальное значение, равное единице, и для каждой страницы \(p\) задается пара неотрицательных весов (\(x^{<p>}, y^{<p>}\)), где \(x^{<p>}, y^{<p>}\) -- веса для авторитетной и для индексной страниц, соответственно.

К весам Веб-страниц применяются две операции: \(I\) и \(O\). С помощью заданных весов \(\{x^{<p>}\}\) и \(\{y^{<p>}\}\) операция \(I\) вычисляет AP-вес для страницы \(p\) следующим образом \cite{Kleinberg}: \(x^{<p>} \leftarrow \sum_{q: (p, q) \in E} y^{<q>}\), а операция \(O\) вычисляет HP-вес для страницы \(p\): \(y^{<p>} \leftarrow \sum_{q: (p, q) \in E} x^{<q>}\). Операции \(I\) и \(O\) образуют упомянутый выше процесс отношения взаимного усиления авторитетной страницы относительно индексной, и наоборот. Поэтому для нахождения стационарных значений AP и HP весов необхо- димо применить операции \(I\) и \(O\) в итерацион- ном процессе. После каждой итерации полученные веса нормируются, т. е. \(\sum_{p \in S_\sigma} (x^{<p>})^2 = 1\) и  \(\sum_{p \in S_\sigma} (y^{<p>})^2 = 1\). В результате работы алгоритма получаем набор как авторитетных, так и индексных Веб-страниц. Далее этот набор фильтруется, и из него выбираются \(c \approx 5-10\) Веб-страниц с наибольшими AP-весами и с страниц с наибольшими HP-весами. Так определяются качественные авторитетные и качественные индексные Веб-страницы.

В работе Клеинберга \cite{Kleinberg} обосновывается быстрая сходимость итерационного процесса операций \(I\) и \(O\). Там указано, что для того, чтобы двадцать наивысших значений AP-весов и HP-весов становились стабильными, достаточно около двадцати итераций.

В данной статье использовалась модифицированная реализация HITS в качестве алгоритма обхода и выбора дальнейшего пути сканирования Веб-графа сфокусированным Веб-краулером. То есть, на каждом шаге получения новых узлов Веб-краулером применялся алгоритм HITS к новому полученному множеству узлов и старому (уже обработанному) множеству. Из полученного набора Веб-страниц качественные авторитетные страницы записывались в индекс, а по качественным индексным продолжался поиск новых авторитетных. В результате обхода Веб-пространства сфокусированный краулер на основе алгоритма HITS выдаст тематическое сообщество авторитетных страниц. В работе \cite{GibsonKleinbergRaghavan} тематическим сообществом называют множество Веб-страниц, в котором каждая страница имеет больше ссылок на другие страницы сообщества, чем на страницы вне сообщества.

\paragraph{Эксперимент.} В ходе эксперимента оценивались результаты тематического краулера на основе алгоритма HITS, полученные после обхода Веб-графа. И сравнивались с отранжированными по HITS результатами, которые получились в результате обхода того же графа стандартным Веб-краулером.

Под стандартным Веб-краулером здесь подразумевается реализация краулера с классическим сценарием поведения: нахождение большого числа страниц (в данной реализации -- с помощью рекурсивного обхода всех встречающихся Веб-страниц) и затем ранжирование с помощью HITS полученных результатов в конце (в отличие от сфокусированного, где это выполняется на каждой итерации).

Эффективность работы Веб-краулеров проверялась в локальном Веб-пространстве (в рамках одного домена, в качестве которого был выбран сайт Википедия -- http://ru.wikipedia.org/) и региональном (по русскоязычному Веб-пространству). Для эксперимента из запросов РОМИП-2007 были выбраны два запроса:

\begin{enumerate}
	\item «Нобелевская премия». Описание: Соответствующая запросу страница должна содержать подробную информацию о Нобелевской премии. Полезной является информация об истории создания премии, создателе, научных областях, по которым присуждается премия, ее лауреатах. Упоминание о присуждении Нобелевской премии конкретному человеку не является исчерпывающим ответом на запрос.
	\item «Квантовый компьютер». Описание: Релевантная страница должна содержать подробную информацию о квантовых компьютерах: историю создания, принципы работы, перспективы развития и т. п. Частично релевантной будет считаться страница с упоминанием о каком-либо достижении или событии в развитии квантовых компьютеров.
\end{enumerate}

По этим запросам строится начальное множество Веб-узлов, с которого начали свою работу Веб-краулеры. Например, начальным множеством узлов для локального Веб-пространстве являлся ответ Википедии на выбранный запрос. Стартовое множество для регионального Веб-пространства строилось следующим образом: посылался выбранный запрос на поисковые системы Яндекс и Google, и выбирались первые десять результатов каждой системы. В итоге получилось четыре множества Веб-страниц (по два множества на каждый выбранный запрос), с которых стартуют сфокусированный и стандартный краулеры.

Результатом работы двух Веб-краулеров на выбранных стартовых множествах является набор Веб-страниц. По этим результатам строится тестовая коллекция. Производится экспертная оценка элементов построенной коллекции, т. е. из описаний запросов, полученных из РОМИПа, выполняется бинарная классификация каждого элемента тестовой коллекции «релевантен/не релевантен» по отношению к выбранному запросу.

Для оценки эффективности разработанного тематического Веб-краулера на основе метода алгоритма HITS в данной статье использовались наиболее популярные критерии: полнота; точность (precision); 11-точечный график полноты/точности, измеренный по методике TREC (11-point matrix (TREC)) \cite{SmartEval,TREC2003,Zobel}. Если \(а\) -- количество документов, найденных системой и релевантных с точки зрения экспертов, \(b\) -- количество документов, найденных системой, \(c\) -- количество релевантных документов, не найденных системой, но нерелевантных с точки зрения экспертов, то метрики вычисляются следующим образом:

\begin{itemize}
	\item \textit{полнота} (\textit{recall}) -- это отношение найденных релевантных документов к общему количеству релевантных документов \(r = \frac{a}{a + c}\).
	\item точность (precision) – это отношение най- денных релевантных документов к общему количеству найденных документов \(p = \frac{a}{a + b}\)
\end{itemize}

В дополнение к этим метрикам приближенно оценивалась вычислительная производительность каждого Веб-краулера.

\paragraph{Итоги эксперимента.} По запросу «Нобелевская премия» в региональном Веб-пространстве тематический краулер на основе алгоритма HITS нашел множество, состоящее из 117 авторитетных Веб-страниц. Первые 10 качественных авторитетных страниц:

\begin{itemize}
	\item \textit{http://wikimediafoundation.org/}
	\item \textit{http://de.wikipedia.org/wiki/Nobelpreis}
	\item \textit{http://ja.wikipedia.org/wiki/\%E3\%83\%8E\linebreak\%E3\%83\%BC\%E3\%83\%99\%E3\%83\%AB\%E8\%B3\%9E}
	\item \textit{http://fr.wikipedia.org/wiki/Prix\_Nobel}
	\item \textit{http://ur.wikipedia.org/wiki/\%D9\%86\%D9\%88\%D8\%A8\linebreak\%D9\%84\_\%D8\%A7\%D9\%86\%D8\%B9\%D8\%A7\%D9\%85}
	\item \textit{http://mg.wikipedia.org/wiki/Loka\_Nobel}
	\item \textit{http://fi.wikipedia.org/wiki/Nobel-palkinto} 
	\item \textit{http://no.wikipedia.org/wiki/Nobelprisen}
	\item \textit{http://nl.wikipedia.org/wiki/Nobelprijs}
	\item \textit{http://nds.wikipedia.org/wiki/Nobelpries}
\end{itemize}

По этому же запросу стандартный Веб-краулер в том же пространстве построил множество из 150 000 страниц. В результате ранжирования по HITS этих Веб-страниц получилось, как и в случае сфокусированного краулера, 117 страниц, первые 10 из которых:

\begin{itemize}
	\item \textit{http://www.mediawiki.org/}
	\item \textit{http://wikimediafoundation.org/}
	\item \textit{http://ru.wikipedia.org/wiki/\%D0\%9D\%D0\%BE\%D0\%B1\linebreak\%D0\%B5\%D0\%BB\%D0\%B5\%D0\%B2\%D1\%81\%D0\%BA\%D0\%B0\linebreak\%D1\%8F\_\%D0\%BF\%D1\%80\%D0\%B5\%D0\%BC\%D0\%B8\%D1\%8F}
	\item \textit{http://de.wikipedia.org/wiki/Nobelpreis}
	\item \textit{http://ja.wikipedia.org/wiki/\%E3\%83\linebreak\%8E\%E3\%83\%BC\%E3\%83\%99\%E3\%83\%AB\%E8\%B3\%9E}
	\item \textit{http://fr.wikipedia.org/wiki/Prix\_Nobel}
	\item \textit{http://ur.wikipedia.org/wiki/\%D9\%86\%D9\linebreak\%88\%D8\%A8\%D9\%84\_\%D8\%A7\%D9\%86\%D8\%B9\%D8\%A7\%D9\%85}
	\item \textit{http://mg.wikipedia.org/wiki/Loka\_Nobel}
	\item \textit{http://fi.wikipedia.org/wiki/Nobel-palkinto} 
	\item \textit{http://no.wikipedia.org/wiki/Nobelprisen}
\end{itemize}

В результате работы двух краулеров получили тестовую коллекцию из 118 Веб-страниц. На
этой коллекции для тематического Веб-краулера метрика точности \(p'_{11} \approx 0,93\), для отранжированных результатов стандартного Веб-краулера  \(p'_{12} \approx 0,93\). График зависимости полноты от точности показан на рис.~\cref{fig:nobelPrizeRegionalPR}.

\begin{figure}[ht]
	\centerfloat{
		\includegraphics[scale=0.25]{nobelPrizeRegionalPR}
	}
	\caption{11-точечный график полноты/точности для запроса «Нобелевская премия» в региональном Веб-пространстве (тематический и стандартный Веб-краулеры).}\label{fig:nobelPrizeRegionalPR}
\end{figure}

Для Википедии (Веб-пространства в рамках одного домена) по тому же запросу «Нобелевская премия» сфокусированный краулер нашел 95 Веб-страниц с метрикой точности \(p'_{21} \approx 0,9\), среди которых 10 наилучших авторитетных:

\begin{itemize}
	\item \textit{http://www.mediawiki.org/}
	\item \textit{http://wikimediafoundation.org/}
	\item \textit{http://de.wikipedia.org/wiki/Nobelpreis}
	\item \textit{http://nl.wikipedia.org/wiki/Nobelprijs}
	\item \textit{https://it.wikipedia.org/wiki/Premio\_Nobel}
	\item \textit{http://fr.wikipedia.org/wiki/Prix\_Nobel}
	\item \textit{http://mg.wikipedia.org/wiki/Loka\_Nobel}
	\item \textit{http://fi.wikipedia.org/wiki/Nobel-palkinto} 
	\item \textit{http://nl.wikipedia.org/wiki/Nobelprijs}
	\item \textit{http://no.wikipedia.org/wiki/Nobelprisen}
\end{itemize}

Стандартный краулер после ранжирования выдал 100 Веб-страниц и точность \( p'_{22} \approx 0,02\). Первые 10 авторитетных Веб-страниц с наивысшими весами:

\begin{itemize}
	\item \textit{http://www.mediawiki.org/}
	\item \textit{http://wikimediafoundation.org/}
	\item \textit{http://km.wikipedia.org/wiki/\%E1\%9E\%9C\linebreak\%E1\%9E\%B7\%E1\%9E\%82\%E1\%9E\%B8\%E1\linebreak\%9E\%97\%E1\%9E\%B8\%E1\%9E\%8C\%E1\%9E\%B6}
	\item \textit{http://eu.wikipedia.org/wiki/Wikipedia}
	\item \textit{http://ur.wikipedia.org/wiki/\%D9\%88\%DB\linebreak\%8C\%DA\%A9\%DB\%8C\%D9\%BE\%DB\%8C\%DA\%88\%DB\%8C\%D8\%A7}
	\item \textit{http://wa.wikipedia.org/wiki/Wikipedia} 
	\item \textit{http://gn.wikipedia.org/wiki/Vikipet\%C3\%A3} 
	\item \textit{http://nah.wikipedia.org/wiki/Huiquipedia}
	\item \textit{http://ay.wikipedia.org/wiki/Wikipidiya}
	\item \textit{http://bar.wikipedia.org/wiki/Wikipedia}
\end{itemize}

11-точечный график полноты/точности показан на рис.~\cref{fig:nobelPrizeLocalPR}.

\begin{figure}[ht]
	\centerfloat{
		\includegraphics[scale=0.25]{nobelPrizeLocalPR}
	}
	\caption{11-точечный график полноты/точности для запроса «Нобелевская премия» в локальном Веб-пространстве (тематический и стандартный Веб-краулеры).}\label{fig:nobelPrizeLocalPR}
\end{figure}

По запросу «Квантовые компьютеры» в региональном Веб-пространстве тематический краулер на основе алгоритма Клеинберга получил 45 авторитетных Веб-страниц и коэффициент точности \(p''_{11} \approx 0,73\). Первые 10 наилучших:

\begin{itemize}
	\item \textit{http://id.wikipedia.org/wiki/Komputer\_kuantum}
	\item \textit{http://ru.wikipedia.org/wiki/\%D0\%9A\%D0\%B2\%D0\%B0\%D0\%BD\linebreak\%D1\%82\%D0\%BE\%D0\%B2\%D1\%8B\%D0\%B9\_\%D0\%BA\%D0\%BE\linebreak\%D0\%BC\%D0\%BF\%D1\%8C\%D1\%8E\%D1\%82\%D0\%B5\%D1\%80}
	\item \textit{http://www.mediawiki.org/} 
	\item \textit{http://wikimediafoundation.org/}
	\item \textit{http://simple.wikipedia.org/wiki/Quantum\_computer}
	\item http://de.wikipedia.org/wiki/Quantencomputer 
	\item \textit{http://cs.wikipedia.org/wiki/Kvantov\linebreak\%C3\%BD\_po\%C4\%8D\%C3\%ADta\%C4\%8D}
	\item \textit{http://ja.wikipedia.org/wiki/\%E9\%87\%8F\%E5\%AD\%90\%E3\%82\linebreak\%B3\%E3\%83\%B3\%E3\%83\%94\%E3\%83\%A5\%E3\%83\%BC\%E3\%82\%BF}
	\item \textit{http://no.wikipedia.org/wiki/Kvantedatamaskin}
	\item \textit{http://hu.wikipedia.org/wiki/Kvantumsz\linebreak\%C3\%A1m\%C3\%ADt\%C3\%B3g\%C3\%A9p}
\end{itemize}

Стандартный Веб-краулер по результатам ранжирования получил 48 Веб-страниц и коэффициент точности \(p''_{12} \approx 0,71\). Первые 10 наилучших авторитетных страниц:

\begin{itemize}
	\item \textit{http://www.mediawiki.org/} 
	\item \textit{http://wikimediafoundation.org/}
	\item \textit{http://id.wikipedia.org/wiki/Komputer\_kuantum}
	\item \textit{http://es.wikipedia.org/wiki/Computaci\%C3\%B3n\_cu\%C3\%A1ntica}
	\item \textit{http://no.wikipedia.org/wiki/Kvantedatamaskin}
	\item \textit{http://cs.wikipedia.org/wiki/Kvantov\linebreak\%C3\%BD\_po\%C4\%8D\%C3\%ADta\%C4\%8D}
	\item \textit{http://ko.wikipedia.org/wiki/\%EC\%96\%91\linebreak\%EC\%9E\%90\_\%EC\%BB\%B4\%ED\%93\%A8\%ED\%84\%B0}
	\item \textit{http://it.wikipedia.org/wiki/Computer\_quantistico}
	\item \textit{http://nl.wikipedia.org/wiki/Kwantumcomputer}
	\item \textit{http://en.wikipedia.org/wiki/Quantum\_computer}
\end{itemize}

Зависимость полноты от точности принимает следующие значения, показанные на рис.~\cref{fig:quantumComputerRegoinalPR}.

\begin{figure}[ht]
	\centerfloat{
		\includegraphics[scale=0.25]{quantumComputerRegionalPR}
	}
	\caption{11-точечный график полноты/точности для запроса «Квантовые компьютеры» в региональном Веб-пространстве (тематический и стандартный Веб-краулеры).}\label{fig:quantumComputerRegoinalPR}
\end{figure}

В локальном Веб-пространстве, на примере Википедии, по запросу «Квантовые компьютеры» тематический Веб-краулер нашел 41 Веб-страницу с коэффициентом точности \(p''_{12} \approx 0,71\). Первые 10 наиболее качественных:

\begin{itemize}
	\item \textit{http://id.wikipedia.org/wiki/Komputer\_kuantum}
	\item \textit{http://www.mediawiki.org/} 
	\item \textit{http://wikimediafoundation.org/}
	\item \textit{http://tph.tuwien.ac.at/~oemer/qcl.html}
	\item \textit{http://www.qubit.org}
	\item \textit{http://www.nature.com/nature/journal/vaop/ncurrent/pdf/nature08121.pdf} 
	\item \textit{http://www.wikimatrix.org/ http://opa.yale.edu/news/article.aspx?id=6764}
	\item \textit{http://www.newscientist.com/article/dn17736-codebreaking-\linebreak quantum-algorithm-run-on-a-silicon-chip.html}
	\item \textit{http://jquantum.sourceforge.net/jQuantumApplet.html}
\end{itemize}

Стандартный Веб-краулер по тому же запросу после ранжирования выдал 73 Веб-страницы с коэффициентом точности \(p''_{22} \approx 0,0\). Первые 10 наилучших авторитетных страниц:

\begin{itemize}
	\item \textit{http://sv.wikipedia.org/wiki/} 
	\item \textit{http://pt.wikipedia.org/wiki/}
	\item \textit{http://it.wikipedia.org/wiki/}
	\item \textit{http://www.wikipedia.org}
	\item \textit{http://www.wikipedia.org/}
	\item \textit{http://wikimediafoundation.org/wiki/Donate/Now/en?utm\_source=\linebreak donate\&utm\_medium=sidebar\&utm\_campaign=spontaneous\_donation}
	\item \textit{http://creativecommons.org/licenses/by-sa/3.0/}
	\item \textit{http://wikimediafoundation.org/wiki/Terms\_of\_Use} 
	\item \textit{http://bh.wikipedia.org/wiki/\%E0\%A4\%B5\%E0\%A4\%BF\%E0\%A4\%95\%E0\%A4\linebreak\%BF\%E0\%A4\%AA\%E0\%A5\%80\%E0\%A4\%A1\%E0\%A4\%BF\%E0\%A4\%AF\%E0\%A4\%BE}
	\item \textit{http://ps.wikipedia.org/wiki/\%D9\%88\%D9\%8A\%DA\linebreak\%A9\%D9\%8A\%D9\%BE\%DB\%90\%DA\%89\%D9\%8A\%D8\%A7}
\end{itemize}

Зависимость полноты от точности принимает следующие значения, проиллюстрированные на рис.~\cref{fig:quantumComputerLocalPR}.

\begin{figure}[ht]
	\centerfloat{
		\includegraphics[scale=0.25]{quantumComputerLocalPR}
	}
	\caption{11-точечный график полноты/точности для запроса «Квантовые компьютеры» в локальном Веб-пространстве (тематический и стандартный Веб-краулеры).}\label{fig:quantumComputerLocalPR}
\end{figure}

Целью данного исследования была оценка эффективности работы и качества поиска разработанного авторами тематического краулера на основе алгоритма HITS по сравнению со стандартным Веб-краулером с учетом ранжирования его результатов по HITS, а также исследование поведения обоих Веб-краулеров в локальном (на примере Википедии) и глобальном Веб-пространствах.

Тематический краулер на основе алгоритма HITS показал лучшую метрику точности \((p'_{11} \approx 0,03, p'_{21} \approx 0,9, p''_{11} \approx 0,73, p''_{21} \approx 0,71)\) во всех проведенных опытах в сравнении со стандартным Веб-краулером \((p'_{12} \approx 0,93, p'_{22} \approx 0,02, p''_{21} \approx 0,71, p''_{22} \approx 0,0)\). В региональном Веб-пространстве по двум запросам тематический краулер нашел более релевантное множество Веб-страниц, чем отранжированное по HITS множество, выданное стандартным Веб-краулером (рис.~\cref{fig:nobelPrizeRegionalPR},~\cref{fig:quantumComputerRegoinalPR}). В свою очередь, стандартный Веб-краулер показал плохие результаты в Википедии (рис.~\cref{fig:nobelPrizeLocalPR},~\cref{fig:quantumComputerLocalPR}). В результирующее множество попадали только нерелевантные Веб-страницы. В локальном Веб-пространстве тематический краулер зачастую выдавал Веб-страницы, релевантные запросу. Его результирующее множество состояло из тематически близких страниц.

В ходе работы стандартного Веб-краулера наблюдается экспоненциальный рост Веб-страниц на каждой его итерации, уже на четвертой итерации насчитывалось 150–200 тысяч страниц. Обработка таких результатов, безусловно, негативно влияет на производительность. Тематический краулер на основе алгоритма HITS отличился и в этом компоненте, на каждой его итерации рост Веб-страниц стремится к некоторой асимптоте (рис.~\cref{fig:crawlerIterationSpeed}). Результатом этого является увеличение производительности.

\begin{figure}[ht]
	\centerfloat{
		\includegraphics[scale=0.25]{crawlerIterationSpeed}
	}
	\caption{Скорость роста количества Веб-страниц на каждой итерации Веб-краулера (тематический и стандартный Веб-краулеры).}\label{fig:crawlerIterationSpeed}
\end{figure}

В дальнейшем планируется расширить эксперимент, сравнив тематический краулер на основе алгоритма Клеинберга, анализирующий гиперссылочную структуру Веб-пространства, и сфокусированный краулер на основе TF-IDF алгоритма \cite{ManningRaghavanSchutze}, анализирующий содержание каждой Веб-страницы, и объединить TF-IDF и HITS алгоритмы и оценить эффективность поиска Веб-краулера, основанного на таком принципе.

\subsection{Оценка эффективности методов поиска тематических сообществ в Веб-пространстве}\label{subsec:ch1/sec3/sub6}

В течение последнего десятилетия наблюдается экспоненциальный рост числа Веб-документов в информационном Веб-пространстве. Только в открытой (индексированной) части Веб на сегодняшний день насчитывается более 20 млрд документов и более 200 млн Веб-сайтов, не говоря уже о скрытой (неиндексированной) части, в которой эти показатели больше в несколько раз \cite{MostReliableHostingCompanySites}.

Сложность задачи поиска в Веб-пространстве привела к появлению целого класса подходов к поиску, учитывающих различные особенности Веб-пространства, что расширило спектр возможностей поисковых систем. Большинство таких систем имеют систему Веб-краулеров \cite{ArasuChoGM}, необходимых для построения индекса существенной части Веб и отслеживающих появление новых и обновление старых документов в нем. В силу быстрого роста Веб-пространства \cite{Kahle,MostReliableHostingCompanySites} проблема построения алгоритма, по которому Веб-краулер его обходит как первично, так и с целью обновления индекса, является очень сложной и актуальной.

В данной статье предлагается алгоритм обхода Веб-пространства, цель которого -- посещение в первую очередь наиболее полезных документов. Для этого предлагается совместное использование алгоритма HITS \cite{Kleinberg}, который учитывает гиперссылочную структуру документов, и TF-IDF \cite{SinghalKaszkiel} взвешивание, учитывающее информацию о тексте документа. Оценивается также качество поиска данного алгоритма обхода Веб-пространства и сравнивается с существующими алгоритмами \cite{BlekanovBondarenko2,Nekrestyanov}.

\paragraph{Принципы и алгоритм работы системы.} Известно, что естественной моделью представления Веб-пространства является ориентированный \textit{Веб-граф} \cite{BroderKumarMaghoul}, в котором вершины соответствуют Веб-страницам, а дуги -- соединяющим страницы гиперссылкам. Под \textit{обходом Веб-пространства} (исследование Веб-графа) понимают процесс поиска информационных источников и гиперссылок, начиная с некоторого начального подмножества Веб-страниц. За выполнение данной процедуры отвечает \textit{Веб-краулер} (поисковый робот).

Задача поисковых роботов -- обход Веб-графа определенным образом с целью сбора информации или понимания структуры и полезности каких-либо Веб-страниц \cite{ArasuChoGM}. А также передача собранной информации для анализа другим приложениям поисковых систем. Веб-краулеры упрощают поиск информации в Веб-пространстве, повышая его качество и эффективность.

В настоящей статье рассматривается тематический поисковый робот с разными алгоритмами обхода Веб-пространства:
\begin{itemize}
	\item тематический Веб-краулер на основе TF-IDF взвешивания \cite{Nekrestyanov};
	\item на основе алгоритма HITS \cite{BlekanovBondarenko2};
	\item на основе совместного использования алго- ритмов HITS и TF-IDF.
\end{itemize}

Одна из главных задач тематического Веб-краулера -- поиск и добавление в коллекцию документов, в первую очередь, наиболее значимых информационных источников, что обеспечивает коллекцию высокого качества \cite{ArasuChoGM}. Результатами обхода Веб-пространства вышеперечисленными алгоритмами являются тематические сообщества \cite{GibsonKleinbergRaghavan} значимых Веб-страниц.

В данном исследовании значимые источники информации в Веб-пространстве определяются анализом гиперссылочной структуры найденных Веб-краулером документов, используя \textit{алгоритм Клеинберга HITS} \cite{Kleinberg}, и текста документа, используя \textit{алгоритм взвешивания TF-IDF} \cite{SinghalKaszkiel} с учетом взаимного положения слов \cite{Gubin}. Наиболее значимыми по HITS Веб-страницами в рамках заданной темы принято считать \textit{авторитетные страницы}, на которые ссылаются другие информационные источники, относящиеся к данной теме. Данное свойство позволяет выявить \textit{индексные страницы (hubs)}. Другими словами, для каждого источника информации \(p\) ставится в соответствие пара неотрицательных весов (\(x^{<p>}, y^{<p>}\)), где \(x^{<p>}, y^{<p>}\) -- веса для авторитетной и для индексной страниц соответственно. В работе Клеинберга \cite{Kleinberg} значение весов как авторитетных, так и индексных источников информации, вычисляется с помощью заданных весов \(\{x^{<p>}\}, \{y^{<p>}\}\) и итеративного применения операций
\begin{equation}
	\label{eqn:1}
	x^{<p>} \leftarrow \sum_{\forall q: q \rightarrow p} y^{<q>}
\end{equation}

\begin{equation}
	\label{eqn:2}
	y^{<p>} \leftarrow \sum_{\forall q: p \rightarrow q} x^{<q>}
\end{equation}

Таким образом, на хорошую авторитетную страницу ссылается много хороших индексных страниц, а хорошая индексная страница ссылается на хорошие авторитетные страницы. Целью алгоритма HITS является поиск наиболее качественных авторитетных и наиболее качественных индексных информационных источников.

Алгоритм TF-IDF использует информацию о частоте встречаемости слов запроса в тексте Веб-страниц. Как правило, частота определяется отношением числа вхождения слова (термина) \(t\) в документ \(d\) к общему количеству слов в \(d\). Данная оценка лежит в основе такого популярного метода вычисления оценки меры релевантности как
\begin{equation}
	\label{eqn:3}
	\textit{tf} - \textit{idf}_{t, d} = \textit{tf}_{t, d} \times \textit{idf}_t ,
\end{equation} 
где \(t\) термин документа (слово запроса); \(d\) -- текущий документ; \(\textit{tf}_{t, d}\) -- число вхождения термина \(t\) в документ \(d\); \( \textit{idf}_t = \log{\frac{N}{ \textit{df}_t}}\) (\( \textit{df}_t\) – множество документов, содержащих термин \(t\); \(N\) -- множество всех документов).

Характеристика\(\textit{tf} - \textit{idf}_{t, d}\) при своей простоте обеспечивает хорошее качество поиска. Недостатком такого метода является то, что в нем недооцениваются длинные документы из-за содержания в них большого количества слов. Для решения такой проблемы в данной статье используется другая формула вычисления TF-IDF веса:
\begin{equation}
	\label{eqn:4}
	\textit{TF} - \textit{IDF} = \textit{TFfactor} \times \textit{IDFfactor} \times \textit{NormalizationFactor},
\end{equation}
\[
\text{где } \textit{TFfactor} = 1 + \ln{(1 + \ln(\textit{tf}_{t, d})}) (\textit{TFfactor} = 0 \text{ в случае } \textit{tf}_{t, d} = 0)
\]
\begin{multline}
	\textit{IDFfactor} = \log{(\frac{N + 1}{\textit{df}_t})} \text{ и } \textit{NormalizationFactor} = \\
	=  \frac{1}{0,8 + 0,2 \times \frac{\textit{длина документа (в байтах)}}{\textit{средняя длина документа (в байтах)}}}
\end{multline}

Обоснованность выбора данной формулы описана в исследованиях \cite{SinghalKaszkiel}.

Однако использование формулы \cref{eqn:4} приводит к потере информации о порядке следования слов в тексте информационного источника, что нарушает его смысл. Чтобы обойти данную проблему, в работе используется алгоритм отбора устойчивых словосочетаний (пары слов) \cite{Gubin} в тексте документа. Формируется следующий алгоритм обработки запросов:
\begin{enumerate}
	\item Из запроса выделяется множество всевозможных пар слов (каждая пара сочетает два слова из слов запроса).
	\item Из сформированного множества пар с по- мощью алгоритма, описанного в исследовании \cite{Gubin}, отбираются «настоящие» словосочетания, которые добавляются к множеству слов запроса в виде отдельных терминов.
	\item Осуществляется поиск и взвешивание документов по формуле \cref{eqn:4}.
	\item Результаты поиска ранжируются по убыванию веса документа.
\end{enumerate}

Данный алгоритм обработки запросов, как показали исследования \cite{Gubin}, увеличивает качество информационного поиска.

В статье проверяется качество поиска информации нового алгоритма обхода и выбора дальнейшего пути сканирования Веб-пространства, основанного на совместном использовании алгоритмов HITS и обработки запросов (с учетов их взвешивания по формуле \cref{eqn:4}). То есть на каждом шаге получения новых источников информации тематическим Веб-краулером применялся алгоритм взешивания TF-IDF \cref{eqn:4}, который на множестве всех найденных документов выделяет подмножество Веб-страниц с ненулевыми TF-IDF весами. Далее, для данного подмножества информационных источников применялся алгоритм Клеинберга HITS. Из полученного набора Веб-страниц качественные авторитетные страницы записывались в индекс, а по качественным индексным продолжался поиск новых авторитетных. В результате об- хода Веб-пространства по описанному алгоритму тематический Веб-краулер выдает тематическое сообщество \cite{GibsonKleinbergRaghavan} авторитетных страниц.

\paragraph{Эксперимент.} В эксперименте решалась классическая задача информационного поиска. Оценивались результаты поиска информации созданного авторами тематического Веб-краулера на основе совместного использования алгоритма HITS и алгоритма взвешивания текста документов TF-IDF, полученные после обхода Веб-графа. Производилось сравнение с результатами поиска, которые получились в результате обхода того же графа уже существующими моделями тематических Веб-краулеров, построенных с использованием алгоритмов HITS и TF-IDF по отдельности \cite{BlekanovBondarenko2,Nekrestyanov}.

Эффективность поиска информации трех реализованных моделей поисковых роботов проверялась в русскоязычной и англоязычной частях Веб-пространства. Для эксперимента были выбраны по пять запросов из тестовых коллекций РОМИП и TREC (табл.~\cref{tab:testCollectionsROMIPTREC}) \cite{ROMIP,TREC}. По данным запросам строилось начальное множество гиперссылок в модуле очереди выбранной модели Веб-краулера. Так, например, начальное множество гиперссылок для русскоязычной части Веб-графа строилось следующим образом: каждый запрос из российской тестовой коллекции РОМИП в табл.~\cref{tab:testCollectionsROMIPTREC} посылался информационно-поисковой системе Google, и выбиралось 10 первых ее результатов. Аналогичным образом строилось стартовое множество гиперссылок и для англоязычной части Веб-пространства на тестовой коллекции TREC. Таким образом, были образованы 10 начальных множеств, с которых запускается каждая из трех созданных моделей тематических Веб-краулеров. Поиск информации каждого Веб-краулера останавливается по истечению 5 итераций (число итераций выбиралось исходя из экономии времени и имеющихся ресурсов).

\begin{table} [htbp]%
	\centering
	\caption{Запросы из тестовых коллекций РОМИП и TREC.}%
	\label{tab:testCollectionsROMIPTREC}% label всегда желательно идти после caption
	\renewcommand{\arraystretch}{1.5}%% Увеличение расстояния между рядами, для улучшения восприятия.
	\begin{SingleSpace}
		\begin{tabulary}{\textwidth}{@{}>{\zz}L >{\zz}C >{\zz}C@{}} %Вертикальные полосы не используются принципиально, как и лишние горизонтальные (допускается по ГОСТ 2.105 пункт 4.4.5) % @{} позволяет прижиматься к краям
			\toprule     %%% верхняя линейка
			№ запроса & Тестовая коллекция РОМИП & Тестовая коллекция TREC \\
			\midrule %%% тонкий разделитель. Отделяет названия столбцов. Обязателен по ГОСТ 2.105 пункт 4.4.5
			1 & Армия России & Art museums \\				
			2 & Гимн Франции & Solar flares \\
			3 & История рентгенологии & International trade \\
			4 & Кельтская музыка & Planet Mars \\
			5 & Нейтронная бомба & Space exploration  \\
			\bottomrule %%% нижняя линейка
		\end{tabulary}%
	\end{SingleSpace}
\end{table}

Результатом работы трех тематических поисковых роботов на выбранных стартовых множествах являются наборы Веб-страниц, образующих тематические сообщества. По данным результатам составлялась тестовая коллекция. Производилась экспертная оценка элементов построенной коллекции, т. е. из описаний запросов, полученных из РОМИПа и TREC, независимыми экспертами (группа студентов СПбГУ факультета ПМ-ПУ) выполнялась бинарная классификация каждого элемента тестовой коллекции «релевантен/не релевантен» по отношению к выбранному запросу.

Для оценки эффективности разработанного тематического Веб-краулера на основе совместного использования алгоритма HITS и взвешивания TF-IDF в данном эксперименте использовались наиболее популярные метрики \cite{ManningRaghavanSchutze,ROMIP,TREC}: полнота; точность; R-точность; точность на уровне 5 и 10 документов; средняя точность системы; 11-точечный график полноты/точности, измеренный по методике TREC.

\paragraph{Результаты эксперимента.} В каждой модели Веб-краулера выбиралось пятьдесят первых найденных документов по каждому запросу из РОМИПа и TREC. По полученным результатам были составленны тестовые коллекции (табл.~\cref{tab:collectionsROMIPTREC}).

\begin{table} [htbp]%
	\centering
	\caption{Составленные коллекции по запросам из РОМИП и TREC.}%
	\label{tab:collectionsROMIPTREC}% label всегда желательно идти после caption
	\renewcommand{\arraystretch}{1.5}%% Увеличение расстояния между рядами, для улучшения восприятия.
	\begin{SingleSpace}
		\begin{tabulary}{\textwidth}{@{}>{\zz}L >{\zz}C >{\zz}C@{}} %Вертикальные полосы не используются принципиально, как и лишние горизонтальные (допускается по ГОСТ 2.105 пункт 4.4.5) % @{} позволяет прижиматься к краям
			\toprule     %%% верхняя линейка
			Запрос & Общее количество документов в коллекции & Количество релевантных документов в коллекции \\
			\midrule %%% тонкий разделитель. Отделяет названия столбцов. Обязателен по ГОСТ 2.105 пункт 4.4.5
			Армия России & 85 & 10 \\				
			Гимн Франции & 93 & 20  \\
			История рентгенологии &  77 & 14\\
			Кельтская музыка &  86 & 17 \\
			Нейтронная бомба &  102 & 20 \\
			Art museums & 97 & 19 \\
			Solar flares & 95 & 18 \\
			International trade & 75 & 16 \\
			Planet Mars  & 109 & 25 \\
			Space exploration & 98 & 18 \\
			\bottomrule %%% нижняя линейка
		\end{tabulary}%
	\end{SingleSpace}
\end{table}

На построенных тестовых коллекциях сначала проверялся тематический Веб-краулер на основе TF-IDF взвешивания. Для него эффективность поиска тестировалась в два этапа: определялось качество поиска для русских и английских тестовых коллекций. По данной модели поискового робота наилучшие результаты в русскоязычных коллекциях были получены по запросу «Нейтронная бомба», средняя точность которого равна 0,45, R-точность -- 0,5, точность на уровне пяти первых документов -- 0,8, точность на уровне десяти первых документов -- 0,9, точность на уровне всех найденных документов за пять итераций Веб-краулера -- 0,22 (табл.~\cref{tab:metricsROMIPTREC}).

\begin{table}[ht]%
	\centering
	\caption{Значения метрик по запросам из коллекций TREC и РОМИП для методов поиска тематических сообществ в Веб-пространстве.}%
	\label{tab:metricsROMIPTREC}% label всегда желательно идти после caption
	\begin{adjustbox}{width=0.9\textwidth}
		\small
		\begin{tabular}{| c | c | c | c | c | c |}% Вертикальные полосы не используются принципиально, как и лишние горизонтальные (допускается по ГОСТ 2.105 пункт 4.4.5) % @{} позволяет прижиматься к краям
			\hline
			\multicolumn{6}{|c|}{\makecell{Тематический Веб-краулер на основе \\ TF-IDF взвешивания}}\\
			\hline
			Метрики & Армия России & Гимн Франции & История рентгенологии &  Кельтская музыка &  Нейтронная бомба \\
			\hline
			Кол. рел. док. & 3 & 6 & 4 & 2 & 11  \\
			\hline
			Recall & 0,3 & 0,3 & 0,29 & 0,12 & 0,55 \\
			\hline
			AvgPrec & 0,06 & 0,21 & 0,12 & 0,04 & 0,45  \\
			\hline
			R-precision & 0,2 & 0,3 & 0,29 & 0,12 & 0,5  \\
			\hline
			precision(5) & 0 & 0,6 & 0,2 & 0,4 & 0,8  \\
			\hline
			precision(10) & 0,2 & 0,6 & 0,4 & 0,2 & 0,9  \\
			\hline
			precision(50) & 0,06 & 0,12 & 0,08 & 0,04 & 0,22  \\
			\hline
			& Art museums & Solar flares & International trade &  Planet Mars &  Space exploration \\
			\hline
			Кол. рел. док. & 5 & 10 & 7 & 14 & 6  \\
			\hline
			Recall & 0,26 & 0,56 & 0,44 & 0,56 & 0,33 \\
			\hline
			AvgPrec & 0,1 & 0,3 & 0,24 & 0,43 & 0,16  \\
			\hline
			R-precision & 0,16 & 0,56 & 0,44 & 0,52 & 0,28  \\
			\hline
			precision(5) & 0,6 & 0,4 & 0,4 & 0,8 & 0,2  \\
			\hline
			precision(10) & 0,3 & 0,6 & 0,5 & 0,7 & 0,4  \\
			\hline
			precision(50) & 0,1 & 0,2 & 0,14 & 0,28 & 0,12  \\
			\hline
			\multicolumn{6}{|c|}{\makecell{Тематический Веб-краулер на основе \\ алгоритма HITS}}\\
			\hline
			Метрики & Армия России & Гимн Франции & История рентгенологии &  Кельтская музыка &  Нейтронная бомба \\
			\hline
			Кол. рел. док. & 5 & 13 & 6 & 11 & 15  \\
			\hline
			Recall & 0,5 & 0,65 & 0,43 & 0,65 & 0,75 \\
			\hline
			AvgPrec & 0,24 & 0,4 & 0,17 & 0,42 & 0,45  \\
			\hline
			R-precision & 0,3 & 0,5 & 0,29 & 0,53 & 0,55  \\
			\hline
			precision(5) & 0,4 & 0,6 & 0,2 & 0,6 & 0,6  \\
			\hline
			precision(10) & 0,3 & 0,6 & 0,4 & 0,8 & 0,7  \\
			\hline
			precision(50) & 0,1 & 0,26 & 0,12 & 0,22 & 0,3  \\
			\hline
			& Art museums & Solar flares & International trade &  Planet Mars &  Space exploration \\
			\hline
			Кол. рел. док. & 13 & 13 & 10 & 23 & 12  \\
			\hline
			Recall & 0,68 & 0,72 & 0,63 & 0,92 & 0,67 \\
			\hline
			AvgPrec & 0,26 & 0,46 & 0,22 & 0,49 & 0,26  \\
			\hline
			R-precision & 0,42 & 0,56 & 0,13 & 0,48 & 0,39  \\
			\hline
			precision(5) & 0,0 & 0,8 & 0,4 & 0,4 & 0,2  \\
			\hline
			precision(10) & 0,3 & 0,6 & 0,2 & 0,4 & 0,3  \\
			\hline
			precision(50) & 0,26 & 0,26 & 0,2 & 0,46 & 0,24  \\
			\hline
			\multicolumn{6}{|c|}{\makecell{Тематический Веб-краулер на основе \\ алгоритмов HITS и TF-IDF}}\\
			\hline
			Метрики & Армия России & Гимн Франции & История рентгенологии &  Кельтская музыка &  Нейтронная бомба \\
			\hline
			Кол. рел. док. & 10 & 20 & 14 & 17 & 20  \\
			\hline
			Recall & 1,0 & 1,0 & 1,0 & 1,0 & 1,0 \\
			\hline
			AvgPrec & 0,59 & 0,66 & 0,57 & 0,73 & 0,73  \\
			\hline
			R-precision & 0,5 & 0,6 & 0,5 & 0,59 & 0,6  \\
			\hline
			precision(5) & 0,6 & 0,8 & 0,6 & 1,0 & 0,8  \\
			\hline
			precision(10) & 0,5 & 0,7 & 0,6 & 0,8 & 0,8  \\
			\hline
			precision(50) & 0,2 & 0,4 & 0,28 & 0,34 & 0,4  \\
			\hline
			& Art museums & Solar flares & International trade &  Planet Mars &  Space exploration \\
			\hline
			Кол. рел. док. & 19 & 18 & 16 & 25 & 18  \\
			\hline
			Recall & 1,0 & 1,0 & 1,0 & 1,0 & 1,0 \\
			\hline
			AvgPrec & 0,66 & 0,67 & 0,65 & 0,65 & 0,57  \\
			\hline
			R-precision & 0,63 & 0,61 & 0,56 & 0,6 & 0,5  \\
			\hline
			precision(5) & 0,6 & 0,8 & 0,6 & 1,0 & 0,8  \\
			\hline
			precision(10) & 0,7 & 0,7 & 0,6 & 0,6 & 0,5  \\
			\hline
			precision(50) & 0,38 & 0,36 & 0,32 & 0,5 & 0,36  \\
			\hline
		\end{tabular}%
	\end{adjustbox}
\end{table}

В свою очередь, эта же модель Веб-краулера в коллекциях, составленных по запросам из TREC, показала несколько лучший результат поиска информации по сравнению с русскоязычными коллекциями (табл.~\cref{tab:metricsROMIPTREC}). Так, наилучшие значения метрик были получены по запросу «Planet Mars»: показатель средней точности равен 0,43, R-точности -- 0,52, точность на уровне пяти первых документов -- 0,8, точность на уровне десяти первых документов -- 0,7, точность на уровне всех найденных документов -- 0,28.

При оценке значений метрик видно, что тематический поисковый робот на основе TF-IDF взвешивания выполняет поиск в английской тестовой коллекции лучше, чем в русской. В ходе эксперимента было также выявлено, что в обеих коллекциях данная модель краулера находит релевантные документы на уровне 10–20 документов и имеет хорошее значение точности (например, по запросу «Нейтронная бомба» precision(10) = 0,9). На большем уровне найденных документов качество поиска значительно ухудшается. Например, значение точности для первых пятидесяти документов колеблется от 0,06 до 0,28, а значения полноты для некоторых запросов 0,12.

Вторая система поиска информации в Веб-пространстве, которая оценивалась введенными метриками по составленным тестовым коллекциям, основывалась на алгоритме Клеинберга HITS. Для русскоязычного Веб-пространства данная модель тематического Веб-краулера показала лучшие значения метрик по запросам «Кельтская музыка» и «Нейтронная бомба». Показатели всех метрик по каждому запросу из РОМИП подробно представлены в табл.~\cref{tab:metricsROMIPTREC}.

Для запросов из TREC данный метод поиска информации показал лучшие значения средней точности (равной 0,49) по запросу «Planet Mars». А для запроса «Solar flares» показатели таких метрик как R-точность, точность на уровне пяти первых документов, точность на уровне десяти первых документов, были наивысшими по отношению к другим запросам из англоязычной части Веб-пространства.

Значение полноты и точность модели Веб-краулера на основе алгоритма HITS выше, чем у модели, основанной на TF-IDF взвешивании. Например, в 80\% случаев данная модель на первой позиции своих результатов выдает релевантный документ.

Последними оценивались результаты поиска информации для тематического Веб-краулера, который учитывает как информацию о тексте, найденных документов, так и информацию о гиперссылочной связи между ними. Как и для других моделей поисковых роботов, данная модель проверялась на тестовых коллекциях из запросов РОМИП и TREC. В отличие от предыдущих двух моделей краулеров, данная модель показывала наилучшие показатели по всем метрикам и тестовым коллекциям (табл.~\cref{tab:metricsROMIPTREC}).

Чтобы получить более подробную информацию об изменении точности в зависимости от требований к полноте, чем единую метрику в виде одной цифры, в эксперименте строился 11-точечный график полноты/точности, измеренный по методике TREC для русской и английской тестовых коллекций (рис.~\cref{fig:precisionRecallROMIP},~\cref{fig:precisionRecallTREC}).

\begin{figure}[ht]
	\centerfloat{
		\hfill
		\subcaptionbox[List-of-Figures entry]{11-точечный график полноты/точности по запросам из РОМИП для трех моделей тематических Веб-краулеров.\label{fig:precisionRecallROMIP}}{%
			\includegraphics[width=0.35\linewidth]{precisionRecallROMIP}}
		\hfill
		\subcaptionbox{11-точечный график полноты/точности по запросам из TREC для трех моделей тематических Веб-краулеров.\label{fig:precisionRecallTREC}}{%
			\includegraphics[width=0.35\linewidth]{precisionRecallTREC}}
		\hfill
	}
	\legend{($\ast$) Веб-краулер на основе TF-IDF; ($\blacklozenge$) Веб-краулер на основе HITS; ($\blacksquare$) Веб-краулер на основе алгоритма TF-IDF и HITS взвешивания.}
	\caption[Этот текст попадает в названия рисунков в списке рисунков]{}\label{fig:precisionRecallROMIPTREC}
\end{figure}

Графики показывают, что качество поиска информации в Веб-пространстве для поискового робота, основанного на алгоритме HITS и взвешивании TF-IDF, лучше для обоих видов коллекций -- график данной модели Веб-краулера находится выше графиков остальных моделей. Для коллекций, составленных из запросов РОМИП и TREC, наблюдается некоторое уменьшение точности, вызванное, скорее всего, флуктуацией. Качество поиска Веб-краулера, основанного на алгоритме Клеинберга, оказалось эффективней, чем поисковый робот, учитывающий только информацию о тексте документа. По всем моделям Веб-краулеров наблюдается наибольший прирост качества поиска при малых значениях полноты. Другими словами, увеличение количества релевантных документов наблюдается в начале выдачи результатов системы, что является полезным качеством с точки зрения пользователя.

Цель данного эксперимента -- оценка качества поиска тематического Веб-краулера на основе со- вместного использования алгоритмов HITS и TF-IDF, сравнив его с моделями Веб-краулеров, построенными с использованием алгоритмов HITS и TF-IDF, по отдельности.

Из поставленного эксперимента можно сделать следующие выводы.

Разработанная модель тематического поискового робота показала наилучшее качество поиска относительно двух других моделей. В рамках поставленного эксперимента значения всех оценок качества информационного поиска для данной модели Веб-краулера были наивысшими.

Тематические Веб-краулеры на основе алгоритма Клеинберга и на основе взвешивания TF-IDF эффективней ищут информацию в тестовых английских коллекциях в то время, как разработанная модель поискового робота обладает при- мерно одинаковым качеством поиска информации и в русских, и в английских коллекциях.

Множество объединений релевантных документов, найденных моделью Веб-краулеров на основе алгоритма HITS и моделью на основе взвешивания TF-IDF, является подмножеством мно-жества релевантных документов, найденных разработанной моделью поискового робота. Данный факт говорит о хорошей полноте созданной системы поиска информации в Веб-пространстве.

Как показал эксперимент, качество поиска информации Веб-краулерами на основе TF-IDF взвешивания самое худшее. Данный факт объясняется тем, что содержание большого количества слов запроса в документе не всегда влияет на его релевантность. К таким документам в тестовых коллекциях относились различные форумы, новостные порталы и рекламные Веб-страницы.

\section{Анализ гиперссылочной структуры ресурсов}\label{sec:ch1/sec4}

\subsection{An Algorithm to Approximate the Total Number of Site Pages Using a Portion of Its Structure}\label{subsec:ch1/sec4/sub1}

\paragraph{Introduction.} One of the main tasks of webometrics is the massive websites inspection to be split into two -- deep webpages analysis and its’ interaction with web-space \cite{Thelwall}. This article generally considers the websites’ different attributes as inherent properties. The website structure is commonly accepted (i.e. \cite{BroderKumarMaghoul}) to be presented as a direct graph therefore it contains the vertices as webpages, the edges as hyperlinks, connectivity, length etc. In the second place -- the ergonomic factors such as website design and usability. Third is the content attributes -- the main topic, tags etc. Considering these factors is the key for the preset website selection. The evaluation of these factors is possible after inspecting, for example, the one tenth of the total number of website hyperlinks (that is comparable to the proper book selection -- in order to understand the book is worth reading you are to look through the part of it). But to understand what part of the website was inspected already we must possess the number of the webpages (graph vertices). Nowadays there is only one known way to determine the true number of the webpages -- the whole web-graph inspection. As long as the massive organizations’ websites (i.e. universities) been estimated of tens or even hundreds of webpages with the zillions of hyperlinks its measurement is widely considered to be the resource-intensive task.

The main idea of the offered approximate webpages and hyperlinks calculation method premised on its part is as follows. At first all the hyperlinks on every webpage are to be retrieved. Then every page followed by every hyperlink is to be inspected for another set of hyperlinks etc. Some links lead to new pages, some of them are cycled in order the new-leading links share to be reduced. The relevant logic is in the works \cite{BlekanovSergeevKlemeshov,BlekanovSergeevMaksimov2016}. Here the authors put forward the algorithm that allows to measure the website size after the inspection of its part. It is based on the new leading hyperlinks appearing slowdown.

\paragraph{Mathematical model.} Let us consider the approximate procedure of a web-graph random walk.

Let us consider the approximate procedure of a web-graph random walk.

\begin{itemize}
	\item Making the sets:
	\begin{enumerate}
		\item An \textit{NLinks} \(\{ a_i \}\). \(a_i\) set is the number of retrieved hyperlinks in \(i\) steps;
		\item An \textit{NPage} \(\{ a_i \}\). \(v_i\) set is the number of webpages that retrieved hyperlinks in \(i\) steps lead to;
		\item An \textit{UrlsList} set is the number of webpages’ domain names that retrieved hyperlinks in \(i\) steps lead to.
	\end{enumerate}
	\item Let us retrieve \(\Delta a_i\) hyperlinks from the website and add \(a_i = a_{i-1} + \Delta a_i\) to \textit{NLinks} set. Every retrieved hyperlink is to be collated with the \textit{UrlsList} set. If there is no webpage that hyperlink leads to then the hyperlink joins the \textit{UrlsList} set. The added webpages number is \(v_i = v_{i-1} + \Delta v_i\). Then been added to the \textit{NPage} set.
\end{itemize}

Let us denote all the website hyperlinks and webpages number by \(a_{total}\) and \(v_{total}\) respectively; the number of retrieved hyperlink by \(a\), and the unique webpages number by \(v\). Let us also intoduce:
\[
a_{remain} = a_{total} - a \text{ and } v_{remain} = v_{total} - v.
\]

Let us denote the unique website pages set by \(V_{total}\) and still unretrieved webpages number by \(V_{remain}\). Let us break the webpages set into subsets by the incoming hyperlinks. Let us
denote the set of the webpages with \(n\) incoming hyperlinks by \(S^0_n\) and the subset of webpages within \(V_{remain}\), with \(n\) incoming hyperlinks by \(S_n (v_{remain})\).

Then \(s^0_n = \lvert S^0_n \rvert, s_n (v_{remain}) = \lvert S_n (v_{remain}) \rvert\).

The probability of the next retrieved hyperlink leads to the webpage from the \(S_n (v_{remain})\) set equals to:
\[
p_n(v_{remain}) = \frac{n S_n (v_{remain})}{a_{remain}} = \frac{n S_n (v_{remain})}{a_{total} - a}
\]
Let any step has \(da\) as the number of hyperlinks retrieved. Then the expected value of the increment \(s_n (v_{remain})\):
\[
ds_n (v_{remain}) = -\frac{n }{a_{total} - a} da
\]
Let us integrate between \(0\) and \(a\). As far as \(v_{remain} = v_{total}\) with \(a = 0\), then:
\[
\ln{s_n (v_{remain})} - \ln{s^0_n} = n(\ln{(a_{total} - a)} - \ln{a_{total}})
\]
Therefore:
\[
s_n(v_{remain}) = s^0_n (\frac{a_{total} - a}{a_{total}})^n
\]
Let us use the trivial par:
\[
\sum_{n} s_n (v_{remain}) = v_{remain} = v_{total} - v
\]
Therefore:
\begin{equation}
	\label{eqn:7}
	v = v_{total} - \sum_{n} s^0_n (\frac{a_{total} - a}{a_{total}})^n
\end{equation}
It is known (i.e. \cite{BarabasiAlbert,KumarRaghavanRajagopalan}) that in large networks nodes are distributed by the incoming hyperlinks number follows the law
\begin{equation}
	\label{eqn:8}
	s^0_n = \frac{\gamma v_{total}}{n^q}
\end{equation} with \(\frac{1}{\gamma} = \sum_{n} \frac{1}{n^q}\), and \(q\) as an invariable.
\begin{equation}
	\label{eqn:9}
	2 < q < 3
\end{equation}

Let us plug the formula \cref{eqn:8} into the equation \cref{eqn:7}. Then the equation that shows the correlation of the websites pages number \(v_{total}\) and the website hyperlinks number \(a_{total}\) is as follows:
\begin{equation}
	\label{eqn:10}
	v = v_{total}(1 - \gamma \sum_{n} \frac{1}{n^q} (\frac{a_{total} - a}{a_{total}})^n)
\end{equation}
Let us perform the \(k\) of consequent steps of hyperlinks and related webpages (the pages these hyperlinks lead to) accidental samplings. Let \(a_i\) hyperlinks and \(v_i\) related webpages were retrieved after the \textit{i}-step. Then the unique equation will correspond to every step. Therefore there is the set of equations that connects the retrieved hyperlinks with the related webpages total numbers \(v_{total}\) and \(a_{total}\) respectively:
\begin{equation}
	\label{eqn:11}
	v_i = v_{total}(1 - \gamma \sum_{n} \frac{1}{n^q}(\frac{a_{total} - a_i}{a_{total}})^n), (i = 1, 2, \ldots , k) 
\end{equation}

\paragraph{The approximate solution.} As far as we need to get the approximate measurement let us assume
\begin{equation}
	\label{eqn:12}
	\frac{1}{n^q} \approx \frac{1}{n(n+1)}
\end{equation}
Then the limits of this distribution were defined by the inequation \cref{eqn:9} (except \(n = 1\)). Therefore the normalizing factor \(\gamma\):
\begin{equation}
	\label{eqn:13}
	\frac{1}{\gamma} = \sum_{n} \frac{1}{n(n+1)} = 1
\end{equation}
Let us plug the formula \cref{eqn:12} into the equation \cref{eqn:11} considering \cref{eqn:13}. Therefore:
\[
v_i = v_{total}(1 - \sum \frac{1}{n(n+1)} (1 - \frac{a_i}{a_{total}})^n)
\]
Let us analyze the sum
\[
\sigma = \sum \frac{1}{n(n+1) x^n}
\]
\[
\sigma = \sum_{1}^{N} \frac{x^n}{n} - \frac{1}{x} \sum_{1}^{N} \frac{x^{n+1}}{n + 1} = \sum_{1}^{N} \frac{x^n}{n} - \frac{1}{x} \sum_{2}^{N + 1}\frac{x^n}{n}
\]
As far as the massive websites \(N\) value reaches several thousand therefore the approximate value:
\[
\sigma = -\ln{(1 - x)} + \frac{1}{x}(\ln{(1 - x)} + 1)
\]
Therefore:
\[
v_i = v_{total} \frac{\frac{a_i}{a_{total}}}{1 - \frac{a_i}{a_{total}}} \ln{\frac{a_{total}}{a_i}}
\]
As far as situation \(a_i \gg a_{total}\) is considered let us assume:
\[
v = v_{total}\frac{a}{a_{total}} (\ln{a_{total}} - \ln{a})
\]
Let us set
\[
\frac{a_{total}}{v_{total}} \equiv x \text{, } \ln{a_{total}} \equiv y
\]
Therefore the equation:
\[
v_i x + a_i y = -c_i, \text{ где } c_i \equiv a_i \ln{a_i}
\]
Let us solve the overspecified set of equations with the LS method. Therefore:
\[
A_{11} x + A_{12} y = C_1
\]
\[
A_{21} x + A_{22} y = C_2 
\]
with
\[
A_{11} = \sum_{1}^{k} v_i^2, A_{12} = A_{21} = -\sum_{1}^{k} a_i v_i, A_{22} = \sum_{1}^{k} a_i^2
\]
\begin{equation}
	\label{eqn:14}
	C_1 = \sum_{1}^{k} v_i c_i, C_2 = \sum_{1}^{k} a_i c_i
\end{equation}
Therefore
\begin{equation}
	\label{eqn:15}
	x = \frac{C_1 A_{22} - C_2 A_{12}}{A_{11} A_{22} - A^2_{12}}, y = \frac{C_2 A_{11} - C_1 A_{12}}{A_{11} A_{22} - A^2_{12}}
\end{equation}
And finally
\begin{equation}
	\label{eqn:16}
	a^* = e^y, v^* = \frac{e^y}{x}
\end{equation}
with \(a^*\) and \(v^*\) are approximate values of \(a_{total}\) and \(v_{total}\).

The formulae \cref{eqn:14,eqn:15,eqn:16} allow to approximately measure the website size. According to the assumptions made, the formula can be used only with low value of \(a_i\).

\paragraph{The experiment.} Authors were to perform the experiment in order to test the offered method capability using 11 universities' sites (table~\cref{tab:universitiesSites}) and determine the share of the website hyperlinks number needed to measure it approximately (the total number of hyperlinks and the total number of webpages).

\begin{table} [htbp]%
	\centering
	\caption{Universities’ sites inspected.}%
	\label{tab:universitiesSites}% label всегда желательно идти после caption
	\renewcommand{\arraystretch}{1.5}%% Увеличение расстояния между рядами, для улучшения восприятия.
	\begin{SingleSpace}
		\begin{tabulary}{\textwidth}{@{}>{\zz}L >{\zz}C >{\zz}C@{}} %Вертикальные полосы не используются принципиально, как и лишние горизонтальные (допускается по ГОСТ 2.105 пункт 4.4.5) % @{} позволяет прижиматься к краям
			\toprule     %%% верхняя линейка
			\# & University & URL \\
			\midrule %%% тонкий разделитель. Отделяет названия столбцов. Обязателен по ГОСТ 2.105 пункт 4.4.5
			1 & Cambridge & www.cam.ac.uk \\
			2 & MIT & www.web.mit.edu \\
			3 & Oxford & www.ox.ac.uk \\
			4 & The University of Turin & www.unito.it \\
			5 & Cornell University & www.cornell.edu \\
			6 & Emory University & www.emory.edu \\
			7 & Berkeley & www.berkeley.edu \\
			8 & Pierre and Marie Curie &  www.upmc.fr \\
			9 & The University of Aizu & www.u-aizu.ac.jp \\
			10 & Syracuse University & www.syr.edu \\
			11 & Penn State University & www.psu.edu \\
			\bottomrule %%% нижняя линейка
		\end{tabulary}%
	\end{SingleSpace}
\end{table}

Using a web-crawler all webpages and internal hyperlinks have been collected for the each university (from table~\cref{tab:universitiesSites}). Actual values of \(v_{total}\) and \(a_{total}\) were found. Since the offered method proposed is based on accidental samplings, a Fisher- Yates shuffle algorithm described in \cite{FisherYates} has been used in order to shuffle the every website hyperlinks set. Then a hundredth part of hyperlinks was to be picked from the shuffled set of hyperlinks, that made up \(v_1\) and \(a_1\). Then in order to get \(v_2\) and \(a_2\)
two hundredth parts of links were to be picked, three hundredth parts for \(v_3\) and \(a_3\) and so on. The obtained values and had been substituted into \cref{eqn:10} to calculate \(a^*\) and \(v^*\). The proportion of the visited hyperlinks was approximately determined as the ratio of \(a_i\) to \(a^*\).

\paragraph{The experiment results.} The table~\cref{tab:actualAndCalculatedWebpages} reports the main experiment results.

\begin{table}[ht]%
	\centering
	\caption{Actual and calculated numbers of webpages and hyperlinks of universities’ sites.}%
	\label{tab:actualAndCalculatedWebpages}% label всегда желательно идти после caption
	\begin{tabular}{| c | c | c | c | c | c |}% Вертикальные полосы не используются принципиально, как и лишние горизонтальные (допускается по ГОСТ 2.105 пункт 4.4.5) % @{} позволяет прижиматься к краям
		\hline
		University & \(a_{total}\) & \(a^*\) &  \(v_{total}\) & \(v^*\) & \(\frac{a_k}{a_{total}}\)  \\
		\hline
		Cambridge & 2258483 &  2993431 & 69661 & 114226 & 0,12 \\
		\hline
		MIT & 590474 & 1363978 & 70751 & 116108 & 0,2 \\
		\hline
		Oxford & 1197127 & 2649532 & 26289 & 60634 & 0,2 \\
		\hline
		The Universitof Turin & 1195984 & 2975332 & 22796 & 32168 & 0,03 \\
		\hline
		Cornell University & 874555 & 205028 & 20908 & 22760 & 0,03 \\
		\hline
		Emory University & 190119 & 124999 & 12497 & 13237 & 0,07 \\
		\hline
		Berkeley & 116240 & 253220 & 10544 & 13207 & 0,03 \\
		\hline
		Pierre and Marie Curie & 548844 & 154885 & 9142 & 6989 & 0,03 \\
		\hline
		The University of Aizu & 116098 & 78238 & 3993 & 4375 & 0,07 \\
		\hline
		Syracuse University & 184773 & 454720 & 4979 & 7576 & 0,04 \\
		\hline
		Penn State University & 32984 & 78880 & 1923 & 3958 & 0,26 \\
		\hline
		\multicolumn{6}{|c|}{\makecell{\(a_{total}, v_{total}\) -- website hyperlinks and webpages number \\ respectively; \(a^*, v^*\) -- approximate values of \(a_{total}\) and \(v_{total}\); \\ \(a_k\) -- the number of hyperlinks used for the calculation}}\\
		\hline
	\end{tabular}%
\end{table}

In table~\cref{tab:actualAndCalculatedWebpages} the values and have been obtained by ratio of \(a_k\) to \(a^*\), which is equal to 0.1 that based on 2 factors. At first all of the formulae have been obtained using the assumption that \(\frac{a_k}{a^*} \ll 1\). The second is if the \(a_i\) value is too low then the representativeness of the sample decreases.

Since actual values of the total hyperlinks number differ from the calculated ones, the table shows the actual share of visited links for each university website, for which theoretical values \(a^*\) and \(v^*\) have been obtained.

\paragraph{Conclusion.} The main results of the work are:
\begin{enumerate}
	\item The set of equations have been obtained that connects the number of web-crawled website hyperlinks with the webpages being found.
	\item There was the experiment performed in order to check the possibility to measure the webpages number premised on its part using the total numbers of the fully inspected webpages and hyperlinks of 11 universities’ websites. The experiment proves the worked-out website size measurement method capability after the fractional inspection. Both overestimated and underestimated assumptions (about website size) have been noticed. The relation between the approximately calculated website size value and actual webpages total number is 2.4, and 4.3 -- for the hyperlinks if worst comes to worst. The results of the experiment show that there were needed to inspect 3-26\% of the total hyperlinks number to approximately measure every university website size. According to the universities’ websites information it is apparent that the visited websites’ pages differ by 37 times and the visited hyperlinks by 68 times. Therefore the webpages and hyperlinks numbers measurement errors in 2.4 and 4.3 times respectively are considered to be acceptable. The \cref{eqn:11} set of equation solution method and the measurement may be improved.
	\item The algorithm that implements the offered method was specified. It allows to reduce the costs of approximate webpages and hyperlinks number as well as web-graph hyperlinks measurements significantly.
\end{enumerate}

\subsection{Исследование структурных характеристик крупных веб-сегментов}\label{subsec:ch1/sec4/sub2}

Исследование свойств больших сайтов, формулирование убедительных критериев оценки их качества, управление их качеством являются главными задачами вебометрики. Наиболее распространенными критериями качества сайтов служат:
\begin{itemize}
	\item рейтинг веб-ресурса в поисковых системах;
	\item позиция веб-ресурса в вебометрических рейтингах \cite{RankingWeb} (на данный момент рейтинг составлен для сайтов университетов, научно-исследовательских центров, больниц и др.);
	\item индекс цитирования (авторитетность) веб-ресурса \cite{ThelwallZuccala,Smith,Nicolaisen,OrtegaAguilloCothey};
	\item удобство пользования информацией веб-ресурса с точки зрения пользователей
	\cite{ChevalierDommesMartins,HarperChen,ZengProctorSalvendy,HuntingtonNicholasJamali}.
\end{itemize}

Общепринято представление сайта в виде ориентированного графа, узлы которого -- документы, а дуги -- гиперссылки \cite{BroderKumarMaghoul}. Естественно исследовать связь между структурой веб-графа и его качеством (рейтингом).

Одной из важных характеристик сайтов признается их связность \cite{Thelwall,ThelwallWilkinsonMusgrove}. При этом под «связностью» понимается соотношение между общим числом ссылок и количеством неделимых документов. Неделимым документом, или просто документом, будем называть такой элементарный подсайт, на часть которого невозможно сослаться. Если на документ ссылаются из нескольких различных мест сайта, то все такие ссылки выглядят одинаково -- это поисковый адрес документа, который называют уникальной ссылкой. Поскольку с точки зрения поисковой системы уникальная ссылка эквивалентна документу, будем говорить о связности как о соотношении общего числа ссылок и количества уникальных ссылок.

Полное исследование связности большого сайта (иногда десятки миллионов документов и сотни миллионов ссылок) связано с большими затратами машинных ресурсов (машинное время, трафик). В данной работе изучается возможность оценить связность большого сайта на основании обследования некоторой его части. Исследования Хипса \cite{Heaps} и других авторов \cite{GelbukhSidorov,Zhang,KuboSatoMatsubara} показали, что при просмотре текста объем словаря увеличивается по степенному закону с ростом числа просмотренных слов.

В статье рассматривается выдвинутое авторами предположение, что аналогичная закономерность существует и в отношении ссылок в веб-графе.

\paragraph{Теоретическая часть.} Пусть просмотр веб-графа производится последовательными шагами, на каждом из которых просматривается \(e\) ссылок. И пусть на шаге \(j\) обнаруживается \(v_j\) уникальных ссылок. Тогда после шага \(i\) окажутся просмотренными \(E_i = ie\) ссылок, из которых уникальными будут \(V_i\) ссылок, где \(V_i = \sum_{j=1}^{i} v_j\). Предполагается, что для достаточно больших \(e\) при всех \(i\) выполняется формула
\begin{equation}
	\label{eqn:5}
	V_i = \alpha E_i^\beta
\end{equation}
где \(\alpha\) и \(\beta\) -- константы \((\alpha > 0, 0 < \beta < 1)\).

Зависимость \cref{eqn:5} -- статистическая, поэтому
\begin{enumerate}
	\item выполняется лишь приближенно;
	\item чем больше \(i\), тем меньше погрешность формулы.
\end{enumerate}

Для анализа, обработки веб-ресурсов, составления списков ссылок и уникальных
ссылок использовалась разработанная программно-аналитическая система для вебометрических исследований, основанная на обобщенном ядре поискового робота \cite{BlekanovSergeevMartynenko} и успешно апробированная \cite{MaksimovBlekanov,BlekanovSergeevMaksimovBOWTIE}. Непосредственный результат обработки поисковым роботом сайта -- таблица \(V_i (E_i) (i = \overline{1,N})\).

Для вычисления значений \(\alpha\) и \(\beta\) используем всю полученную таблицу -- решаем систему \(N\) линейных уравнений, полученных из \cref{eqn:5} логарифмированием:
\begin{equation}
	\label{eqn:6}
	x + A_i y = B_i, i = \overline{1,N},
\end{equation} где \(x = \ln{\alpha}; y = \beta; A_i = \ln{E_i}; B_i = \ln{V_i}\).

В \cref{eqn:6} решение избыточной системы находим методом наименьших квадратов:
\[
x = \frac{C_1 C_4 - C_2 C_3}{N (C_1^2 - C_3)}, y = \frac{C_1 C_2 - C_4}{C_1^2 - C_3},
\] здесь \( C_1 = \sum_{i=1}^{N} A_i, C_2 = \sum_{i = 1}^{N} B_i, C_3 = \sum_{i = 1}^{N} A_i^2 , C_4 = \sum_{i=1}^{N} A_i B_i .\)

\paragraph{Эксперимент.} На примере сайтов 15 научно-образовательных подразделений Санкт-Петербургского государственного университета (СПбГУ) (табл.~\cref{tab:spbuDivisions}) проведем проверку гипотезы с целью получения параметров \(\alpha, \beta\) по каждому сайту.

\begin{table} [htbp]%
	\centering
	\caption{Исследуемые подразделения СПбГУ.}%
	\label{tab:spbuDivisions}% label всегда желательно идти после caption
	\renewcommand{\arraystretch}{1.5}%% Увеличение расстояния между рядами, для улучшения восприятия.
	\begin{SingleSpace}
		\begin{tabulary}{\textwidth}{@{}>{\zz}L >{\zz}C >{\zz}C@{}} %Вертикальные полосы не используются принципиально, как и лишние горизонтальные (допускается по ГОСТ 2.105 пункт 4.4.5) % @{} позволяет прижиматься к краям
			\toprule     %%% верхняя линейка
			Научно-образовательное подразделение СПбГУ & Сайт подразделения & Аббревиатура подразделения \\
			\midrule %%% тонкий разделитель. Отделяет названия столбцов. Обязателен по ГОСТ 2.105 пункт 4.4.5
			Прикладная математика и процессы управления & www.apmath.spbu.ru & APM \\ 
			Экономика & www.econ.spbu.ru & FE \\
			Математико-механическое & www.math.spbu.ru & MaM \\
			Институт философии & www.philosophy.spbu.ru & IPh \\
			Социология & www.soc.spbu.ru & FS \\
			Высшая школа менеджмента & www.gsom.spbu.ru & IGSM \\ 
			Школа журналистики и массовых коммуникаций & www.jf.spbu.ru & ISJaC \\
			Художественное & www.arts.spbu.ru & FA \\
			Политические науки & www.politology.spbu.ru & FPS \\
			Психология & www.psy.spbu.ru & DP \\
			Восточное & www.orient.spbu.ru & FAaA \\
			История & www.history.spbu.ru & IH\\
			Физика & www.phys.spbu.ru & PF\\
			Юридическое & www.law.spbu.ru  & FL\\
			Философия & www.phil.spbu.ru & FPh \\
			\bottomrule %%% нижняя линейка
		\end{tabulary}%
	\end{SingleSpace}
\end{table}

\begin{table} [htbp]%
	\centering
	\caption{Зависимость числа уникальных ссылок \(V\) от их общего числа \(E\).}%
	\label{tab:uniqueLinkDependency}% label всегда желательно идти после caption
	\renewcommand{\arraystretch}{1.5}%% Увеличение расстояния между рядами, для улучшения восприятия.
	\def\tabularxcolumn#1{m{#1}}
	\begin{tabularx}{\textwidth}{@{}>{\raggedright}X >{\centering}m{1cm} >{\centering}m{1cm} >{\centering}m{1cm} >{\centering}m{1cm} >{\centering}m{1cm} >{\centering}m{1cm} >{\centering}m{1cm} >{\centering}m{1cm} >{\centering}m{1cm} >{\centering\arraybackslash}m{1cm}@{}}% Вертикальные полосы не используются принципиально, как и лишние горизонтальные (допускается по ГОСТ 2.105 пункт 4.4.5) % @{} позволяет прижиматься к краям
		\toprule     %%% верхняя линейка
		Подразделение & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\
		\midrule %%% тонкий разделитель. Отделяет названия столбцов. Обязателен по ГОСТ 2.105 пункт 4.4.5
		%Санкт-Петербургский государственный университет & www.spbu.ru & 41 183 & 2 664 000 \\
		APM & 2655 & 4566 & 7247 & 9171 & 10 552 & 11 714 & 13 150 & 15 110 & 15 722 & 16 283\\ 
		FE & 6274 & 8044 & 10 576 & 12 308 & 13 481 & 14 180 & 14 720 & 15 193 & 15 525 & 15 856\\
		MaM & 3366 & 5537 & 8896 & 11 860 & 13 542 & 15 304 & 16 750 & 18 974 & 20 043 & 20 907\\
		IPh & 3468 & 6723 & 7274 & 9895 & 11 912 & 14 862 & 17 113 & 19 904 & 20 876 & 21 597\\
		FS & 4132 & 5906 & 7545 & 9007 & 10 678 & 11 623 & 12 731 & -- & -- & -- \\
		IGSM & 4727 & 7643 & 12 620 & 15 341 & 19 968 & 23 356 & 27 854 & 31 259 & 35 894 & 38 771 \\
		ISJaC & 5003 & 9572 & 12 464 & 16 576 & 19 782 & 24 226 & 28 961 & 32 705 & 35 306 & 36 978 \\
		FA & 3821 & 4215 & 4583 & -- & -- & -- & -- & -- & -- & -- \\
		FPS & 1003 & 1405 -- & -- & -- & -- & -- & -- & -- & -- & -- \\
		DP & 2002 & 3104 & 4336 & 5006 & 5601 & -- & -- & -- & -- & -- \\
		FAaA & 2805 & 4097 & 6812 & 8734 & 9805 & 10 975 & 12 046 & 13 208 & -- & -- \\
		IH & 6175 & -- & -- & -- & -- & -- & -- & -- & -- & -- \\
		PF & 1603 & 2561 & 3197 & 3196 & 4308 & -- & -- & -- & -- & -- \\
		FL & 3468 & 6680 & 9975 & 12 877 & 15 023 & 17 113 & 18 363 & 20 004 & 21 117 & 22 537 \\
		FPh & 3198 & 5204 & 6923 & 8477 & 9951 & -- & -- & -- & -- & -- \\		
		\bottomrule %%% нижняя линейка
	\end{tabularx}%
\end{table}

\paragraph{Результаты эксперимента.} В табл.~\cref{tab:uniqueLinkDependency} приведены основные результаты эксперимента. В верхней строке -- номер шага \((i)\). Шаг равен 40 000 ссылок. В следующих строках -- суммарное количество \(\overline{V_i}\) уникальных ссылок, найденных в результате \(i\) шагов.

В табл.~ даны параметры  \(\alpha\) и \(\beta\), полученные по результатам эксперимента. Расчет \(\alpha\) и \(\beta\) производился на более подробных данных (с шагом, равным 1000 ссылок), чем приведены в табл.~\cref{tab:uniqueLinkDependency}.

\begin{table} [htbp]%
	\centering
	\caption{Исследуемые подразделения СПбГУ.}%
	\label{tab:spbuScientificAndEduDepartments}% label всегда желательно идти после caption
	\renewcommand{\arraystretch}{1.5}%% Увеличение расстояния между рядами, для улучшения восприятия.
	\begin{SingleSpace}
		\begin{tabulary}{\textwidth}{@{}>{\zz}L >{\zz}C >{\zz}C@{}} %Вертикальные полосы не используются принципиально, как и лишние горизонтальные (допускается по ГОСТ 2.105 пункт 4.4.5) % @{} позволяет прижиматься к краям
			\toprule     %%% верхняя линейка
			Подразделение СПбГУ & \(\alpha\) & \( \beta \) \\
			\midrule %%% тонкий разделитель. Отделяет названия столбцов. Обязателен по ГОСТ 2.105 пункт 4.4.5
			APM & 860.976 & 0.23 \\ 
			FE & 2016.226 & 0.16 \\
			MaM & 1087.405 & 0.23 \\
			IPh & 212 & 0.36 \\
			FS & 204.529 & 0.33 \\
			IGSM & 3056.799 & 0.20 \\ 
			ISJaC & 4926.89 & 0.16  \\
			FA & 1427.368 & 0.1  \\
			FPS & 33.967 & 0.33  \\
			DP & 29.545 & 0.43  \\
			FAaA & 18.956 & 0.52  \\
			IH & 0.446 & 0.9 \\
			PF & 159.687 & 0.27 \\
			FL & 118.332 & 0.41 \\
			FPh & 1.939 & 0.70 \\
			\bottomrule %%% нижняя линейка
		\end{tabulary}%
	\end{SingleSpace}
\end{table}

Функции с вычисленными параметрами изображены на рисунке~\cref{fig:spbuDepartments}.

\begin{figure}[ht]
	\centerfloat{
		\includegraphics[scale=1]{spbuDepartments}
	}
	\caption{Графики функции \(V = \alpha E^{\beta} \) для разных подразделений.}\label{fig:spbuDepartments}
\end{figure}

Для подтверждения выдвинутой гипотезы были определены относительные погрешности представления числа уникальных ссылок через общее число ссылок с помощью формулы \cref{eqn:5}:
\[
\Delta_i = \frac{\lvert V_i - \overline{V_i} \rvert}{V_i}.
\]

Результаты помещены в табл.~\cref{tab:equationDependency}. Из нее видно, что погрешность мала и убывает с увеличением числа просмотренных ссылок.

\begin{table} [htbp]%
	\centering
	\caption{Зависимость погрешности формулы \cref{eqn:6} от номера шага.}%
	\label{tab:equationDependency}% label всегда желательно идти после caption
	\renewcommand{\arraystretch}{1.5}%% Увеличение расстояния между рядами, для улучшения восприятия.
	\def\tabularxcolumn#1{m{#1}}
	\begin{tabularx}{\textwidth}{@{}>{\raggedright}X >{\centering}m{1cm} >{\centering}m{1cm} >{\centering}m{1cm} >{\centering}m{1cm} >{\centering}m{1cm} >{\centering}m{1cm} >{\centering}m{1cm} >{\centering}m{1cm} >{\centering}m{1cm} >{\centering\arraybackslash}m{1cm}@{}}% Вертикальные полосы не используются принципиально, как и лишние горизонтальные (допускается по ГОСТ 2.105 пункт 4.4.5) % @{} позволяет прижиматься к краям
		\toprule     %%% верхняя линейка
		Подразделение & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\
		\midrule %%% тонкий разделитель. Отделяет названия столбцов. Обязателен по ГОСТ 2.105 пункт 4.4.5
		%Санкт-Петербургский государственный университет & www.spbu.ru & 41 183 & 2 664 000 \\
		APM & 0.730 & 0.606 & 0.428 & 0.323 & 0.260 & 0.212 & 0.146 & 0.049 & 0.037 & 0.026\\ 
		FE & 0.428 & 0.344 & 0.192 & 0.102 & 0.051 & 0.031 & 0.018 & 0.008 & 0.005 & 0.001 \\
		MaM & 0.729 & 0.620 & 0.444 & 0.307 & 0.248 &  0.185 & 0.139 & 0.054 & 0.028 & 0.010  \\
		IPh & 0.639 & 0.455 & 0.492 & 0.375 & 0.306 & 0.189 & 0.116 & 0.021 & 0.015 & 0.019  \\
		FS & 0.388 & 0.304 & 0.222 & 0.155 & 0.070 &  0.047 & 0.007  & -- & -- & -- \\
		IGSM & 0.814 & 0.738 & 0.601 &  0.543 & 0.431 & 0.358 & 0.258 & 0.189 & 0.091 & 0.038 \\
		ISJaC & 0.813 & 0.680 & 0.610 & 0.505 & 0.430 & 0.322 & 0.209 & 0.126 & 0.074 & 0.047  \\
		FA & 0.072 & 0.045 & 0.003 & -- & -- & -- & -- & -- & -- & -- \\
		FPS & 0.105 & 0.003 & -- & -- & -- & -- & -- & -- & -- & -- \\
		DP & 0.288 & 0.181 & 0.039 & 0.02 & 0.003 & -- & -- & -- & -- & -- \\
		FAaA & 0.401 & 0.390 & 0.179 & 0.093 & 0.093 & 0.077 & 0.065 & 0.044 & -- & -- \\
		IH & 0.001 & -- & -- & -- & -- & -- & -- & -- & -- & -- \\
		PF & 0.425 & 0.239 & 0.148 & 0.064 & 0.000 & -- & -- & -- & -- & -- \\
		FL & 0.619 & 0.448 & 0.302 & 0.200 & 0.148 & 0.099 & 0.093 & 0.064 & 0.059 & 0.038 \\
		FPh & 0.009 & 0.007 & 0.006 & 0.005 & 0.001 & -- & -- & -- & -- & -- \\		
		\bottomrule %%% нижняя линейка
	\end{tabularx}%
\end{table}

Таким образом, гипотеза о параболической зависимости числа уникальных ссылок от их общего количества подтверждена.

Кроме того, табл.~\cref{tab:spbuScientificAndEduDepartments} позволяет прийти к следующим предварительным выводам: 1) для нахождения приблизительного значения параметров \(\alpha\) и \(\beta\) достаточно исследовать часть сайта, сделав, например, лишь 3-4 шага; 2) параметры \(\alpha\) и \(\beta\) могут использоваться для кластерного анализа веб-ресурсов.

\subsection{Исследование закономерностей в гиперссылочной структуре сайтов большой размерности (должна быть на англ.)}\label{subsec:ch1/sec4/sub4}

\paragraph{Актуальность.} В настоящее время редкая организация не имеет собственного сайта. Сайт, в определенном смысле, лицо организации и его качеству придается большое значение. Качество сайта складывается из многих составляющих. Это и количество страниц, и их дизайн, и содержание страниц, и структура сайта.

Представление структуры сайта в виде ориентированного графа, узлы которого -- документы, а дуги -- ссылки, является общепринятым \cite{BroderKumarMaghoul}. Глобальными структурными характеристиками информационных сетей в Вебе занимаются многие исследователи. Так, в работе Broder A. и Kumar F. \cite{BroderKumarMaghoul} крупные сообщества сайтов представляются в ввиде графовой компоненты сильной связности, компонент In, Out и Tubes; в работах \cite{Thelwall,ThelwallZuccala,ThelwallWilkinsonMusgrove,PechnikovNwohiri} рассматривается распределение внешних ссылок и индекс цитирования различных университетских сайтов; в \cite{BlekanovSergeevMaksimovBOWTIE} вводятся характеристики связности сайта; в \cite{KenekayoroBuckleyThelwall} исследуется метод автоматической классификации ссылок и страниц по их характеристикам; и др.

Цель настоящей работы – исследование структуры сайта. Точнее – установления вида функциональной зависимости, связывающей число страниц сайта с числом его внутренних ссылок.

\subsubsection{Проверка гипотезы}

\paragraph{Постановка задачи.} Для решения этой задачи использовался специальный поисковый робот \cite{BlekanovSergeevMartynenko}. В процессе обхода сайта робот составляет два списка: список найденных страниц (узлов веб-графа) и список ссылок, связывающих найденные страницы (дуг веб-графа).

Шагом алгоритма просмотра сайта будем считать акт нахождения \(e\) ссылок. То есть, в результате \(i\) шагов находятся \(E_i = ei\) ссылок. Количество найденных на \(i\) шагах страниц обозначим через \(v_i = v(E_i)\). Будем обозначать число всех ссылок сайта через \(e_0\), число всех страниц -- \(v(e_0) = v_0\) . Поисковый робот позволяет не только найти число страниц и ссылок сайта, но и получить график функции \(v(e)\)).

Очевидно, что \(v \leq e \). Причем \(v_0 < e_0\) (случай \(v_0 = e_0\) для сайтов -- экзотический). Очевидно, также, что рост \(v(e)\) постепенно замедляется. Это объясняется тем, что с ростом числа найденных страниц растет вероятность того, что очередная ссылка указывает на уже найденную страницу. 

Попытаемся подобрать аналитическую функцию, приближенно описывающую экспериментальную \(v(E)\), со следующими свойствами:
\begin{enumerate}
	\item На плоскости \((e, v)\) проходит через начало координат.
	\item Возрастает с ростом \(e\).
	\item Производная функции убывает.
	\item Функция имеет простой вид и зависит от небольшого числа параметров.
\end{enumerate}

\paragraph{Эксперимент.} Будем сопоставлять функцию с экспериментальным набором \(V_i, E_i (i = 1,2, \dots, N)\), где \(N = \lceil \frac{e_0}{e} \rceil\). Нами исследованы сайты четырех университетов и получены соответствующие наборы (табл.~\cref{tab:uniSitesWithPages}).

\begin{table} [htbp]%
	\centering
	\caption{Сайты исследуемых университетов.}%
	\label{tab:uniSitesWithPages}% label всегда желательно идти после caption
	\renewcommand{\arraystretch}{1.5}%% Увеличение расстояния между рядами, для улучшения восприятия.
	\def\tabularxcolumn#1{m{#1}}
	\begin{tabularx}{\textwidth}{@{}>{\raggedright}X >{\centering}m{3.5cm} >{\centering}m{2.5cm} >{\centering\arraybackslash}m{2.5cm}@{}}% Вертикальные полосы не используются принципиально, как и лишние горизонтальные (допускается по ГОСТ 2.105 пункт 4.4.5) % @{} позволяет прижиматься к краям
		\toprule     %%% верхняя линейка
		Университет & URL-адрес & Всего страниц & Всего ссылок \\
		\midrule %%% тонкий разделитель. Отделяет названия столбцов. Обязателен по ГОСТ 2.105 пункт 4.4.5
		%Санкт-Петербургский государственный университет & www.spbu.ru & 41 183 & 2 664 000 \\
		Московский государственный университет & www.msu.ru & 47 832 & 1 891 000 \\				
		Университет Айдзу & www.u-aizu.ac.jp & 4 161 & 49 900 \\
		Токийский университет & www.u-tokyo.ac.jp & > 17 000 & 240 000 \\			
		\bottomrule %%% нижняя линейка
	\end{tabularx}%
\end{table}

Для оценки качества приближения будем использовать среднюю относительную погрешность:
\[
\Delta = \frac{1}{N} \sum \frac{\lvert V_i - v_i \rvert}{v_i}
\]

Далее рассматриваются три варианта аппроксимирующей функции:
\[v^{(1)} = \alpha E^\beta ,\]
\[v^{(2)} = \frac{\alpha E}{E + \beta},\]
\[v^{(3)} = \alpha (\ln(1 + E))^\beta .\]

Все три функции удовлетворяют требованиям 1, 2 и 3 при \(\alpha > 0, \beta > 0\). \(v^{(1)}\) удовлетворяет требованию 3, если к тому же  \(\beta < 0\).

Рассмотрим их последовательно:

\begin{enumerate}
	\item Прологарифмируем уравнение: \[\ln v^{(1)} = \ln \alpha + \beta \ln E .\] Получаем систему линейных уравнений \[x + A_i y = B_i, i = \overline{1,N} ,\] где \(x = \ln \alpha, y = \beta, A_i = \ln E_i, B_i = \ln V_i\). Ее решение методом наименьших квадратов: \[\alpha = \exp \frac{C_1C_4 - C_2C_3}{N(C_1^2 - C_3)}, \beta = \frac{C_1C_2 - C_4}{C_1^2 - C_3},\] где \(C_1 = \sum_{1}^{N}A_i, C_2 = \sum_{1}^{N}B_i, C_3 = \sum_{1}^{N}A_i^2, C_4 = \sum_{1}^{N}A_i B_i\).
	\item Система линейных уравнений: \[\alpha E_i - \beta V_i = V_i E_i, i = \overline{1,N}.\] Ее решение методом наименьших квадратов: \[\alpha = \frac{a_3 a_4 - a_2 a_5}{a_1 a_4 - a_2^2}, \beta = \frac{a_2 a_3 = a_1 a_5}{a_1 a_4 - a_2^2}, \] где \(a_1 = \sum_{1}^{N} E_i^2, a_2 = \sum_{1}^{N} E_i V_i, a_3 = \sum_{1}^{N} E_i^2 V_i, a_4 = \sum_{1}^{N} V_i^2, a_5 = \sum_{1}^{N} V_i^2 E_i\).
	\item Логарифмируем \(v^{(3)}\): \[\ln v^{(3)} = \ln \alpha + \beta \ln \ln (1 + E).\] Получаем систему, почти совпадающую со случаем 1). Ее решение будет таким же, за исключением того, что в первом случае \(A_i = \ln E_i\), а в этом \[A_i = \ln \ln (1+E).\]
\end{enumerate}

\paragraph{Результаты эксперимента.} На рисунках~\cref{fig:msuFunc},~\cref{fig:aizuFunc},~\cref{fig:tuniFunc} представлены графики функций \(v^{(1)}, v^{(2)}, v^{(3)}\) и \(V\) для каждого из перечисленных университетов.

\begin{figure}[ht]
	\centerfloat{
		\includegraphics[scale=0.35]{msuFunc}
	}
	\caption{Функции \(v^{(1)}, v^{(2)}, v^{(3)}\) и \(V\) Московского государственного университета.}\label{fig:msuFunc}
\end{figure}

\begin{figure}[ht]
	\centerfloat{
		\includegraphics[scale=0.35]{aizuFunc}
	}
	\caption{Функции \(v^{(1)}, v^{(2)}, v^{(3)}\) и \(V\) для университета Айдзу.}\label{fig:aizuFunc}
\end{figure}

\begin{figure}[ht]
	\centerfloat{
		\includegraphics[scale=0.35]{tuniFunc}
	}
	\caption{Функции \(v^{(1)}, v^{(2)}, v^{(3)}\) и \(V\) для Токийского университета.}\label{fig:tuniFunc}
\end{figure}

В табл.~\cref{tab:uniErrors} приведены относительные погрешности каждой из формул для каждого из университетов.

\begin{table} [htbp]%
	\centering
	\caption{Средняя относительная погрешность функций \(v^{(1)}, v^{(2)}, v^{(3)}\) для каждого университета.}%
	\label{tab:uniErrors}% label всегда желательно идти после caption
	\renewcommand{\arraystretch}{1.5}%% Увеличение расстояния между рядами, для улучшения восприятия.
	\def\tabularxcolumn#1{m{#1}}
	\begin{tabularx}{\textwidth}{@{}>{\raggedright}X >{\centering}m{3.5cm} >{\centering}m{2.5cm} >{\centering\arraybackslash}m{2.5cm}@{}}% Вертикальные полосы не используются принципиально, как и лишние горизонтальные (допускается по ГОСТ 2.105 пункт 4.4.5) % @{} позволяет прижиматься к краям
		\toprule     %%% верхняя линейка
		Университет & \(v^{(1)}\) & \(v^{(2)}\) & \(v^{(3)}\) \\
		\midrule %%% тонкий разделитель. Отделяет названия столбцов. Обязателен по ГОСТ 2.105 пункт 4.4.5
		%Санкт-Петербургский государственный университет & www.spbu.ru & 41 183 & 2 664 000 \\
		Московский государственный университет & 0,092 & 0,037 & 0,014 \\				
		Университет Айдзу & 0,167&  0,209 &  0,229 \\
		Токийский университет & 0,068 & 0,076 & 0,062 \\			
		\bottomrule %%% нижняя линейка
	\end{tabularx}%
\end{table}

\paragraph{Выводы.} Рассмотрена проблема нахождения функции, аппроксимирующей экспериментальный график зависимости числа найденных страниц сайта от числа найденных ссылок. Предложены три варианта аппроксимации. Выяснено, что наилучшие приближения получаются для степенной и дробно-линейной функции. Предложенные функции могут быть использованы для изучения параметров сайтов и их кластеризации.

\subsection{Применение модифицированного алгоритма LSH для кластеризации внешнего окружения веб-пространства университетов}\label{subsec:ch1/sec4/sub6}

Для большинства крупных организаций немаловажное значение имеет их рейтинг, который рассчитывается в зависимости от параметров, связанных с их родом деятельности. В частности, на общий рейтинг ВУЗа большое влияние оказывает его вебометрический рейтинг. Известно, одним из главных показателей, который влияет на вебометрический рейтинг любой организации, в том числе и университета, является количество внешних ресурсов \cite{RankingWeb}, ссылающихся на сайт университета. Но кроме количества ссылающихся ресурсов также важно понимать качество этих ресурсов, их природу, определить к какой области относится тот или иной внешний ресурс. Данные веб-ресурсы образуют группы сайтов с одинаковым родом деятельности. Таким образом, возникает задача выявления этих групп -- кластеризации. Требуется определить степень влияния количества и размеров найденных групп на вебометрический рейтинг сайтов университетов. По найденным кластерам можно определить, с какими группами внешних ресурсов следует сайтам университетов выстраивать гиперссылочные взаимосвязи для повышения цитируемости.

Объектом данного исследования являются университетские сайты, которые имеют чрезвычайно большие размеры (например, сайт СПбГУ содержит более \textit{50} тыс. внутренних веб-страниц и около 5 млн. гиперссылок) \cite{BlekanovMoskalets,BlekanovSergeevMaksimovBOWTIE}. Окружение таких сайтов может составлять десятки – сотни тысяч страниц. Стандартными методами кластеризации такого объема внешних веб-ресурсов не обойтись, так как при работе с большими коллекциями документов многие из данных методов показывают крайне неудовлетворительные результаты в плане производительности \cite{EneImMoseley} (например, метод Single linkage позволяет создавать кластеры произвольной формы, однако имеет высокую трудоемкость -- \(O(n^2)\), где \(n\) -- число документов). Для того чтобы избавиться от «проклятия» размерности применяется вероятностный метод понижения размерности многомерных данных Locality-Sensitive Hashing (в дальнейшем LSH), основная идея которого состоит в подборе хэш-функций для некоторых измерений для того, чтобы похожие объекты попадали в одну корзину \cite{Buhler}.

\subsubsection{Уменьшение размерности для анализа больших коллекций текстовых документов}

Для преобразования текстовых документов в числовые множества в данной работе использовался метод Shingling \cite{Broder}, который разбивает каждый документ на небольшие множества по \(k\) слов в каждом. Далее, для сравнения документов, применялась технология Min Hashing, которая позволяет быстро сравнивать множества, содержащие большое число элементов.

\textit{Мера Жаккара.} В работе для определения похожести двух текстов применяется мера Жаккара, которая похожесть множеств определяет отношением числа элементов, входящих в пересечение двух множеств к числу элементов, входящих в объединение этих множеств \cite{SingthongchaiNiwattanakul}:
\[
\textit{Sim}(C_1, C_2) = \frac{\lvert C_1 \cap C_2\rvert}{\lvert C_1 \cup C_2\rvert}
\]

Так как каждый документ преобразуется во множества, состоящие из \(k\) слов, то набор документов можно представить в виде сильно разреженной булевой матрицы, где столбцы представляют собой документы, а строки -- элементы универсального множества (например, множество элементов, где каждый элемент представляет собой множество \(k\) слов). Элемент такой матрицы равен единице, если документ (столбец) содержит данное множество k-слов. В противном случае элемент матрицы равен нулю.

\textit{Minhashing.} Для большой коллекции документов булева матрица будет сильно разрежена. Соответственно, матрица, описывающая такую коллекцию, займет много места в памяти. Кроме того, дальнейшая ее обработка так же займет большое количество времени. Для того чтобы решить возникшие проблемы, данную матрицу преобразуем в матрицу, хранящую определенное количество хэш-функций и информацию о сходстве между похожими документами.

\textit{Minhash}-функция \(h(C)\) -- это номер первой строки для столбца \(C\) в булевой матрице, где строки перемешаны случайным образом \cite{ChumPerdochMatas}.

Как видим, число случайных перестановок задает число Minhash-функций. Например, можно использовать сто случайных перестановок для создания ста сигнатур для каждого столбца матрицы.

Сигнатуры могут быть записаны в другой матрице сигнатур (Signature matrix), чьи колонки представляют собой документы, а строки -- Minhash-значения.

Чем больше хэш-функций, тем выше вероятность того, что \(\textit{Sim}(C_1, C_2) = \textit{Sim}(M[C_1], M[C_2])\), где \(C_i\) -- столбец булевой матрицы, \(M[C_i]\) -- столбец матрицы сигнатур. Это важное свойство позволяет преобразовывать булевы матрицы с большим количеством строк в небольшие матрицы сигнатур, сохраняя сходство похожих множеств. Соответственно, похожесть двух столбцов определяется долей строк, в которых они равны.

Таким образом, каждый документ может быть представлен в виде вектора, число элементов которого равно количеству Minhash-функций.

\textit{Locality-Sensitive Hashing (LSH).} Основная идея: сгенерировать из большого множества документов маленькие списки пар документов, чья схожесть должна быть посчитана. Для сравнения двух документов (столбцов) устанавливается порог \(t\) \((t < 1)\). Пара документов считается похожей только в том случае, если доля одинаковых значений в матрице сигнатур больше \(t\). Для матрицы сигнатур необходимо несколько раз вычислить хэш-значения, и поместить документы с одинаковым значением в одну корзину (\textit{bucket}). Документы, которые хоть раз попали в одну корзину, будут рассмотрены как кандидаты на сравнение \cite{GionisIndykMotwani}.

Для множественного подсчета хэш-функций столбцов, необходимо разбить матрицу сигнатур \textit{M} на \(b\) частей по \(r\) строк в каждой. Для каждого \(b\) подсчитать хэш-значения столбцов и поместить столбцы с равным значением в одну корзину. Кандидатами на сравнение будут те столбцы, которые хоть раз попали в одну корзину. Для правильной работы алгоритма необходимо настроить \(r\) и \(b\) таким образом, чтобы похожие документы попадали в одну корзину, а непохожие -- в разные.

\subsubsection{Иерархическая кластеризация с использованием LSH}

Данный алгоритм кластеризации использует хэш-таблицы, которые были сформированы в результате LSH \cite{KogaIshibashiWatanabe}. Данный алгоритм кластеризации с высокой долей вероятности создает такие же кластеры, как и в методе Single linkage. Ниже представлено детальное описание алгоритма.

\textit{Предварительные условия:}
\begin{itemize}
	\item \(t < 1\) -- порог, задающий минимальную схожесть документов;
	\item \(r = 1\) -- начальное значение строк матрицы сигнатур в каждой группе;
	\item \(r_\textit{min}\) -- минимальное значение строк матрицы сигнатур в каждой группе;
	\item \(\Delta\) -- коэффициент уменьшения параметра \(r\).
\end{itemize}

Каждый документ представляет собой отдельный кластер.

\textit{Шаг 1.} Для каждой группы \(b\) в каждом столбце вычислить хэш-функции, и сохранить столбцы, у которых хэш-значения хотя бы раз попали в одну корзину, при условии, что в корзине должны находиться столбцы, принадлежащие разным кластерам. Если, например, какие то два столбца принадлежат одному кластеру, то один из случайно выбранных столбцов удаляется из корзины.

\textit{Шаг 2.} Для каждого из столбцов, входящих в одну корзину, отобрать пары кластеров, расстояние между которыми больше \(t\).

\textit{Шаг 3.} Пары кластеров, соответствующие парам, полученным на шаге 2, объединяются в один.

\textit{Шаг 4.} Если \(r \leq r_\textit{min}\), то алгоритм прекращает работу. Иначе, переход на шаг 5.

\textit{Шаг 5.} \(r = r - \Delta\). Переход на шаг 1.

\textit{Оценка качества кластеризации на основе LSH.} Для оценки качества модифицированного метода агломеративной кластеризации с использованием LSH в данной работе использовалась тестовая коллекция текстовых документов Reuters-21578, содержащая \textit{21 578} документов. На данной коллекции оценивались два метода иерархической кластеризации: стандартный алгоритм иерархической кластеризации single-link и модифицированный алгоритм single-link, основанный на применении LSH. В качества основных метрик качества в работе использовались точность (\(R\)) полнота (\(P\)), аккуратность (\textit{Acc}) и \(F\)-мера.

В табл.~\cref{tab:clasterizationReuters} приведены результаты оценки качества иерархической кластеризации для каждого из алгоритмов, а так же время работы каждого алгоритма при кластеризации \textit{1000}, \textit{10 000} и \textit{20 000} документов.

\begin{table}[ht]%
	\caption{Результаты оценки качества кластеризации на коллекции Reuters.}%
	\label{tab:clasterizationReuters}% label всегда желательно идти после caption
	\renewcommand{\arraystretch}{1.6}%% Увеличение расстояния между рядами, для улучшения восприятия.
	\def\tabularxcolumn#1{m{#1}}
	\begin{tabularx}{\textwidth}{@{}>{\raggedright}X >{\centering}m{3.2cm} >{\centering}m{1.5cm} >{\centering}m{1.5cm} >{\centering}m{1.5cm} >{\centering}m{2.3cm} >{\centering\arraybackslash}m{2cm}@{}}% Вертикальные полосы не используются принципиально, как и лишние горизонтальные (допускается по ГОСТ 2.105 пункт 4.4.5) % @{} позволяет прижиматься к краям
		\toprule     %%% верхняя линейка
		Алгоритм & Количество документов & \textit{Acc}, \% & \(R\), \% & \(P\), \% & \(F\)-мера, \% & Время работы\\
		\midrule %%% тонкий разделитель. Отделяет названия столбцов. Обязателен по ГОСТ 2.105 пункт 4.4.5
		 & 1000 & 75 & 79 & 60 & 68 & 4 с \\
		 Single-Link & 10 000 & 79 & 82 & 62 & 71 & 410 с \\
		 & 20 000 & -- & -- & -- & -- & > 1 ч \\
		 \midrule
		 & 1000 & 72 & 81 & 66 & 73 & 3 с \\
		 Single-Link + LSH & 10 000 & 72 & 80 & 69 & 74 & 41 с \\
		 & 20 000 & 78 & 83 & 64 & 72 & 90 с \\
		\bottomrule %%% нижняя линейка
	\end{tabularx}%
\end{table}

Как видно из таблицы, аккуратность и полнота метода Single-Link + LSH близки по значениям к аккуратности и полноте метода Single-Link. Точность же разработанного метода иногда превосходит точность метода Single-Link. \(F\)-меры у обоих методов примерно одинаковые.

На рис.~\cref{fig:clasterizationTime} приведена зависимость продолжительности работы алгоритмов кластеризации от числа входных документов.

\begin{figure}[ht]
	\centerfloat{
		\includegraphics[scale=0.7]{clasterizationTime}
	}
	\caption{Время работы алгоритмов кластеризации.}\label{fig:clasterizationTime}
\end{figure}

Графики показываю, что алгоритм кластеризации с использованием LSH работает гораздо быстрее, чем алгоритм без использования LSH. С увеличением числа документов время работы алгоритма с использованием LSH линейно растет.

\subsubsection{Эксперимент. Исследование сайтов университетов методом кластеризации на основе LSH}
\paragraph{Постановка эксперимента.} В эксперименте требовалось для заданного списка сайтов университетов России, США и Великобритании, занимающих по своим регионам ведущие позиции в вебометрическом рейтинге \cite{RankingWeb}, с помощью специализированного поискового робота \cite{BlekanovSergeevMartynenko} и базы данных Majestic \cite{Majestic} получить списки и содержимое всех внешних веб-страниц, которые их цитируют. А также с помощью разработанного авторами метода агломеративной кластеризации на основе алгоритма LSH выявить целевые группы найденных внешних веб-ресурсов с одинаковым родом деятельности и степень их влияния на вебометрический рейтинг сайтов исследуемых ВУЗов.

Для исследования выбраны следующие сайты ВУЗов:
\begin{itemize}
	\item Московский государственный университет им. М.В. Ломоносова (msu.ru), Россия;
	\item Санкт-Петербургский государственный университет (spbu.ru), Россия;
	\item Новосибирский государственный университет (nsu.ru), Россия;
	\item Массачусетский технологический институт (mit.edu), США;
	\item Гарвардский университет (harvard.edu), США;
	\item Стэнфордский университет (stanford.edu), США;
	\item Кембриджский университет (cam.ac.uk), Великобритания;
	\item Оксфордский университет (ox.ac.uk), Великобритания;
	\item Университетский колледж Лондона (ucl.ac.uk), Великобритания.
\end{itemize}

\paragraph{Результаты сбора.} В итоге сбора данных для анализа внешних веб-ресурсов сайтов исследуемых университетов были получены следующие результаты, представленные в табл.~\cref{tab:externalResources}.

\begin{table}[ht]%
	\caption{Количество внешних ресурсов, окружающих университеты.}%
	\label{tab:externalResources}% label всегда желательно идти после caption
	\renewcommand{\arraystretch}{1.6}%% Увеличение расстояния между рядами, для улучшения восприятия.
	\def\tabularxcolumn#1{m{#1}}
	\begin{tabularx}{\textwidth}{@{}>{\centering}X  >{\centering}m{2.6cm} >{\centering}m{2.6cm} >{\centering}m{2.6cm} >{\centering}m{2.6cm} >{\centering\arraybackslash}m{2.6cm}@{}}% Вертикальные полосы не используются принципиально, как и лишние горизонтальные (допускается по ГОСТ 2.105 пункт 4.4.5) % @{} позволяет прижиматься к краям
		\toprule     %%% верхняя линейка
		URL-адрес ВУЗа & Позиция в рейтинге Webometrics &  Внешние ссылки на сайте ВУЗа & Внешние домены на сайте ВУЗа & Количество цитирующих сайт ВУЗа веб-страниц & Количество цитирующих сайт ВУЗа доменов \\
		\midrule %%% тонкий разделитель. Отделяет названия столбцов. Обязателен по ГОСТ 2.105 пункт 4.4.5
		spbu.ru & 539 & 3 280 & 469 & 1 599 059 & 16 028 \\
		msu.ru & 129 & 817 & 262 & 7 039 127 & 39 416 \\
		nsu.ru & 616 & 5 747 & 921 & 926 004 & 15 186 \\
		harvard.edu & 1 & 918 & 163 & 75 994 723 & 319 445 \\
		stanford.edu & 2 & 175 & 79 & 29 551 130 & 311 148 \\
		mit.edu & 3 & 9 & 6 & 41 271 678 & 324 989 \\
		ox.ac.uk & 16 & 8 075 & 1 631 & 8 920 524 & 117 959 \\
		cam.ac.uk & 15 & 3 385 & 1 174 & 12 084 107 & 120 796 \\
		ucl.ac.uk & 24 & 25 218 & 5 683 & 4 733 566 & 66 035 \\
		\bottomrule %%% нижняя линейка
	\end{tabularx}%
\end{table}

Таблица показывает, что университеты США, занимающие первые три позиции вебометрического рейтинга, имеют значительно больше внешних ресурсов, ссылающихся на них, чем ведущие университеты России и Великобритании. Ведущие университеты Великобритании также опережают российские по общему количеству ссылок и доменов, ссылающихся на них. Количество внешних ресурсов напрямую влияет на такой вебометрический индикатор, как Impact ресурса в Вебе.

На показатель видимости ресурса в Вебе, помимо числа внешних ссылающихся ресурсов, также влияет и их качество. Для кластеризации внешних Веб-ресурсов были выбраны только англоязычные и русскоязычные страницы Веба. В табл.~\cref{tab:externalResourcesClasterization} приведено количество доменов для каждого университета, к которым применялся метод агломеративной кластеризации с использованием LSH.

\begin{table}[ht]%
	\centering
	\caption{Количество внешних ресурсов для кластеризации.}%
	\label{tab:externalResourcesClasterization}% label всегда желательно идти после caption
		\begin{tabular}{| c | c | c | c | c |}% Вертикальные полосы не используются принципиально, как и лишние горизонтальные (допускается по ГОСТ 2.105 пункт 4.4.5) % @{} позволяет прижиматься к краям
			\hline
			\makecell{\\Университет} & \multicolumn{2}{c|}{\makecell{Внешние домены на \\ сайте ВУЗа}} &  \multicolumn{2}{c|}{\makecell{Цитирующие сайт ВУЗа \\ домены}}\\
			\cline{2-5}
			& на русском & на английском & на русском & на английском\\
			\hline
			spbu.ru & 342 & 81 & 7 373 & 3 929  \\
			\hline
			msu.ru & 177 & 62 & 3 343 & 1 008 \\
			\hline
			nsu.ru & 535 & 242 & 2 508 & 1 428  \\
			\hline
			harvard.edu & 4 & 151 & 1 545 & 16 632  \\
			\hline
			stanford.edu & 5 & 65 & 409 & 6 088  \\
			\hline
			mit.edu & 3 & 3 & 1 625 & 24 930  \\
			\hline
			ox.ac.uk & 29 & 1 399 & 126 & 3 187  \\
			\hline
			cam.ac.uk & 91 & 4 668 & 671 & 9 196 \\
			\hline
			ucl.ac.uk & 14 & 1 054 & 162 & 3 112  \\
			\hline
		\end{tabular}%
\end{table}

Ниже приведены результаты кластеризации внешних ресурсов. Для определения их тематики извлекались наиболее часто встречающиеся множества слов (shingles), которые присутствовали в кластере. В результате анализа были выделены основные часто встречающиеся группы: университеты, научные сообщества, поисковые системы, социальные сети (включая различные блоги, форумы и т.д.), медиасфера, новостные порталы и сайты других организаций. Таким образом, определялась численность каждой группы для университета.

\paragraph{Результаты кластеризации внешних доменов на сайте ВУЗа.} Внешние домены -- это домены, на которые ссылаются исследуемые сайты университетов. Ввиду ограниченного числа внешних ссылок на сайтах университетов США, были рассмотрены только внешние ресурсы университетов России и Великобритании. Были получены кластеры для каждого из исследуемых университетов. Ниже представлена сравнительная гистограмма внешнего окружения университетов России и Великобритании (рис.~\cref{fig:histogramUKRU}).

\begin{figure}[ht]
	\centerfloat{
		\includegraphics[scale=0.7]{histogramUKRU}
	}
	\caption{Сравнительная гистограмма университетов России и Великобритании.}\label{fig:histogramUKRU}
\end{figure}

Рисунок показывает, что внешнее ресурсы, на которые ссылаются сайты университетов, практически совпадают. Университеты и научные сообщества составляют значительную долю внешних ресурсов, на которые ссылаются сайты университетов России и Великобритании.

\paragraph{Результаты кластеризации цитирующих сайт ВУЗа доменов.} Цитирующие веб-ресурсы -- это сайты, сгруппированные по доменам, которые ссылаются на исследуемые сайты университетов. Были получены кластеры для каждого из исследуемых университетов. Ниже приведена сравнительная гистограмма внешнего окружения университетов, представляющих страны России, США и Великобритании (рис.~\cref{fig:histogramUKRUUS}).

\begin{figure}[ht]
	\centerfloat{
		\includegraphics[scale=0.7]{histogramUKRUUS}
	}
	\caption{Сравнительная гистограмма ресурсов, ссылающихся на университеты России, США и Великобритании.}\label{fig:histogramUKRUUS}
\end{figure}

Анализ сайтов, ссылающихся на сайты ВУЗов, показал, что университеты из одной страны имеют схожее внешнее окружение во многих областях. У всех университетов преобладают такие группы, как Университеты и Научные сообщества. У университетов США и Великобритании эта доля выше, чем у России. Соответственно, в других областях доля российских университетов больше.

Можно сделать вывод о том, что чем выше доля университетов и научных сообществ среди внешних ресурсов университета, тем выше его вебометрический показатель Impact. Этот показатель зависит от количества и качества внешних ресурсов, ссылающихся на сайт. Внешние группы, содержащие университеты и научные сообщества, имеют гораздо больший вес для вебометрического рейтинга, чем другие группы внешних ресурсов.

\paragraph{Общие выводы.} Разработан алгоритм иерархической кластеризации с использованием вероятностного метода понижения размерности многомерных данных Locality-Sensitive Hashing, оптимально подходящий для кластеризации больших коллекций документов (massive datasets). Данный алгоритм был апробирован на тестовой коллекции текстовых документов Reuters. На тестовой коллекции алгоритм показал приемлемую точность (accuracy) и \(F\)-measure в сравнении с классическими методами иерархической кластеризации. Метод кластеризации с использованием LSH значительно превосходит по скорости работы классические методы иерархической кластеризации.

Разработанный алгоритм был использован для анализа веб-пространства нескольких университетов с их окружением.

Получены данные о внешнем окружении веб-пространства университетов России и Великобритании и о ресурсах, ссылающихся на сайты университетов России, США и Великобритании.

\subsection{Теоретико-графовые характеристики в вебометрических исследованиях внутренней топологии крупных сегментов Веба}\label{subsec:ch1/sec4/sub7}

\paragraph{Введение.} В настоящее время стремительно развивается молодое научное направление вебометрика \cite{AlmindIngwersen,Thelwall,Pechnikov}, которая занимается исследованиями количественных и качественных характеристик топологии (гиперссылочной структуры) различных веб-сегментов. Основоположниками таких исследований являются испанская группа Cybermetrics Lab, которая разработала вебометрический рейтинг сайтов различных крупных организаций \cite{RankingWeb} (таких как научно-образовательные учреждения, больницы, научно-исследовательские центры и т. п.).  

Одной из актуальных задач вебометрики является задача анализа внутренней топологии различных сайтов \cite{Thelwall,BlekanovSergeevMaksimov,MaksimovBlekanov,BlekanovSergeevMaksimovBOWTIE}, а также выявление критериев оценки качества веб-сегментов и сравнение этих сегментов по полученным оценкам.  

Для решения вышеуказанной задачи в статье авторами рассмотрены известные теоретико-графовые характеристики веб-графов и разработан комплекс программ на их основе. Данный программный комплекс используется для вычисления указанных выше характеристик, их сравнения на разных веб-сегментах большой размерности, а также для визуализации результатов сравнительного анализа.  

\paragraph{Эксперимент.} В работе ставился эксперимент, в котором разработанный авторами комплекс программ для выявления значимых страниц внутренней топологии крупных веб-сегментов (с помощью используемых теоретико-графовых характеристик) и их визуализации апробировался на крупных сайтах университетского Веба. 

Получение внутренней топологии веб-ресурсов в эксперименте выполнялось с помощью программно-аналитического комплекса для вебометрических исследований, основанного на обобщенном ядре поискового робота \cite{BlekanovSergeevMartynenko} и успешно апробированного в исследованиях \cite{BlekanovSergeevMaksimov,MaksimovBlekanov,BlekanovSergeevMaksimovBOWTIE}.

В качестве теоретико-графовых характеристик сайтов были выбраны несколько мер центральности и связности веб-графов, а именно:
\begin{enumerate}
	\item Степень вершины (Degree Centrality) -- показатель, указывающий для каждой страницы количество страниц, связанных с ней. В веб-графе вычисляются полустепень захода (indegree, количество входящих ссылок) и полустепень исхода (outdegree, количество исходящих ссылок) ссылок \cite{WassermanFaust,OrtegaAguillo}.
		\item Мера центральности Betweenneess -- показатель, указывающий насколько часто данная веб-страница лежит на кратчайшем пути между всеми парами страниц сайта \cite{WassermanFaust,OrtegaAguillo}.
		\item Мера центральности Closeness -- среднее расстояние от данной веб-страницы до всех остальных страниц сайта \cite{WassermanFaust}.
		\item Мера центральности PageRank -- показатель важности страницы. Чем выше ее показатель, тем она важнее \cite{PageBrinMotwani}.
		\item Мера связности p-Cliques -- подграф, который представляет собой полный граф. p -- количество вершин в данном подграфе \cite{OrtegaAguillo}.
		\item Мера связности k-Cores -- подграф, в котором каждая страница связана по крайне мере с k другими страницами в этом подграфе \cite{OrtegaAguillo}.
\end{enumerate}

А также использовались такие общие показатели топологии, как:
\begin{enumerate}
	\item Расстояние (Distance) -- средняя длина всех кратчайших путей в графе \cite{MaksimovBlekanov}. 
	\item Диаметр (Diameter) -- длина самого большого кратчайшего пути в графе \cite{MaksimovBlekanov}.
\end{enumerate}

В качестве сайтов университетского Веба были взяты следующие сайты из Мирового вебометрического рейтинга университетов \cite{RankingWeb}:
\begin{enumerate}
	\item Сайт, занимающий первое место в общем рейтинге:
		\begin{enumerate}
				\item Сайт Гарвардского университета -- ГУ (www.harvard.edu). 
			\end{enumerate}
	\item Сайты, занимающие первые места в рейтинге по Российской Федерации: 
		\begin{enumerate}
				\item Сайт Московского государственного университета -- МГУ (www.msu.ru). 
				\item Сайт Санкт-Петербургского государственного университета -- СПбГУ (www.spbu.ru).
			\end{enumerate}
\end{enumerate}

При получении внутренней топологии выше указанных сайтов учитывались только главные их домены (поддомены не рассматривались). 

Используя вышеуказанный программный комплекс, требовалось получить визуальное представление топологии сайтов университетов в виде композиции наилучших значимых веб-страниц по всем мерам центральности (по каждой мере выбирался топ-10 страниц с наилучшими весами), а также построить и оценить расстояние от главной страницы до этих страниц.

\paragraph{Результаты эксперимента.} В ходе эксперимента для выбранных сайтов университетского Веба были получены следующие значения мер связности и общих показателей топологии (табл.~\cref{tab:uniPagesInnerTopology}):

\begin{table} [htbp]%
	\centering
	\caption{Теоретико-графовые характеристики внутренней топологии университетских сайтов.}%
	\label{tab:uniPagesInnerTopology}% label всегда желательно идти после caption
	\renewcommand{\arraystretch}{1.5}%% Увеличение расстояния между рядами, для улучшения восприятия.
	\begin{SingleSpace}
		\begin{tabulary}{\textwidth}{@{}>{\zz}L >{\zz}C >{\zz}C >{\zz}C@{}} %Вертикальные полосы не используются принципиально, как и лишние горизонтальные (допускается по ГОСТ 2.105 пункт 4.4.5) % @{} позволяет прижиматься к краям
			\toprule     %%% верхняя линейка
			Теоретико-графовые характеристики & ГУ & МГУ & СПбГУ\\
			\midrule %%% тонкий разделитель. Отделяет названия столбцов. Обязателен по ГОСТ 2.105 пункт 4.4.5
			Количество страниц & 451 & 45 557 & 51 945 \\				
			Количество ссылок, принадлежащих главному домену & 27 615 & 2 043 221 & 4 862 750 \\
			Общее количество ссылок & 72 355 & 2 143 474 & 5 017 815 \\
			Расстояние & 3.04 & 7.19 & 6.02 \\
			Расстояние & 7 & 30 & 18 \\
			Среднее значение indegree, outdegree & 13.59 & 28.1 & 69.79 \\
			Количество вершин в p-Cliques & 26 & 10 & 144 \\
			\bottomrule %%% нижняя линейка
		\end{tabulary}%
	\end{SingleSpace}
\end{table}

Композиция всех наилучших (по заданным мерам центральности) значимых веб-страниц исследуемых сайтов представлена на рис.~\cref{fig:harvardUComposition}-\cref{fig:spbUComposition}. На рис.~\cref{fig:harvardUComposition}-\cref{fig:spbUComposition} введены следующие обозначения цветов: 
\begin{itemize}
	\item Красный -- главная страница сайта; 
	\item  Зеленый -- полустепень исхода; 
	\item Бирюзовый -- полустепень захода; 
	\item Синий -- мера Betweenness; 
	\item Розовый -- мера Closeness; 
	\item Оранжевый -- мера PageRank.
\end{itemize}

\begin{figure}[ht]
	\centerfloat{
		\includegraphics[scale=0.48]{harvardUComposition}
	}
	\caption{Композиция всех наилучших значимых веб-страниц сайта ГУ.}\label{fig:harvardUComposition}
\end{figure}

\begin{figure}[ht]
	\centerfloat{
		\includegraphics[scale=0.5]{moscowSUComposition}
	}
	\caption{Композиция всех наилучших значимых веб-страниц сайта МГУ.}\label{fig:moscowSUComposition}
\end{figure}

\begin{figure}[ht]
	\centerfloat{
		\includegraphics[scale=0.46]{spbUComposition}
	}
	\caption{Композиция всех наилучших значимых веб-страниц сайта СПбГУ.}\label{fig:spbUComposition}
\end{figure}

\paragraph{Общие выводы.} По результатам эксперимента можно сделать следующие выводы:
\begin{enumerate}
	\item Сайт ГУ представляет собой навигационный сайт, перенеся большую часть информации на свои поддомены (ссылки главного домена составляют \textit{38.2\%} от общего числа ссылок). Данное явление подтверждается тем, что веб-страницы по полустепени исхода у ГУ лежат близко к главной странице (глубина 2-4), в то время как у МГУ и СПбГУ они лежат на определенном удалении (8-11 у МГУ и 6-9 у СПбГУ).
	\item Показатели СПбГУ (такие как расстояние, диаметр, количество вершин в p-Cliques, положение значимых вершин относительно начальной страницы) лучше, чем таковые у МГУ. Это значит, что сайт СПбГУ показывает более высокую связность, нежели сайт МГУ.
	\item Авторитетные страницы расположены на следующем уровне глубины после главной страницы, о чем говорит их высокий вес PageRank.
	\item Значимые веб-страницы по мере Betweenness у всех сайтов лежат в радиусе пяти ссылок от главной страницы, однако у сайта МГУ они расположены последовательно. Это объясняется тем, что множество кратчайших путей проходит через эти страницы. При отказе одной из таких страниц увеличиваются размеры множества кратчайших путей, а также появляется риск полной потери связей с множеством страниц.
	\item Значимые веб-страницы по мере Closeness у ГУ лежат на уровнях 1 и 3, у МГУ -- на уровне 4 и 5, у СПбГУ -- на уровне 3. Это показывает, что рассмотренные сайты централизованы недалеко от главной страницы.
\end{enumerate}

Также эксперимент показал некоторую особенность главных страниц рассмотренных сайтов, а именно:
\begin{itemize}
	\item главная страница ГУ не попала ни в один из топ-10;
	\item главная страница МГУ попала лишь в топ-10 по степени захода;
	\item главная страница СПбГУ попала сразу в три топ-10: по степени захода, по мере Betweenness и по мере PageRank. 
\end{itemize}

В дальнейшем планируется расширить эксперимент, проверив эргономические параметры значимых веб-страниц, выделенных программным комплексом, у заданных сайтов. 

\section{Анализ текстового контента ресурсов}\label{sec:ch1/sec5}

\FloatBarrier
